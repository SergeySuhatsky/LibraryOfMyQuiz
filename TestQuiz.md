<!--
author: AI Assistant
email:  ai@example.com
version: 0.0.2
language: en
narrator: US English Female
comment: Тест по теории вероятностей (Темы 1-3) - Исправленные отступы
-->

Вот 45 вопросов (по 15 на каждую из тем 1, 2 и 3) в формате LiaScript без мета-шапки.

# Тема 1: Случайные события

**1. Что является пространством элементарных исходов $\Omega$ при однократном броске игральной кости?**

[[ ]] $\{1, 6\}$
[[ ]] $\{0, 1\}$
[[X]] $\{1, 2, 3, 4, 5, 6\}$
[[ ]] Любое подмножество чисел.
[[?]] Это множество всех возможных, взаимоисключающих результатов.
**************************************************
Пространство $\Omega$ содержит все грани кубика от 1 до 6.
**************************************************

**2. Как называется событие, которое обязательно происходит в результате опыта?**

[[ ]] Случайное
[[ ]] Невозможное
[[X]] Достоверное
[[ ]] Элементарное
[[?]] Его вероятность равна 1.
**************************************************
Достоверное событие ($U$ или $\Omega$) происходит всегда.
**************************************************

**3. Если $A$ — случайное событие, то каким множеством оно является по отношению к $\Omega$?**

[[ ]] Надмножеством
[[X]] Подмножеством
[[ ]] Пересечением
[[ ]] Дополнением
[[?]] Событие состоит из элементарных исходов.
**************************************************
По определению, любое событие $A \subseteq \Omega$.
**************************************************

**4. Какие события называются несовместными?**

[[ ]] Которые зависят друг от друга.
[[ ]] Которые всегда наступают вместе.
[[X]] Которые не могут наступить одновременно.
[[ ]] Которые образуют полную группу.
[[?]] Пересечение таких событий пусто.
**************************************************
Несовместные события не имеют общих элементарных исходов.
**************************************************

**5. Два стрелка стреляют в мишень. Событие A — «попал первый», событие B — «попал второй». Являются ли они несовместными?**

[[ ]] Да, всегда.
[[X]] Нет, они могут попасть одновременно.
[[ ]] Да, если они стреляют по очереди.
[[ ]] Нет, это одно и то же событие.
[[?]] Могут ли в мишени оказаться две дырки?
**************************************************
Эти события совместны, так как возможно одновременное попадание (пересечение $A \cap B \neq \emptyset$).
**************************************************

**6. Что такое элементарное событие ($\omega$)?**

[[ ]] Любое подмножество исходов.
[[X]] Неразложимый, простейший исход опыта.
[[ ]] Событие, вероятность которого мала.
[[ ]] Объединение всех исходов.
[[?]] Это «атом» теории вероятностей.
**************************************************
Элементарное событие — это точка в пространстве $\Omega$, которую нельзя разбить на более мелкие составляющие.
**************************************************

**7. Как обозначается невозможное событие?**

[[ ]] $\Omega$
[[ ]] $0$
[[X]] $\emptyset$ (или $V$)
[[ ]] $\infty$
[[?]] Символ пустого множества.
**************************************************
Невозможное событие не содержит ни одного элементарного исхода, поэтому обозначается как пустое множество.
**************************************************

**8. Образуют ли события «Выпало четное число» и «Выпало нечетное число» полную группу при броске кубика?**

[[X]] Да.
[[ ]] Нет, они совместны.
[[ ]] Нет, они не покрывают все исходы.
[[ ]] Нет, так как вероятность разная.
[[?]] Проверьте два условия: $A \cap B = \emptyset$ и $A \cup B = \Omega$.
**************************************************
Да: они несовместны, и их объединение дает все возможные грани $\{1..6\}$.
**************************************************

**9. В чем суть понятия «противоположное событие» $\bar{A}$?**

[[ ]] Это событие, которое наступает вместе с $A$.
[[ ]] Это любое событие, несовместное с $A$.
[[X]] Это событие, состоящее в ненаступлении $A$.
[[ ]] Это событие с отрицательной вероятностью.
[[?]] «Не А».
**************************************************
$\bar{A}$ происходит тогда и только тогда, когда не происходит $A$.
**************************************************

**10. В коробке лежат только красные карандаши. Событие «вынули синий карандаш» является:**

[[ ]] Случайным.
[[ ]] Достоверным.
[[X]] Невозможным.
[[ ]] Элементарным.
[[?]] В коробке нет синих карандашей.
**************************************************
Это событие никогда не наступит в данном опыте.
**************************************************

**11. Сколько элементарных исходов при подбрасывании одной монеты?**

[[ ]] 1
[[X]] 2
[[ ]] 3 (включая ребро)
[[ ]] 0
[[?]] Орел и решка.
**************************************************
В стандартной модели два исхода: $\Omega = \{Г, Р\}$.
**************************************************

**12. Может ли полная группа событий состоять из трех событий?**

[[ ]] Нет, только из двух.
[[X]] Да, если они попарно несовместны и их сумма — достоверное событие.
[[ ]] Нет, только из четного числа.
[[ ]] Да, но только если они равновероятны.
[[?]] Пример: выпадение 1-2, 3-4, 5-6 на кубике.
**************************************************
Полная группа может состоять из любого количества событий $n \ge 1$, если выполняются условия полноты и несовместности.
**************************************************

**13. Являются ли события $A$ и $\bar{A}$ несовместными?**

[[X]] Да, всегда.
[[ ]] Нет, иногда пересекаются.
[[ ]] Только если $P(A) = 0.5$.
[[ ]] Зависит от типа опыта.
[[?]] Может ли событие произойти и не произойти одновременно?
**************************************************
По определению $A \cap \bar{A} = \emptyset$.
**************************************************

**14. Опыт: сдача экзамена. Событие $A$ — «получить оценку 5». Является ли исход «получить 4» благоприятствующим событию $A$?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Да, если 4 — это тоже хорошо.
[[ ]] Зависит от студента.
[[?]] Входит ли исход «4» в множество $A=\{5\}$?
**************************************************
Нет, исход «4» не входит в подмножество исходов события $A$.
**************************************************

**15. Если событие $A$ включает в себя все элементарные исходы $\Omega$, то оно:**

[[ ]] Невозможное.
[[ ]] Случайное.
[[X]] Достоверное.
[[ ]] Элементарное.
[[?]] $A = \Omega$.
**************************************************
Событие, совпадающее с пространством исходов, является достоверным.
**************************************************

# Тема 2: Алгебра событий

**1. Какому логическому союзу соответствует сумма событий $A \cup B$?**

[[ ]] И (AND)
[[X]] ИЛИ (OR)
[[ ]] НЕ (NOT)
[[ ]] ЕСЛИ (IF)
[[?]] Наступает хотя бы одно.
**************************************************
Объединение множеств соответствует логической дизъюнкции (ИЛИ).
**************************************************

**2. Событие $C = A \cap B$ означает:**

[[ ]] Наступление $A$ или $B$.
[[X]] Одновременное наступление $A$ и $B$.
[[ ]] Наступление $A$ без $B$.
[[ ]] Ненаступление ни $A$, ни $B$.
[[?]] Это пересечение, общая часть.
**************************************************
Пересечение (произведение) событий означает, что произошли оба события.
**************************************************

**3. Чему равно выражение $A \cup A$?**

[[ ]] $2A$
[[ ]] $\Omega$
[[X]] $A$
[[ ]] $\emptyset$
[[?]] Закон идемпотентности.
**************************************************
Объединение события с самим собой ничего не меняет.
**************************************************

**4. Если $A \subset B$, чему равно $A \cap B$?**

[[ ]] $B$
[[X]] $A$
[[ ]] $\Omega$
[[ ]] $\emptyset$
[[?]] Какая часть является общей, если $A$ внутри $B$?
**************************************************
Пересечение подмножества с надмножеством равно подмножеству.
**************************************************

**5. Что означает запись $\bar{A}$?**

[[ ]] Событие $A$ произошло дважды.
[[ ]] Событие $A$ влечет событие $B$.
[[X]] Противоположное событие (дополнение).
[[ ]] Разность событий.
[[?]] Черта сверху — стандартное обозначение отрицания.
**************************************************
Это событие «Не А», состоящее из всех исходов, не входящих в $A$.
**************************************************

**6. Запишите с помощью операций событие: «Произошло событие A, но не произошло B».**

[[ ]] $A \cup \bar{B}$
[[ ]] $\bar{A} \cap B$
[[X]] $A \cap \bar{B}$ (или $A \setminus B$)
[[ ]] $\overline{A \cap B}$
[[?]] «А» И «Не В».
**************************************************
Это пересечение $A$ с дополнением $B$.
**************************************************

**7. Чему равно $A \cup \emptyset$?**

[[ ]] $\emptyset$
[[X]] $A$
[[ ]] $\Omega$
[[ ]] $\bar{A}$
[[?]] Добавить «ничего» к $A$.
**************************************************
Объединение с невозможным событием не меняет исходное событие.
**************************************************

**8. Чему равно $A \cap \emptyset$?**

[[X]] $\emptyset$
[[ ]] $A$
[[ ]] $\Omega$
[[ ]] $\bar{A}$
[[?]] Что общего у события и пустоты?
**************************************************
Пересечение с пустым множеством всегда пусто.
**************************************************

**9. Закон де Моргана: $\overline{A \cap B} = ...$**

[[ ]] $\bar{A} \cap \bar{B}$
[[X]] $\bar{A} \cup \bar{B}$
[[ ]] $A \cup B$
[[ ]] $\overline{A} \cap B$
[[?]] Отрицание «И» меняет знак на «ИЛИ».
**************************************************
«Неверно, что А и В» равносильно «Не А или Не В».
**************************************************

**10. Как записывается событие «Наступило хотя бы одно из событий $A_1, A_2, A_3$»?**

[[ ]] $A_1 \cap A_2 \cap A_3$
[[X]] $A_1 \cup A_2 \cup A_3$
[[ ]] $A_1 + A_2 - A_3$
[[ ]] $\overline{A_1 A_2 A_3}$
[[?]] Ключевые слова «хотя бы одно» указывают на сумму.
**************************************************
Это объединение всех трех событий.
**************************************************

**11. Чему равно $A \cup \bar{A}$?**

[[ ]] $\emptyset$
[[ ]] $A$
[[X]] $\Omega$ (Достоверное событие)
[[ ]] $A \cap \bar{A}$
[[?]] Событие или произойдет, или не произойдет — третьего не дано.
**************************************************
Объединение события и его отрицания покрывает всё пространство исходов.
**************************************************

**12. Верно ли равенство $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$?**

[[X]] Да, это дистрибутивный закон.
[[ ]] Нет, скобки нельзя раскрывать так.
[[ ]] Только если события несовместны.
[[ ]] Нет, знаки должны поменяться местами.
[[?]] Аналог $a(b+c) = ab + ac$.
**************************************************
Это свойство дистрибутивности пересечения относительно объединения.
**************************************************

**13. Что такое диаграммы Венна?**

[[ ]] Таблицы истинности.
[[ ]] Графики плотности вероятности.
[[X]] Геометрическое изображение множеств (событий) в виде фигур.
[[ ]] Деревья решений.
[[?]] Круги внутри прямоугольника.
**************************************************
Это графический способ визуализации операций над множествами.
**************************************************

**14. Если $A$ и $B$ несовместны, чему равно $A \cap B$?**

[[ ]] $A$
[[ ]] $B$
[[X]] $\emptyset$
[[ ]] $\Omega$
[[?]] Определение несовместности.
**************************************************
Несовместные события не имеют общих элементов.
**************************************************

**15. Событие $(A \cap B) \cup (A \cap \bar{B})$ можно упростить до:**

[[ ]] $B$
[[X]] $A$
[[ ]] $\Omega$
[[ ]] $\emptyset$
[[?]] Вынесите $A$ за скобку: $A \cap (B \cup \bar{B})$.
**************************************************
$A \cap \Omega = A$. Это означает, что $A$ наступило безразлично к тому, наступило $B$ или нет.
**************************************************

# Тема 3: Определение вероятности

**1. Формула классической вероятности:**

[[X]] $P(A) = m/n$
[[ ]] $P(A) = n/m$
[[ ]] $P(A) = 1 - m/n$
[[ ]] $P(A) = m \times n$
[[?]] Отношение благоприятных к общим.
**************************************************
Вероятность равна числу благоприятных исходов $m$, деленному на общее число равновозможных исходов $n$.
**************************************************

**2. Какое условие обязательно для применения классической формулы?**

[[ ]] $n$ должно быть бесконечным.
[[X]] Исходы должны быть равновозможными и образовывать полную группу.
[[ ]] События должны быть независимыми.
[[ ]] $m$ должно быть больше $n$.
[[?]] Симметрия исходов (например, правильный кубик).
**************************************************
Классическое определение работает только для схемы случаев (конечное число равновероятных исходов).
**************************************************

**3. Статистическая вероятность — это:**

[[ ]] Теоретически вычисленное число.
[[X]] Относительная частота события при большом числе опытов.
[[ ]] Мнение эксперта.
[[ ]] Геометрическая мера.
[[?]] $W(A) = N(A)/N$ при $N \to \infty$.
**************************************************
Это предел частоты появления события в длинной серии испытаний.
**************************************************

**4. Геометрическая вероятность применяется, когда:**

[[ ]] Исходов конечное число.
[[X]] Множество исходов бесконечно и заполняет область (отрезок, фигуру).
[[ ]] Вероятности исходов неизвестны.
[[ ]] Опыт нельзя повторить.
[[?]] Вероятность попадания точки в область.
**************************************************
Обобщение классической схемы на непрерывные пространства (меры).
**************************************************

**5. Аксиома нормировки гласит:**

[[ ]] $P(A) \ge 0$
[[ ]] $P(\emptyset) = 0$
[[X]] $P(\Omega) = 1$
[[ ]] $P(A \cup B) = P(A) + P(B)$
[[?]] Вероятность достоверного события.
**************************************************
Вероятность всего пространства элементарных исходов равна единице.
**************************************************

**6. Если $A$ и $B$ несовместны, то $P(A \cup B) = ...$**

[[ ]] $P(A) \cdot P(B)$
[[X]] $P(A) + P(B)$
[[ ]] $P(A) - P(B)$
[[ ]] $1$
[[?]] Аксиома сложения (аддитивности).
**************************************************
Для несовместных событий вероятность суммы равна сумме вероятностей.
**************************************************

**7. Чему равна вероятность невозможного события?**

[[ ]] 1
[[X]] 0
[[ ]] 0.5
[[ ]] -1
[[?]] Следствие из аксиом.
**************************************************
$P(\emptyset) = 0$.
**************************************************

**8. Чему равна вероятность события $\bar{A}$?**

[[ ]] $P(A) - 1$
[[X]] $1 - P(A)$
[[ ]] $1 / P(A)$
[[ ]] $P(A)$
[[?]] $P(A) + P(\bar{A}) = 1$.
**************************************************
Так как $A$ и $\bar{A}$ образуют полную группу несовместных событий.
**************************************************

**9. Может ли вероятность быть равна 1.2?**

[[ ]] Да, в особых случаях.
[[X]] Нет, вероятность всегда $\le 1$.
[[ ]] Да, если это плотность вероятности.
[[ ]] Нет, она не может быть дробной.
[[?]] Аксиома: $0 \le P(A) \le 1$.
**************************************************
Вероятность — это нормированная мера, она не превышает 1.
**************************************************

**10. В урне 5 белых и 5 черных шаров. Какова вероятность вынуть белый шар?**

[[ ]] 0.2
[[X]] 0.5
[[ ]] 1
[[ ]] 0
[[?]] $m=5, n=10$.
**************************************************
$P = 5/10 = 0.5$.
**************************************************

**11. Свойство монотонности вероятности:**

[[ ]] Если $A \subset B$, то $P(A) > P(B)$.
[[X]] Если $A \subset B$, то $P(A) \le P(B)$.
[[ ]] $P(A)$ не зависит от $B$.
[[ ]] Вероятность растет со временем.
[[?]] Часть не может быть больше целого.
**************************************************
Если событие $A$ влечет событие $B$, то вероятность $A$ не может превышать вероятность $B$.
**************************************************

**12. Для совместных событий формула сложения выглядит так:**

[[ ]] $P(A) + P(B)$
[[X]] $P(A) + P(B) - P(A \cap B)$
[[ ]] $P(A) \cdot P(B)$
[[ ]] $P(A) + P(B) + P(A \cap B)$
[[?]] Нужно вычесть «двойной счет» пересечения.
**************************************************
В общей теореме сложения вычитается вероятность совместного появления, так как она была учтена дважды.
**************************************************

**13. Бросают два кубика. Всего исходов $n = ...$?**

[[ ]] 12
[[X]] 36
[[ ]] 6
[[ ]] 21
[[?]] $6 \times 6$.
**************************************************
Каждый из 6 исходов первого кубика комбинируется с 6 исходами второго.
**************************************************

**14. Кто сформулировал современную аксиоматику теории вероятностей?**

[[ ]] Ньютон
[[ ]] Бернулли
[[X]] Колмогоров
[[ ]] Гаусс
[[?]] В 1933 году.
**************************************************
Андрей Николаевич Колмогоров заложил строгий фундамент теории вероятностей на основе теории меры.
**************************************************

**15. Если вероятность события $P(A)=0$, обязательно ли оно невозможное?**

[[ ]] Да, всегда.
[[X]] В классике — да, в геометрической вероятности — нет.
[[ ]] Нет, это значит оно редкое.
[[ ]] Да, если $n$ конечно.
[[?]] Попадание точки в линию имеет нулевую площадь, но возможно.
**************************************************
В непрерывных пространствах возможны события с нулевой вероятностью, которые не являются пустым множеством (например, попадание в конкретную точку).
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 4, 5 и 6) в формате LiaScript.

# Тема 4: Элементы комбинаторики

**1. Что вычисляет факториал числа $n$ ($n!$)?**

[[ ]] Сумму чисел от 1 до $n$.
[[X]] Произведение натуральных чисел от 1 до $n$.
[[ ]] Количество подмножеств множества.
[[ ]] $n$ в степени $n$.
[[?]] $1 \times 2 \times ... \times n$.
**************************************************
Факториал — это произведение всех натуральных чисел от 1 до $n$ включительно.
**************************************************

**2. Какая формула используется для подсчета числа перестановок из $n$ элементов?**

[[X]] $P_n = n!$
[[ ]] $C_n^k = \frac{n!}{k!(n-k)!}$
[[ ]] $A_n^k = \frac{n!}{(n-k)!}$
[[ ]] $n^k$
[[?]] Нужно упорядочить *все* имеющиеся элементы.
**************************************************
Перестановки ($P_n$) показывают сколькими способами можно расставить $n$ различных объектов в ряд.
**************************************************

**3. Главное отличие размещений ($A_n^k$) от сочетаний ($C_n^k$):**

[[ ]] В размещениях не важен порядок, в сочетаниях важен.
[[X]] В размещениях важен порядок элементов, в сочетаниях — нет.
[[ ]] Размещения используют все элементы, сочетания — только часть.
[[ ]] В формуле сочетаний нет факториалов.
[[?]] Код от сейфа (123 и 321 разные) против салата (огурцы+помидоры то же самое, что помидоры+огурцы).
**************************************************
В размещениях порядок элементов имеет значение (упорядоченные наборы), в сочетаниях — нет (просто подмножества).
**************************************************

**4. Чему равно число сочетаний из 5 по 2 ($C_5^2$)?**

[[ ]] 20
[[X]] 10
[[ ]] 60
[[ ]] 5
[[?]] $\frac{5 \times 4}{2 \times 1}$.
**************************************************
$C_5^2 = \frac{5!}{2!3!} = \frac{120}{2 \times 6} = 10$.
**************************************************

**5. Правило произведения в комбинаторике гласит: если объект А можно выбрать $n$ способами, а объект В — $m$ способами, то пару (А, В) можно выбрать...**

[[ ]] $n + m$ способами.
[[X]] $n \times m$ способами.
[[ ]] $n^m$ способами.
[[ ]] $n! / m!$ способами.
[[?]] Каждому выбору А соответствует любой выбор В.
**************************************************
Если выборы независимы и последовательны, количество вариантов перемножается.
**************************************************

**6. Чему равен $0!$ (ноль факториал)?**

[[ ]] 0
[[X]] 1
[[ ]] Не определен.
[[ ]] $\infty$
[[?]] Это математическое соглашение для корректности формул.
**************************************************
$0! = 1$. Это число способов упорядочить пустое множество (один способ — ничего не делать).
**************************************************

**7. У вас есть 10 человек. Сколькими способами можно выбрать команду из 3 человек (должности не важны)?**

[[ ]] $A_{10}^3$
[[X]] $C_{10}^3$
[[ ]] $P_{10}$
[[ ]] $3^{10}$
[[?]] Порядок людей в команде не важен.
**************************************************
Так как порядок выбора не важен, используем формулу сочетаний.
**************************************************

**8. Свойство симметрии сочетаний записывается как:**

[[X]] $C_n^k = C_n^{n-k}$
[[ ]] $C_n^k = C_n^{k-1}$
[[ ]] $C_n^k = A_n^k$
[[ ]] $C_n^k = C_{n-1}^{k-1} + C_{n-1}^k$
[[?]] Выбрать 2 предмета, которые возьмем, это то же самое, что выбрать $n-2$ предмета, которые оставим.
**************************************************
Число способов выбрать $k$ элементов равно числу способов выбрать оставшиеся $n-k$ элементов.
**************************************************

**9. В урне $N$ шаров, из них $M$ белых. Выбираем $n$ шаров. Сколькими способами можно это сделать (знаменатель гипергеометрической формулы)?**

[[ ]] $C_M^n$
[[ ]] $A_N^n$
[[X]] $C_N^n$
[[ ]] $N^n$
[[?]] Общее число исходов: выбор $n$ любых из $N$ имеющихся.
**************************************************
Мы выбираем $n$ объектов из общего количества $N$, порядок не важен.
**************************************************

**10. Сколько различных четырехзначных пин-кодов можно составить из цифр 0-9, если цифры могут повторяться?**

[[ ]] $10 \times 9 \times 8 \times 7$
[[ ]] $C_{10}^4$
[[X]] $10^4 = 10000$
[[ ]] $10!$
[[?]] Размещения с повторениями. На каждой из 4 позиций может быть любая из 10 цифр.
**************************************************
Согласно правилу произведения: $10 \times 10 \times 10 \times 10 = 10 000$.
**************************************************

**11. Формула размещений $A_n^k$ вычисляется как:**

[[ ]] $\frac{n!}{k!}$
[[X]] $\frac{n!}{(n-k)!}$
[[ ]] $\frac{n!}{k!(n-k)!}$
[[ ]] $n! - k!$
[[?]] Это как сочетания, но мы не делим на перестановки внутри выборки ($k!$).
**************************************************
$A_n^k = n(n-1)...(n-k+1) = \frac{n!}{(n-k)!}$.
**************************************************

**12. Сколькими способами можно расставить 5 книг на полке?**

[[ ]] 5
[[ ]] 25
[[X]] 120
[[ ]] 10
[[?]] Перестановки $P_5$.
**************************************************
$P_5 = 5! = 1 \times 2 \times 3 \times 4 \times 5 = 120$.
**************************************************

**13. Если в задаче сказано «выбрать председателя и секретаря» из группы, какую формулу использовать?**

[[ ]] Сочетания (порядок не важен).
[[X]] Размещения (порядок важен).
[[ ]] Перестановки.
[[ ]] Сумму факториалов.
[[?]] Важно ли, кто будет председателем, а кто секретарем?
**************************************************
Роли различаются, значит, важен порядок (или привязка человека к месту). Это размещения.
**************************************************

**14. Чему равно $C_n^0$ (число способов выбрать 0 элементов из $n$)?**

[[ ]] 0
[[X]] 1
[[ ]] $n$
[[ ]] $n!$
[[?]] Сколькими способами можно ничего не взять?
**************************************************
Существует только один способ выбрать пустое множество — ничего не выбирать.
**************************************************

**15. Правило суммы ($n + m$) используется, когда:**

[[ ]] Элементы выбираются последовательно (и то, и другое).
[[X]] Элементы выбираются взаимоисключающе (или то, или другое).
[[ ]] Выбор происходит с возвращением.
[[ ]] Элементы образуют пару.
[[?]] Логическое «ИЛИ».
**************************************************
Если нужно выбрать один элемент из множества $A$ ИЛИ из множества $B$ (и они не пересекаются), варианты складываются.
**************************************************

# Тема 5: Прямые методы вычисления вероятности

**1. Что является числителем в формуле классической вероятности при решении задач с урнами?**

[[ ]] Общее число сочетаний $C_N^n$.
[[ ]] Число перестановок $P_n$.
[[X]] Произведение сочетаний благоприятных групп (например, $C_K^k \cdot C_{N-K}^{n-k}$).
[[ ]] $1$.
[[?]] Число способов сформировать нужный состав выборки.
**************************************************
Числитель — это число благоприятных исходов, которое часто вычисляется как произведение сочетаний (выбрать нужные белые И нужные черные).
**************************************************

**2. Геометрическая вероятность события $A$ (попадание в область $g \subset G$) равна:**

[[ ]] $mes(G) - mes(g)$
[[X]] $mes(g) / mes(G)$
[[ ]] $mes(G) / mes(g)$
[[ ]] $mes(g) \times mes(G)$
[[?]] Отношение меры части к мере целого.
**************************************************
Вероятность пропорциональна размеру области (длине, площади, объему): $P = S_{fav} / S_{total}$.
**************************************************

**3. В задаче о встрече (два лица приходят в интервале 1 час) пространством исходов $\Omega$ является:**

[[ ]] Отрезок длины 60.
[[ ]] Прямая линия.
[[X]] Квадрат со стороной 60 (или 1).
[[ ]] Треугольник.
[[?]] У нас две независимые переменные времени: $x$ и $y$.
**************************************************
Так как времена прихода $x$ и $y$ независимы, $\Omega$ — это декартово произведение отрезков, то есть квадрат.
**************************************************

**4. Главное условие применимости геометрической вероятности:**

[[ ]] Области должны быть круглыми.
[[ ]] Исходов должно быть конечное число.
[[X]] Вероятность попадания точки в любую часть области пропорциональна ее мере и не зависит от расположения.
[[ ]] Область $G$ должна быть неограниченной.
[[?]] Равновозможность попадания в любую точку.
**************************************************
Это аналог равновероятности исходов в классике, но для непрерывного пространства.
**************************************************

**5. Чтобы найти вероятность того, что в группе из $n$ человек у всех разные дни рождения, мы используем:**

[[ ]] Формулу полной вероятности.
[[X]] Размещения: $\frac{A_{365}^n}{365^n}$.
[[ ]] Сочетания: $C_{365}^n$.
[[ ]] Сумму вероятностей.
[[?]] Первый выбирает любой день, второй — любой кроме одного... порядок важен (люди различимы).
**************************************************
Числитель — число размещений без повторений (все дни разные), знаменатель — размещения с повторениями (все возможные комбинации).
**************************************************

**6. Если $P(A) = 0.99$ (вероятность отказа), как проще найти вероятность «хотя бы одного отказа» в серии?**

[[ ]] Складывать вероятности 1, 2, ... отказов.
[[X]] Перейти к противоположному событию (ни одного отказа).
[[ ]] Перемножить 0.99 на число опытов.
[[ ]] Использовать формулу Байеса.
[[?]] $P(\text{хотя бы один}) = 1 - P(\text{ни одного})$.
**************************************************
Прямой подсчет громоздок. Проще найти вероятность того, что отказов нет, и вычесть её из 1.
**************************************************

**7. Парадокс Бертрана показывает, что:**

[[ ]] Вероятность зависит от времени суток.
[[X]] В геометрической вероятности фраза «наудачу» требует уточнения метода проведения эксперимента.
[[ ]] Хорда всегда длиннее радиуса.
[[ ]] Классическое определение неверно.
[[?]] Задача о хорде в круге имеет разные ответы.
**************************************************
В бесконечных пространствах понятие «равновозможности» не является однозначным без описания процедуры выбора.
**************************************************

**8. В лифт 9-этажного дома сели 3 пассажира. Сколькими способами они могут выйти (общее число исходов $n$)?**

[[ ]] $9 \times 8 \times 7$
[[ ]] $C_9^3$
[[X]] $9^3 = 729$ (если этажи 2-9, то $8^3$).
[[ ]] $3^9$
[[?]] Каждый пассажир выбирает этаж независимо.
**************************************************
Размещения с повторениями. Каждый из 3 человек может выбрать любой из доступных этажей.
**************************************************

**9. Какова вероятность, что случайно брошенная точка в квадрат со стороной $a$ попадет в вписанный круг?**

[[ ]] $1/2$
[[ ]] $\pi$
[[X]] $\pi / 4$
[[ ]] $1 - \pi$
[[?]] Отношение площади круга ($\pi R^2$) к площади квадрата ($(2R)^2$).
**************************************************
$P = \frac{\pi (a/2)^2}{a^2} = \frac{\pi a^2 / 4}{a^2} = \frac{\pi}{4}$.
**************************************************

**10. В урне 5 белых и 5 черных. Вытащили 2 шара. Вероятность, что оба белые:**

[[ ]] $1/4$
[[X]] $2/9$
[[ ]] $1/2$
[[ ]] $5/10 \times 5/10$
[[?]] Без возвращения: $\frac{5}{10} \times \frac{4}{9}$.
**************************************************
$P = \frac{C_5^2}{C_{10}^2} = \frac{10}{45} = \frac{2}{9}$. Или через умножение зависимых событий.
**************************************************

**11. Бросают два кубика. Какова вероятность выпадения дубля (1-1, 2-2...)?**

[[ ]] $1/36$
[[ ]] $1/2$
[[X]] $6/36 = 1/6$
[[ ]] $1/12$
[[?]] Благоприятных исходов 6, всего 36.
**************************************************
$P = m/n = 6/36 = 1/6$.
**************************************************

**12. При расчете вероятности в покере (собрать комбинацию) порядок карт в руке:**

[[ ]] Важен.
[[X]] Не важен.
[[ ]] Зависит от масти.
[[ ]] Важен только для тузов.
[[?]] Набор карт {Туз, Король} и {Король, Туз} — это одна и та же рука.
**************************************************
Используются сочетания ($C_{52}^5$), так как порядок получения карт игроком не влияет на силу комбинации.
**************************************************

**13. Задача Бюффона с иглой — это пример:**

[[ ]] Классической вероятности.
[[ ]] Статистической вероятности.
[[X]] Геометрической вероятности.
[[ ]] Аксиоматической вероятности.
[[?]] Игла падает на разграфленную плоскость.
**************************************************
Это классическая задача геометрической вероятности, позволяющая экспериментально определить число $\pi$.
**************************************************

**14. Может ли геометрическая вероятность быть равна 0, если событие возможно?**

[[X]] Да.
[[ ]] Нет.
[[ ]] Только в трехмерном пространстве.
[[ ]] Только если мера всей области бесконечна.
[[?]] Попадание точки в линию на плоскости.
**************************************************
Площадь линии равна 0, но попасть в неё теоретически возможно. В непрерывных пространствах вероятность 0 не означает невозможность.
**************************************************

**15. В лотерее нужно угадать 3 числа из 10. Какова вероятность выиграть (угадать все 3)?**

[[ ]] $1/1000$
[[ ]] $1/720$
[[X]] $1/120$
[[ ]] $3/10$
[[?]] $n = C_{10}^3 = \frac{10 \cdot 9 \cdot 8}{6}$.
**************************************************
Число исходов $C_{10}^3 = 120$. Благоприятный — 1. Вероятность $1/120$.
**************************************************

# Тема 6: Зависимые и независимые события

**1. Условная вероятность $P(A|B)$ это:**

[[ ]] Вероятность одновременного наступления $A$ и $B$.
[[X]] Вероятность $A$, вычисленная при условии, что $B$ уже произошло.
[[ ]] Вероятность $A$, деленная на вероятность $B$.
[[ ]] Вероятность $A$ минус вероятность $B$.
[[?]] Пространство исходов сужается до $B$.
**************************************************
Это переоценка вероятности события $A$ с учетом новой информации о наступлении $B$.
**************************************************

**2. Формула умножения вероятностей для ЗАВИСИМЫХ событий:**

[[ ]] $P(AB) = P(A) \cdot P(B)$
[[X]] $P(AB) = P(A) \cdot P(B|A)$
[[ ]] $P(AB) = P(A) + P(B)$
[[ ]] $P(AB) = P(A) / P(B)$
[[?]] Вероятность первого умножить на вероятность второго с учетом первого.
**************************************************
Общая теорема умножения: $P(AB) = P(A)P(B|A)$.
**************************************************

**3. Два события $A$ и $B$ называются независимыми, если:**

[[ ]] $P(AB) = 0$
[[ ]] $P(A|B) \neq P(A)$
[[X]] $P(AB) = P(A) \cdot P(B)$ (или $P(A|B) = P(A)$)
[[ ]] Они не могут наступить одновременно.
[[?]] Появление одного не меняет шансы другого.
**************************************************
Независимость означает отсутствие вероятностной связи: условная вероятность равна безусловной.
**************************************************

**4. Из колоды берут карту, запоминают и ВОЗВРАЩАЮТ обратно. Затем берут вторую. События зависимы?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Зависит от масти.
[[ ]] Только если выпали тузы.
[[?]] Меняется ли состав колоды перед вторым вытягиванием?
**************************************************
Так как карту вернули, условия опыта восстановились. События независимы.
**************************************************

**5. Из колоды берут карту и НЕ возвращают. Затем берут вторую. События зависимы?**

[[X]] Да.
[[ ]] Нет.
[[ ]] Только если первая карта — король.
[[ ]] Нет, так как карты случайны.
[[?]] Изменилось ли общее число карт?
**************************************************
Состав колоды изменился, вероятности вытягивания второй карты изменились. События зависимы.
**************************************************

**6. Если $P(A) = 0.5, P(B) = 0.2$ и события независимы, чему равно $P(A \cap B)$?**

[[ ]] 0.7
[[ ]] 0.3
[[X]] 0.1
[[ ]] 0
[[?]] Просто перемножьте.
**************************************************
$0.5 \times 0.2 = 0.1$.
**************************************************

**7. Могут ли несовместные события (с ненулевыми вероятностями) быть независимыми?**

[[ ]] Да, всегда.
[[X]] Нет, никогда.
[[ ]] Да, если их сумма равна 1.
[[ ]] Зависит от природы опыта.
[[?]] Если $A$ наступило, какова вероятность, что наступит несовместное с ним $B$?
**************************************************
Если $A$ наступило, вероятность $B$ становится 0 (была $>0$). Вероятность изменилась $\to$ зависимы.
**************************************************

**8. Чему равна $P(A|A)$ (вероятность А при условии А)?**

[[ ]] $P(A)$
[[ ]] 0
[[X]] 1
[[ ]] $P(A)^2$
[[?]] Если $A$ произошло, какова вероятность, что $A$ произошло?
**************************************************
Это достоверное событие.
**************************************************

**9. Формула определения условной вероятности $P(A|B) = ...$**

[[X]] $P(AB) / P(B)$
[[ ]] $P(AB) / P(A)$
[[ ]] $P(A) \cdot P(B)$
[[ ]] $P(A) - P(B)$
[[?]] Отношение пересечения к условию.
**************************************************
$P(A|B) = \frac{P(A \cap B)}{P(B)}$.
**************************************************

**10. Вероятность безотказной работы одного элемента 0.9. В цепи 2 таких элемента соединены последовательно и работают независимо. Какова надежность цепи?**

[[ ]] 1.8
[[ ]] 0.9
[[X]] 0.81
[[ ]] 0.18
[[?]] Цепь работает, если работает 1-й И 2-й.
**************************************************
$P(A_1 A_2) = P(A_1) P(A_2) = 0.9 \times 0.9 = 0.81$.
**************************************************

**11. Если события $A, B, C$ независимы в совокупности, то:**

[[ ]] $P(ABC) = P(A)+P(B)+P(C)$
[[ ]] Они попарно несовместны.
[[X]] Вероятность произведения любых из них равна произведению их вероятностей.
[[ ]] Достаточно попарной независимости.
[[?]] Это более сильное требование, чем попарная независимость.
**************************************************
Независимость в совокупности требует, чтобы правило умножения работало для любой комбинации событий.
**************************************************

**12. Пример Бернштейна с тетраэдром показывает, что:**

[[ ]] Попарная независимость влечет независимость в совокупности.
[[X]] Из попарной независимости НЕ следует независимость в совокупности.
[[ ]] Сумма вероятностей может быть больше 1.
[[ ]] Геометрическая вероятность не работает в 3D.
[[?]] Грани: красная, синяя, зеленая, пестрая.
**************************************************
События могут быть независимы по двое, но зависимы в тройке (знание двух цветов определяет третий).
**************************************************

**13. Вероятность попадания стрелка 0.8. Он сделал промах. Какова вероятность попадания следующим выстрелом (выстрелы независимы)?**

[[ ]] Меньше 0.8 (расстроился).
[[ ]] Больше 0.8 (должен отыграться).
[[X]] 0.8
[[ ]] 0
[[?]] "У монеты нет памяти", у независимых выстрелов тоже.
**************************************************
Независимость означает, что прошлый результат не влияет на будущую вероятность.
**************************************************

**14. Если $P(A|B) > P(A)$, то события:**

[[ ]] Независимы.
[[X]] Зависимы (коррелируют положительно).
[[ ]] Несовместны.
[[ ]] Противоположны.
[[?]] Наступление $B$ повышает шансы $A$.
**************************************************
Это признак вероятностной зависимости (событие $B$ "притягивает" $A$).
**************************************************

**15. Можно ли применять теорему умножения $P(AB) = P(A)P(B)$ для зависимых событий?**

[[ ]] Да, всегда.
[[ ]] Да, если ошибка невелика.
[[X]] Нет, это приведет к ошибке.
[[ ]] Только если $P(A) = P(B)$.
[[?]] Для зависимых обязателен условный множитель.
**************************************************
Для зависимых событий формула верна только в виде $P(A)P(B|A)$. Использование формулы для независимых событий даст неверный результат.
**************************************************


Вот 60 вопросов (по 15 на каждую из тем 7, 8, 9 и 10) в формате LiaScript.

# Тема 7: Теорема сложения вероятностей

**1. Теорема сложения для двух СОВМЕСТНЫХ событий $A$ и $B$:**

[[ ]] $P(A \cup B) = P(A) + P(B)$
[[X]] $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
[[ ]] $P(A \cup B) = P(A) \cdot P(B)$
[[ ]] $P(A \cup B) = P(A) + P(B) + P(AB)$
[[?]] Нужно исключить двойной счет их пересечения.
**************************************************
Так как область пересечения $A \cap B$ входит и в $A$, и в $B$, при простом сложении мы считаем её дважды. Один раз нужно вычесть.
**************************************************

**2. Если события $A$ и $B$ несовместны, чему равно $P(A \cap B)$ в формуле сложения?**

[[X]] 0
[[ ]] 1
[[ ]] $P(A)P(B)$
[[ ]] $P(A)+P(B)$
[[?]] Они не могут наступить одновременно.
**************************************************
Для несовместных событий пересечение — это пустое множество, его вероятность равна нулю.
**************************************************

**3. Какое событие является противоположным событию «Наступило хотя бы одно из независимых событий $A_1, ... A_n$»?**

[[ ]] Наступили все события.
[[ ]] Не наступило хотя бы одно.
[[X]] Не наступило ни одного события.
[[ ]] Наступило ровно одно.
[[?]] Отрицание «ИЛИ» — это «И» отрицаний (Закон де Моргана).
**************************************************
Противоположностью «хотя бы одному успеху» является «полный провал» (ни одного успеха).
**************************************************

**4. Формула вероятности наступления хотя бы одного из $n$ независимых событий с вероятностями $p_i$:**

[[ ]] $1 - p_1 p_2 ... p_n$
[[X]] $1 - (1-p_1)(1-p_2)...(1-p_n)$
[[ ]] $p_1 + p_2 + ... + p_n$
[[ ]] $(1-p_1) + ... + (1-p_n)$
[[?]] 1 минус вероятность того, что все промахнулись.
**************************************************
$P(A) = 1 - P(\text{все не наступили}) = 1 - q_1 q_2 ... q_n$.
**************************************************

**5. Может ли сумма $P(A) + P(B)$ быть больше 1?**

[[X]] Да, если события совместны.
[[ ]] Нет, вероятность не может быть больше 1.
[[ ]] Только если это полная группа.
[[ ]] Нет, это ошибка вычислений.
[[?]] Пример: $P(A)=0.8, P(B)=0.8$ (они сильно пересекаются).
**************************************************
Сумма вероятностей может превышать 1, но вероятность объединения $P(A \cup B)$ всегда $\le 1$ (за счет вычитания пересечения).
**************************************************

**6. Вероятность попадания в мишень первого стрелка 0.7, второго 0.8. Они стреляют залпом. Вероятность хотя бы одного попадания:**

[[ ]] 1.5
[[ ]] 0.56
[[X]] $1 - 0.3 \cdot 0.2 = 0.94$
[[ ]] $0.7 + 0.8 = 1.5 \to 1$
[[?]] Через вероятность промаха обоих.
**************************************************
Вероятность одновременного промаха $0.3 \times 0.2 = 0.06$. Искомая вероятность $1 - 0.06 = 0.94$.
**************************************************

**7. Чему равна вероятность объединения события $A$ и достоверного события $U$?**

[[ ]] $P(A)$
[[ ]] 0
[[X]] 1
[[ ]] $1 - P(A)$
[[?]] $A \cup \Omega = \Omega$.
**************************************************
Объединение с достоверным событием дает достоверное событие.
**************************************************

**8. Для трех совместных событий формула сложения (включений-исключений) содержит слагаемое $+P(ABC)$. Почему?**

[[ ]] Потому что его забыли посчитать.
[[X]] Потому что мы вычли его три раза (в составе парных пересечений) и добавили три раза (в одиночных), итого 0 раз. Надо вернуть.
[[ ]] Это ошибка, там должен быть минус.
[[ ]] Это вероятность независимости.
[[?]] Геометрия трех пересекающихся кругов.
**************************************************
Центральная область $ABC$ была учтена 3 раза в одиночных суммах и вычтена 3 раза в парных. В итоге она не учтена совсем, поэтому ее нужно прибавить.
**************************************************

**9. Если $A \subset B$, чему равно $P(A \cup B)$?**

[[ ]] $P(A)$
[[X]] $P(B)$
[[ ]] $P(A) + P(B)$
[[ ]] $P(B) - P(A)$
[[?]] Объединение части и целого есть целое.
**************************************************
Если $A$ внутри $B$, то их сумма — это само $B$.
**************************************************

**10. Вероятность отказа детали 0.1. Работают 2 детали параллельно (система работает, если работает хотя бы одна). Надежность системы:**

[[ ]] 0.2
[[ ]] 0.81
[[X]] $1 - 0.1 \cdot 0.1 = 0.99$
[[ ]] 0.9
[[?]] Система откажет, только если откажут ОБЕ.
**************************************************
Вероятность отказа системы $0.1^2 = 0.01$. Надежность $1 - 0.01 = 0.99$.
**************************************************

**11. Можно ли применять теорему сложения для зависимых событий?**

[[X]] Да, формула $P(A)+P(B)-P(AB)$ универсальна.
[[ ]] Нет, только для независимых.
[[ ]] Только если известны условные вероятности.
[[ ]] Нет, для зависимых используют теорему умножения.
[[?]] Пересечение $P(AB)$ для зависимых считается через условную вероятность, но сама теорема сложения верна.
**************************************************
Теорема верна, просто слагаемое $P(AB)$ нужно будет вычислять с учетом зависимости ($P(A)P(B|A)$).
**************************************************

**12. Что означает, если $P(A \cup B) = P(A) + P(B)$?**

[[ ]] События независимы.
[[X]] События несовместны (или вероятность пересечения равна 0).
[[ ]] События образуют полную группу.
[[ ]] Это всегда так.
[[?]] Это частный случай теоремы.
**************************************************
Это верно тогда и только тогда, когда $P(A \cap B) = 0$.
**************************************************

**13. Два студента ищут книгу. Вероятность найти у первого 0.5, у второго 0.5. Вероятность, что книгу найдут (кто-то из них):**

[[ ]] 1 (100%)
[[X]] 0.75
[[ ]] 0.25
[[ ]] 0.5
[[?]] $0.5 + 0.5 - 0.25$.
**************************************************
$P(A \cup B) = 0.5 + 0.5 - (0.5 \cdot 0.5) = 0.75$.
**************************************************

**14. Свойство субаддитивности вероятности (неравенство Буля):**

[[ ]] $P(\cup A_i) = \sum P(A_i)$
[[X]] $P(\cup A_i) \le \sum P(A_i)$
[[ ]] $P(\cup A_i) \ge \sum P(A_i)$
[[ ]] $P(AB) \le P(A) + P(B)$
[[?]] Сумма вероятностей — это верхняя граница для объединения.
**************************************************
Вероятность суммы событий не превосходит суммы их вероятностей (равенство только для несовместных).
**************************************************

**15. Какое событие противоположно событию «Все $n$ выстрелов попали в цель»?**

[[ ]] Все $n$ выстрелов — промахи.
[[X]] Хотя бы один промах.
[[ ]] Ровно один промах.
[[ ]] Половина промахов.
[[?]] Отрицание «ВСЕ» — это «НЕ ВСЕ» (то есть нашелся хоть один плохой).
**************************************************
$\overline{A_1 A_2 ... A_n} = \bar{A}_1 \cup \bar{A}_2 \cup ... \cup \bar{A}_n$.
**************************************************

# Тема 8: Формула полной вероятности и Байеса

**1. Что такое гипотезы $H_1, ..., H_n$ в формуле полной вероятности?**

[[ ]] Любые события из условия задачи.
[[ ]] Независимые события.
[[X]] Полная группа попарно несовместных событий.
[[ ]] События с одинаковой вероятностью.
[[?]] Они должны покрывать все возможные сценарии и не пересекаться.
**************************************************
Гипотезы должны быть несовместны и в сумме давать достоверное событие (сумма их вероятностей равна 1).
**************************************************

**2. Чему равна сумма априорных вероятностей гипотез $\sum P(H_i)$?**

[[ ]] Зависит от задачи.
[[ ]] $P(A)$
[[X]] 1
[[ ]] 0
[[?]] Условие нормировки полной группы.
**************************************************
Так как гипотезы образуют полную группу, одно из них обязательно произойдет.
**************************************************

**3. Формула полной вероятности используется для:**

[[ ]] Вычисления вероятности гипотезы.
[[X]] Вычисления вероятности события $A$, которое зависит от того, какая гипотеза реализуется.
[[ ]] Проверки независимости событий.
[[ ]] Расчета дисперсии.
[[?]] $P(A) = \sum P(H_i)P(A|H_i)$.
**************************************************
Она позволяет найти безусловную вероятность события $A$, усредняя условные вероятности по всем гипотезам.
**************************************************

**4. В формуле Байеса знаменатель — это:**

[[ ]] Вероятность гипотезы.
[[ ]] Условная вероятность.
[[X]] Полная вероятность события $A$ ($P(A)$).
[[ ]] 1.
[[?]] Нормировочный коэффициент.
**************************************************
Знаменатель вычисляется по формуле полной вероятности.
**************************************************

**5. Формула Байеса позволяет вычислить:**

[[ ]] Априорную вероятность $P(H_i)$.
[[X]] Апостериорную вероятность гипотезы $P(H_i|A)$ после того, как событие $A$ произошло.
[[ ]] Вероятность события $A$.
[[ ]] Совместную вероятность $P(A H_i)$.
[[?]] «Переоценка вероятностей гипотез».
**************************************************
Байес отвечает на вопрос: «Событие произошло. Какова вероятность, что причиной была именно эта гипотеза?».
**************************************************

**6. Если $P(H_1) = 0.8, P(H_2) = 0.2$, а $P(A|H_1) = 1, P(A|H_2) = 0.5$, чему равна $P(A)$?**

[[ ]] 1.5
[[X]] $0.8 \cdot 1 + 0.2 \cdot 0.5 = 0.9$
[[ ]] 0.5
[[ ]] 0.8
[[?]] Полная вероятность.
**************************************************
Взвешенная сумма: $0.8 + 0.1 = 0.9$.
**************************************************

**7. Урна 1: 2 белых шара. Урна 2: 2 черных шара. Наугад выбирают урну и берут шар. Вероятность, что он белый:**

[[ ]] 1
[[ ]] 0
[[X]] 0.5
[[ ]] 0.25
[[?]] Гипотезы выбора урны равновероятны (0.5).
**************************************************
$P(W) = 0.5 \cdot 1 (из 1-й) + 0.5 \cdot 0 (из 2-й) = 0.5$.
**************************************************

**8. Апостериорная вероятность гипотезы $H_k$ всегда:**

[[ ]] Больше априорной.
[[ ]] Меньше априорной.
[[X]] Может быть больше или меньше, в зависимости от того, насколько $H_k$ «поддерживает» событие $A$.
[[ ]] Равна 1.
[[?]] Если гипотеза предсказывала событие лучше других, ее вероятность растет.
**************************************************
Байес перераспределяет вероятности в пользу тех гипотез, при которых наблюденное событие было наиболее вероятным.
**************************************************

**9. В медицинской диагностике: болезнь редкая ($P(H)=0.001$), тест точный ($P(A|H)=0.99$). Если тест положителен ($A$), вероятность болезни:**

[[ ]] 0.99
[[ ]] Около 0.5
[[X]] Может быть низкой (например, 0.09) из-за влияния здорового большинства.
[[ ]] 0.001
[[?]] Эффект базового процента (base rate fallacy).
**************************************************
Из-за огромного количества здоровых людей, даже малый процент ложных срабатываний на них может перевесить верные срабатывания на больных.
**************************************************

**10. Может ли полная вероятность события $A$ быть больше 1?**

[[ ]] Да, если гипотез много.
[[X]] Нет, никогда.
[[ ]] Только в квантовой механике.
[[ ]] Если сумма вероятностей гипотез > 1.
[[?]] $P(A)$ — это вероятность, она ограничена.
**************************************************
Так как это вероятность, она лежит в $[0, 1]$.
**************************************************

**11. Если событие $A$ не зависит от гипотез (то есть $P(A|H_i) = p$ для всех $i$), то $P(A)$ равна:**

[[X]] $p$
[[ ]] $p/n$
[[ ]] 1
[[ ]] $\sum P(H_i)$
[[?]] Вынесите $p$ за скобку в сумме.
**************************************************
$P(A) = \sum P(H_i) p = p \sum P(H_i) = p \cdot 1 = p$.
**************************************************

**12. Что произойдет с апостериорными вероятностями, если событие $A$ оказалось невозможным ($P(A)=0$)?**

[[ ]] Они станут равны 0.
[[ ]] Они станут равны 1.
[[X]] Формула Байеса неприменима (деление на ноль).
[[ ]] Они не изменятся.
[[?]] Мы не можем обуславливаться событием, которое не происходит.
**************************************************
Если событие невозможно, знаменатель Байеса равен 0.
**************************************************

**13. Два завода производят детали. 1-й: 60% продукции, 1% брака. 2-й: 40% продукции, 2% брака. Выбрана деталь. Вероятность, что она бракованная:**

[[ ]] 0.03
[[ ]] 0.015
[[X]] $0.6 \cdot 0.01 + 0.4 \cdot 0.02 = 0.014$
[[ ]] 1.4
[[?]] Формула полной вероятности.
**************************************************
$0.006 + 0.008 = 0.014$ (или 1.4%).
**************************************************

**14. В той же задаче: деталь оказалась бракованной. Какова вероятность, что она со 2-го завода?**

[[ ]] $0.008 / 0.014 \approx 0.57$
[[X]] $0.008 / 0.014 \approx 0.57$
[[ ]] $0.4$
[[ ]] $0.02$
[[?]] Вклад 2-го завода (0.008) делить на общий брак (0.014).
**************************************************
По формуле Байеса: доля «вины» второго завода в общем браке.
**************************************************

**15. Можно ли использовать формулу Байеса последовательно (много раз)?**

[[X]] Да, апостериорная вероятность первого шага становится априорной для второго.
[[ ]] Нет, только один раз.
[[ ]] Только если события независимы.
[[ ]] Нет, это приведет к ошибке накопления.
[[?]] Байесовское обновление знаний.
**************************************************
Это основа байесовского обучения: знание обновляется с каждым новым фактом.
**************************************************

# Тема 9: Схема Бернулли

**1. Какие условия определяют схему Бернулли?**

[[ ]] Разные вероятности успеха, зависимые опыты.
[[X]] $n$ независимых испытаний, два исхода (успех/неудача), постоянная вероятность успеха $p$.
[[ ]] Бесконечное число испытаний.
[[ ]] Испытания проводятся до первого успеха.
[[?]] Стандартная модель: монета или кубик.
**************************************************
Главное: независимость и постоянство $p$ в каждом опыте.
**************************************************

**2. Формула Бернулли вычисляет:**

[[ ]] Вероятность успеха в одном опыте.
[[ ]] Вероятность того, что успех наступит хотя бы раз.
[[X]] Вероятность того, что в $n$ опытах успех наступит ровно $k$ раз.
[[ ]] Наивероятнейшее число успехов.
[[?]] $P_n(k)$.
**************************************************
Это вероятность конкретного числа успехов в серии.
**************************************************

**3. Множитель $C_n^k$ в формуле Бернулли означает:**

[[ ]] Вероятность одного сценария.
[[X]] Количество способов выбрать, в каких именно $k$ опытах произойдет успех.
[[ ]] Общее число исходов.
[[ ]] Вероятность неудачи.
[[?]] Успехи могут чередоваться по-разному (УНУ, УУН, НУУ).
**************************************************
Это число сочетаний, учитывающее порядок расположения успехов.
**************************************************

**4. Если $p$ — вероятность успеха, то $q$ (вероятность неудачи) равна:**

[[ ]] $p - 1$
[[ ]] $1 / p$
[[X]] $1 - p$
[[ ]] $p$
[[?]] Успех и неудача образуют полную группу.
**************************************************
$q = 1 - p$.
**************************************************

**5. Вероятность получить 0 успехов в $n$ испытаниях (все неудачи):**

[[ ]] 0
[[X]] $q^n$
[[ ]] $p^n$
[[ ]] $C_n^0$
[[?]] Неудача И неудача И ...
**************************************************
По теореме умножения для независимых событий: $q \times q \times ... \times q$.
**************************************************

**6. Монету бросают 3 раза. Вероятность выпадения орла ровно 2 раза:**

[[ ]] $1/8$
[[ ]] $1/4$
[[X]] $3/8$
[[ ]] $1/2$
[[?]] $C_3^2 (0.5)^2 (0.5)^1$.
**************************************************
$3 \times 0.125 = 0.375$ (или 3/8).
**************************************************

**7. Наивероятнейшее число успехов $k_0$ удовлетворяет неравенству:**

[[ ]] $np - p \le k_0 \le np + q$
[[X]] $np - q \le k_0 \le np + p$
[[ ]] $k_0 = np$
[[ ]] $k_0 = n/2$
[[?]] Интервал длиной 1 вокруг мат. ожидания.
**************************************************
$k_0$ — целое число, заключенное в этом интервале.
**************************************************

**8. Если $np + p$ — целое число, то наивероятнейших значений:**

[[ ]] Одно.
[[X]] Два.
[[ ]] Ни одного.
[[ ]] $n$.
[[?]] Например, $k_0$ и $k_0 - 1$.
**************************************************
В этом случае максимумов вероятности два, и они идут подряд.
**************************************************

**9. Сумма всех вероятностей Бернулли $\sum_{k=0}^n P_n(k)$ равна:**

[[ ]] $p$
[[ ]] $n$
[[X]] 1
[[ ]] $0.5$
[[?]] Бином Ньютона $(p+q)^n$.
**************************************************
Это сумма вероятностей полной группы несовместных событий (0 успехов, 1 успех ... n успехов). $(p+q)^n = 1^n = 1$.
**************************************************

**10. Для схемы Бернулли математическое ожидание числа успехов равно:**

[[ ]] $n$
[[X]] $np$
[[ ]] $p$
[[ ]] $npq$
[[?]] Среднее число успехов.
**************************************************
$M[X] = np$.
**************************************************

**11. Дисперсия числа успехов в схеме Бернулли:**

[[ ]] $np$
[[ ]] $\sqrt{npq}$
[[X]] $npq$
[[ ]] $(np)^2$
[[?]] Мера разброса.
**************************************************
$D[X] = npq$.
**************************************************

**12. При $p=0.5$ биномиальное распределение:**

[[ ]] Убывающее.
[[ ]] Возрастающее.
[[X]] Симметричное.
[[ ]] Равномерное.
[[?]] Шансы успеха и неудачи равны.
**************************************************
Гистограмма вероятностей симметрична относительно центра.
**************************************************

**13. Если вероятность успеха $p > 0.5$, то наивероятнейшее число $k_0$ будет:**

[[X]] Больше $n/2$.
[[ ]] Меньше $n/2$.
[[ ]] Равно $n$.
[[ ]] Равно 0.
[[?]] Центр распределения смещается вправо.
**************************************************
Так как $k_0 \approx np$, а $p > 0.5$.
**************************************************

**14. Можно ли применять формулу Бернулли, если вероятность успеха меняется от опыта к опыту?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Да, если взять среднее $p$.
[[ ]] Только для малых $n$.
[[?]] Это нарушает определение схемы Бернулли.
**************************************************
Схема Бернулли требует $p = const$. Если $p$ меняется, это схема Пуассона (обобщенная).
**************************************************

**15. Вероятность выбросить 6 очков хотя бы один раз при 4 бросках кубика:**

[[ ]] $4/6$
[[X]] $1 - (5/6)^4$
[[ ]] $(1/6)^4$
[[ ]] $C_4^1 (1/6)$
[[?]] Через противоположное событие.
**************************************************
$1 - P(\text{ни одной шестерки}) = 1 - (5/6)^4 \approx 0.517$.
**************************************************

# Тема 10: Асимптотические формулы

**1. Когда применяется формула Пуассона в схеме Бернулли?**

[[ ]] Когда $n$ мало, $p$ велико.
[[ ]] Всегда.
[[X]] Когда $n$ велико, а $p$ очень мало (редкие события).
[[ ]] Когда $p=0.5$.
[[?]] $\lambda = np = const$.
**************************************************
Закон редких событий: большое число попыток с малой вероятностью успеха в каждой.
**************************************************

**2. Параметр $\lambda$ в формуле Пуассона равен:**

[[ ]] $n$
[[ ]] $p$
[[X]] $np$
[[ ]] $\sqrt{npq}$
[[?]] Среднее число успехов.
**************************************************
$\lambda = np$ — интенсивность потока успехов.
**************************************************

**3. Формула Пуассона выглядит как:**

[[ ]] $\frac{\lambda^k}{k!}$
[[X]] $\frac{\lambda^k e^{-\lambda}}{k!}$
[[ ]] $\varphi(x)$
[[ ]] $e^{-\lambda}$
[[?]] Содержит экспоненту.
**************************************************
$P_n(k) \approx \frac{\lambda^k e^{-\lambda}}{k!}$.
**************************************************

**4. Локальная теорема Муавра-Лапласа применяется для:**

[[ ]] Вычисления вероятности интервала.
[[X]] Вычисления вероятности ровно $k$ успехов при больших $n$ и не малых $p$.
[[ ]] Малых $n$.
[[ ]] Редких событий.
[[?]] $P_n(k) \approx ...$
**************************************************
Она приближает вероятность конкретного значения биномиального распределения через нормальную плотность.
**************************************************

**5. Функция $\varphi(x)$ в локальной теореме Лапласа:**

[[ ]] Нечетная.
[[X]] Четная ($\varphi(-x) = \varphi(x)$).
[[ ]] Периодическая.
[[ ]] Всегда положительная и > 1.
[[?]] График — колокол, симметричный относительно оси Y.
**************************************************
Это плотность стандартного нормального распределения, она симметрична.
**************************************************

**6. Аргумент $x$ в теоремах Муавра-Лапласа вычисляется как:**

[[ ]] $k - np$
[[X]] $\frac{k - np}{\sqrt{npq}}$
[[ ]] $\frac{k}{\sqrt{n}}$
[[ ]] $np$
[[?]] Это стандартизация случайной величины.
**************************************************
Отклонение от среднего, деленное на стандартное отклонение.
**************************************************

**7. Интегральная теорема Муавра-Лапласа используется для нахождения вероятности:**

[[ ]] Конкретного числа успехов.
[[X]] Того, что число успехов будет в интервале от $k_1$ до $k_2$.
[[ ]] Того, что успех никогда не наступит.
[[ ]] Ошибки первого рода.
[[?]] $P(k_1 \le m \le k_2)$.
**************************************************
Она использует интеграл (функцию Лапласа) для оценки вероятности попадания в диапазон.
**************************************************

**8. Функция Лапласа $\Phi(x)$ обладает свойством:**

[[ ]] $\Phi(-x) = \Phi(x)$
[[X]] $\Phi(-x) = -\Phi(x)$ (нечетная).
[[ ]] $\Phi(0) = 1$
[[ ]] $\Phi(\infty) = 1$
[[?]] Интеграл от симметричной функции от 0 до $x$.
**************************************************
При смене знака предела интегрирования знак интеграла меняется. $\Phi(x)$ нечетна.
**************************************************

**9. Чему равен предел $\Phi(x)$ при $x \to \infty$ (в нормировке от 0 до $x$)?**

[[ ]] 1
[[X]] 0.5
[[ ]] $\pi$
[[ ]] 0
[[?]] Половина площади под нормальной кривой.
**************************************************
Вся площадь равна 1, половина (от 0 до $\infty$) равна 0.5.
**************************************************

**10. Какое условие должно выполняться для хорошей точности теоремы Муавра-Лапласа?**

[[ ]] $npq > 10$ (или 20).
[[ ]] $n > 10$.
[[ ]] $p < 0.1$.
[[ ]] $np = 1$.
[[?]] "Правило большого пальца" для нормальной аппроксимации.
**************************************************
Произведение $npq$ (дисперсия) должно быть достаточно большим, чтобы распределение стало симметричным и "колоколообразным".
**************************************************

**11. Завод отправил 5000 бутылок. Вероятность разбить одну — 0.0002. Вероятность, что разобьют ровно 2:**

[[ ]] Считать по Бернулли.
[[ ]] Считать по Муавру-Лапласу.
[[X]] Считать по Пуассону с $\lambda = 1$.
[[ ]] 0.
[[?]] $n$ велико, $p$ мало, $\lambda = 5000 \cdot 0.0002 = 1$.
**************************************************
$P(2) = \frac{1^2 e^{-1}}{2!} = \frac{1}{2e} \approx 0.18$.
**************************************************

**12. Множитель перед $\varphi(x)$ в локальной формуле равен:**

[[ ]] $1/n$
[[ ]] $\sqrt{npq}$
[[X]] $\frac{1}{\sqrt{npq}}$
[[ ]] 1
[[?]] Нормировочный коэффициент, обратный СКО.
**************************************************
$P_n(k) \approx \frac{1}{\sqrt{npq}} \varphi(x)$.
**************************************************

**13. Если в интегральной теореме границы интервала симметричны относительно $np$ (например, $np \pm \Delta$), то вероятность равна:**

[[ ]] 0
[[X]] $2\Phi(\frac{\Delta}{\sqrt{npq}})$
[[ ]] $\Phi(\Delta)$
[[ ]] 1
[[?]] $\Phi(z) - \Phi(-z) = 2\Phi(z)$.
**************************************************
Из-за нечетности функции Лапласа вероятности складываются.
**************************************************

**14. Чем больше $n$, тем распределение Бернулли:**

[[ ]] Больше похоже на равномерное.
[[X]] Больше похоже на нормальное (Гаусса).
[[ ]] Больше похоже на показательное.
[[ ]] Становится хаотичным.
[[?]] Суть Центральной Предельной Теоремы.
**************************************************
Биномиальное распределение асимптотически стремится к нормальному.
**************************************************

**15. Если $\Phi(x)$ задана как интеграл от $-\infty$ до $x$, то $\Phi(0)$ равна:**

[[ ]] 0
[[X]] 0.5
[[ ]] 1
[[ ]] -0.5
[[?]] Площадь левой половины колокола.
**************************************************
В альтернативной таблице значение в нуле равно 0.5. (Важно смотреть на определение в конкретном задачнике, здесь мы обсуждали версию от 0 до $x$, где ответ был бы 0).
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 11, 12 и 13) в формате LiaScript.

# Тема 11: Закон распределения дискретной случайной величины

**1. Какая величина называется дискретной случайной величиной (ДСВ)?**

[[ ]] Величина, принимающая любые значения из интервала.
[[X]] Величина, множество значений которой конечно или счетно (можно пронумеровать).
[[ ]] Величина, которая всегда равна константе.
[[ ]] Величина, график распределения которой — гладкая кривая.
[[?]] Ключевое слово — «дискретный» (прерывистый).
**************************************************
ДСВ принимает отдельные, изолированные значения $x_1, x_2, ...$.
**************************************************

**2. Что такое ряд распределения?**

[[ ]] График функции плотности.
[[ ]] Формула Бернулли.
[[X]] Таблица, содержащая возможные значения величины и соответствующие им вероятности.
[[ ]] Совокупность всех исходов эксперимента.
[[?]] Таблица с двумя строками: $x_i$ и $p_i$.
**************************************************
Ряд распределения — это основной способ задания ДСВ, устанавливающий соответствие между значениями и их вероятностями.
**************************************************

**3. Условие нормировки для ряда распределения ДСВ:**

[[ ]] $\sum x_i = 1$
[[X]] $\sum p_i = 1$
[[ ]] $\sum p_i = n$
[[ ]] $\sum x_i p_i = 0$
[[?]] Сумма вероятностей полной группы событий.
**************************************************
Так как случайная величина обязательно примет одно из своих возможных значений, сумма их вероятностей равна единице.
**************************************************

**4. Функция распределения $F(x)$ определяется как:**

[[ ]] $P(X = x)$
[[X]] $P(X < x)$ (или $P(X \le x)$ в зависимости от соглашения).
[[ ]] $P(X > x)$
[[ ]] Производная от плотности.
[[?]] Вероятность того, что величина примет значение *левее* точки $x$.
**************************************************
В данном курсе используется определение непрерывности слева: $F(x) = P(X < x)$.
**************************************************

**5. График функции распределения $F(x)$ для ДСВ представляет собой:**

[[ ]] Прямую линию.
[[ ]] Параболу.
[[X]] Разрывную ступенчатую функцию.
[[ ]] Колоколообразную кривую.
[[?]] Она постоянна между значениями и скачет в точках $x_i$.
**************************************************
Функция сохраняет значение на интервалах между возможными значениями СВ и делает скачок вверх в точках $x_i$.
**************************************************

**6. Высота скачка функции $F(x)$ в точке разрыва $x_i$ равна:**

[[ ]] Значению $x_i$.
[[ ]] 1.
[[X]] Вероятности $p_i = P(X=x_i)$.
[[ ]] 0.5.
[[?]] На сколько возрастает накопленная вероятность?
**************************************************
Скачок соответствует вероятности того, что величина примет именно это значение.
**************************************************

**7. Чему равно $F(+\infty)$?**

[[ ]] 0
[[X]] 1
[[ ]] $\infty$
[[ ]] $x_{max}$
[[?]] Вероятность того, что $X < \infty$.
**************************************************
Это вероятность достоверного события (величина примет какое-нибудь конечное значение).
**************************************************

**8. Многоугольник распределения — это:**

[[ ]] Диаграмма Венна.
[[X]] Ломаная, соединяющая точки $(x_i, p_i)$ на плоскости.
[[ ]] График функции $F(x)$.
[[ ]] Область значений СВ.
[[?]] Графическое представление ряда распределения.
**************************************************
Это визуализация таблицы распределения.
**************************************************

**9. Вероятность попадания ДСВ в полуинтервал $[a, b)$ равна:**

[[ ]] $F(a) - F(b)$
[[X]] $F(b) - F(a)$
[[ ]] $F(b) + F(a)$
[[ ]] $f(b) - f(a)$
[[?]] Разность значений функции распределения на концах.
**************************************************
$P(a \le X < b) = P(X < b) - P(X < a) = F(b) - F(a)$.
**************************************************

**10. Если $X$ принимает значения $1, 2, 5$ с вероятностями $0.2, 0.3, 0.5$, чему равно $F(3)$?**

[[ ]] 0.2
[[ ]] 0.3
[[X]] $0.2 + 0.3 = 0.5$
[[ ]] 1
[[?]] Сумма вероятностей всех значений меньше 3.
**************************************************
$F(3) = P(X < 3) = P(X=1) + P(X=2) = 0.5$.
**************************************************

**11. Может ли функция распределения ДСВ убывать?**

[[ ]] Да, на отдельных участках.
[[X]] Нет, она неубывающая.
[[ ]] Да, если вероятности отрицательные (невозможно).
[[ ]] Только при $x < 0$.
[[?]] Накопленная вероятность не может уменьшаться при расширении интервала.
**************************************************
$F(x)$ — неубывающая функция, так как вероятность неотрицательна.
**************************************************

**12. Влияет ли порядок расположения $x_i$ в таблице на математическую суть распределения?**

[[X]] Нет, но обычно упорядочивают по возрастанию для удобства построения $F(x)$.
[[ ]] Да, порядок жестко фиксирован.
[[ ]] Да, сумма вероятностей зависит от порядка.
[[ ]] Нет, $x_i$ можно менять местами с $p_i$.
[[?]] Сумма не меняется от перестановки слагаемых.
**************************************************
Набор пар $(x_i, p_i)$ определяет распределение однозначно, порядок важен только для графиков.
**************************************************

**13. Если случайная величина имеет всего одно значение $C$ с вероятностью 1, она называется:**

[[ ]] Бернуллиевской.
[[ ]] Непрерывной.
[[X]] Вырожденной (или детерминированной).
[[ ]] Пуассоновской.
[[?]] Случайности нет.
**************************************************
Такая величина по сути является константой.
**************************************************

**14. Как найти вероятность $P(a \le X \le b)$ для ДСВ, зная ряд распределения?**

[[ ]] $F(b) - F(a)$.
[[X]] Просуммировать вероятности всех $x_i$, входящих в отрезок $[a, b]$.
[[ ]] $p_b - p_a$.
[[ ]] Интегрировать ряд.
[[?]] Внимание на границы: включены обе.
**************************************************
Для ДСВ формула разности $F(x)$ работает только для полуинтервала. Для отрезка нужно аккуратно суммировать вероятности конкретных точек.
**************************************************

**15. Может ли $x_i$ (значение СВ) быть отрицательным?**

[[ ]] Нет, только положительным.
[[X]] Да, это любое действительное число.
[[ ]] Нет, так как вероятность положительна.
[[ ]] Только в нормальном распределении.
[[?]] Пример: температура зимой или проигрыш в лотерее.
**************************************************
Значения случайной величины могут быть любыми, ограничения ($0 \le p \le 1$) касаются только вероятностей.
**************************************************

# Тема 12: Распределение Бернулли и Биномиальное

**1. Случайная величина, имеющая распределение Бернулли, принимает значения:**

[[ ]] Любые целые.
[[X]] 0 и 1.
[[ ]] От 0 до $n$.
[[ ]] Только положительные.
[[?]] Успех (1) или неудача (0).
**************************************************
Это индикатор события: 1 с вероятностью $p$, 0 с вероятностью $q$.
**************************************************

**2. Биномиальное распределение описывает:**

[[ ]] Время ожидания успеха.
[[ ]] Номер первого успеха.
[[X]] Число успехов в $n$ независимых испытаниях Бернулли.
[[ ]] Число событий в единицу времени.
[[?]] Сумма $n$ независимых величин Бернулли.
**************************************************
Оно показывает, сколько раз выпал «успех» в серии фиксированной длины.
**************************************************

**3. Формула вероятности для биномиального закона:**

[[ ]] $p^k q^{n-k}$
[[ ]] $C_n^k p^n$
[[X]] $C_n^k p^k q^{n-k}$
[[ ]] $e^{-\lambda} \lambda^k / k!$
[[?]] Формула Бернулли.
**************************************************
Вероятность получить ровно $k$ успехов.
**************************************************

**4. Область возможных значений биномиальной случайной величины:**

[[ ]] $0, 1, ..., \infty$
[[X]] $0, 1, 2, ..., n$
[[ ]] $1, 2, ..., n$
[[ ]] Любые числа.
[[?]] Нельзя получить больше успехов, чем было попыток.
**************************************************
Минимум 0 успехов, максимум $n$.
**************************************************

**5. Если $n=1$, биномиальное распределение превращается в:**

[[ ]] Равномерное.
[[X]] Распределение Бернулли.
[[ ]] Распределение Пуассона.
[[ ]] Геометрическое.
[[?]] Одно испытание.
**************************************************
Распределение Бернулли — это частный случай биномиального при $n=1$.
**************************************************

**6. Сумма вероятностей биномиального распределения $\sum_{k=0}^n P_n(k)$ равна:**

[[ ]] $n$
[[ ]] $p$
[[X]] 1
[[ ]] $2^n$
[[?]] Бином Ньютона $(p+q)^n$.
**************************************************
Так как $p+q=1$, то и сумма вероятностей равна 1.
**************************************************

**7. Параметрами биномиального распределения являются:**

[[ ]] $M[X]$ и $D[X]$.
[[X]] $n$ (число опытов) и $p$ (вероятность успеха).
[[ ]] Только $p$.
[[ ]] $\lambda$.
[[?]] От чего зависит формула $C_n^k p^k q^{n-k}$?
**************************************************
Распределение полностью задается количеством попыток и вероятностью успеха в одной попытке.
**************************************************

**8. Какое значение является модой (наивероятнейшим) биномиального распределения?**

[[ ]] $n/2$
[[ ]] $p$
[[X]] Целое число $k$ вблизи $np$.
[[ ]] 0
[[?]] См. тему 9.
**************************************************
Мода находится в интервале $[np-q, np+p]$.
**************************************************

**9. Индикатор события $I_A$ — это случайная величина, которая:**

[[ ]] Равна вероятности события $A$.
[[X]] Равна 1, если $A$ наступило, и 0, если нет.
[[ ]] Считает количество наступлений $A$.
[[ ]] Всегда равна 1.
[[?]] Функция, «индицирующая» наступление события.
**************************************************
Это простейшая ДСВ (распределение Бернулли).
**************************************************

**10. Симметрично ли биномиальное распределение?**

[[ ]] Да, всегда.
[[X]] Только при $p=0.5$.
[[ ]] Нет, никогда.
[[ ]] Только при больших $n$.
[[?]] Если шансы успеха и неудачи равны.
**************************************************
При $p \neq 0.5$ распределение скошено ("хвост" длиннее с одной стороны).
**************************************************

**11. Биномиальное распределение применяется, если испытания:**

[[X]] Независимы.
[[ ]] Зависимы (без возвращения).
[[ ]] Проводятся до первого успеха.
[[ ]] Имеют разные вероятности успеха.
[[?]] Условие схемы Бернулли.
**************************************************
Если испытания зависимы, это гипергеометрическое распределение (или другое), но не биномиальное.
**************************************************

**12. Максимальное значение вероятности в биномиальном распределении при больших $n$:**

[[ ]] Стремится к 1.
[[X]] Уменьшается (распределение «размазывается»).
[[ ]] Равно 0.5.
[[ ]] Постоянно.
[[?]] Сумма всех вероятностей 1, а количество точек $n$ растет.
**************************************************
Чем больше $n$, тем больше возможных исходов, и вероятность каждого конкретного исхода (даже моды) падает.
**************************************************

**13. В партии из 10 деталей 3 бракованные. Берем 4 детали *с возвращением*. Число бракованных распределено по:**

[[ ]] Гипергеометрическому закону.
[[X]] Биномиальному закону.
[[ ]] Закону Пуассона.
[[ ]] Равномерному закону.
[[?]] Возвращение обеспечивает постоянство $p=0.3$ и независимость.
**************************************************
Схема с возвращением $\to$ независимость $\to$ Биномиальное.
**************************************************

**14. Если $X \sim B(n, p)$, то случайная величина $Y = n - X$ (число неудач) имеет распределение:**

[[ ]] $B(n, p)$
[[X]] $B(n, q)$
[[ ]] Геометрическое.
[[ ]] Пуассона.
[[?]] Мы просто меняем местами успех и неудачу.
**************************************************
Это тоже биномиальное распределение, но с вероятностью $1-p$.
**************************************************

**15. Можно ли задать биномиальное распределение таблицей?**

[[X]] Да, таблица будет иметь $n+1$ столбец.
[[ ]] Нет, только формулой.
[[ ]] Да, но только для малых $p$.
[[ ]] Нет, это непрерывное распределение.
[[?]] Это ДСВ с конечным числом значений.
**************************************************
Любую ДСВ с конечным числом значений можно представить в виде ряда (таблицы).
**************************************************

# Тема 13: Геометрическое и Пуассоновское распределения

**1. Геометрическое распределение описывает:**

[[ ]] Число успехов в $n$ опытах.
[[X]] Количество испытаний до первого успеха (включительно).
[[ ]] Время между событиями.
[[ ]] Количество событий за фиксированное время.
[[?]] «Сколько раз надо бросить монету, чтобы выпал орел?»
**************************************************
Это распределение номера первой успешной попытки в схеме Бернулли.
**************************************************

**2. Формула вероятности для геометрического распределения ($X=k$):**

[[ ]] $p^k$
[[ ]] $C_n^k p^k q^{n-k}$
[[X]] $q^{k-1} p$
[[ ]] $\lambda e^{-\lambda}$
[[?]] $k-1$ неудача, потом 1 успех.
**************************************************
Вероятность цепочки НН...НУ.
**************************************************

**3. Множество значений геометрической случайной величины:**

[[ ]] $0, 1, 2, ...$
[[X]] $1, 2, 3, ...$ (счетное множество).
[[ ]] Конечный набор.
[[ ]] Интервал $[0, \infty)$.
[[?]] Минимум 1 попытка нужна.
**************************************************
Попыток может быть сколь угодно много, но минимум одна.
**************************************************

**4. Распределение Пуассона применяется для:**

[[ ]] Описания серии из 5 бросков монеты.
[[X]] Описания потока редких событий (число событий за интервал времени).
[[ ]] Описания времени безотказной работы.
[[ ]] Зависимых испытаний.
[[?]] «Закон редких событий».
**************************************************
Когда $n$ велико, $p$ мало, а произведение $np$ конечно.
**************************************************

**5. Формула Пуассона:**

[[ ]] $\frac{\lambda^k}{k!}$
[[X]] $\frac{\lambda^k e^{-\lambda}}{k!}$
[[ ]] $e^{-\lambda x}$
[[ ]] $C_n^k p^k$
[[?]] Содержит экспоненту и факториал.
**************************************************
Вероятность того, что произойдет ровно $k$ событий.
**************************************************

**6. Параметр $\lambda$ в распределении Пуассона означает:**

[[ ]] Вероятность успеха.
[[ ]] Максимальное число событий.
[[X]] Среднее число событий за рассматриваемый интервал ($np$).
[[ ]] Дисперсию ошибки.
[[?]] Интенсивность потока.
**************************************************
$\lambda = M[X]$ — среднее ожидаемое количество событий.
**************************************************

**7. Свойство «отсутствия памяти» в дискретном случае характерно для:**

[[ ]] Биномиального распределения.
[[X]] Геометрического распределения.
[[ ]] Распределения Пуассона.
[[ ]] Всех ДСВ.
[[?]] Если мы уже сделали 10 неудачных попыток, вероятность успеха в 11-й такая же, как была в 1-й.
**************************************************
Геометрическое распределение — единственное дискретное с этим свойством. Прошлое не влияет на будущее ожидание.
**************************************************

**8. Сумма вероятностей геометрического распределения $\sum_{k=1}^\infty q^{k-1}p$ равна:**

[[ ]] $\infty$
[[ ]] $1/p$
[[X]] 1
[[ ]] $q/p$
[[?]] Сумма бесконечно убывающей геометрической прогрессии.
**************************************************
$p \cdot (1 + q + q^2 + ...) = p \cdot \frac{1}{1-q} = p \cdot \frac{1}{p} = 1$.
**************************************************

**9. Множество значений случайной величины Пуассона:**

[[ ]] $0, 1, ..., n$
[[X]] $0, 1, 2, ...$ (счетное, теоретически до $\infty$).
[[ ]] $1, 2, ...$
[[ ]] Непрерывное.
[[?]] Число звонков в колл-центр не ограничено сверху жестким пределом $n$.
**************************************************
Теоретически число событий может быть любым целым неотрицательным числом.
**************************************************

**10. При каких условиях Биномиальное распределение переходит в Пуассоновское?**

[[ ]] $n \to \infty, p \to 1$
[[X]] $n \to \infty, p \to 0, np \to \lambda = const$
[[ ]] $n$ мало, $p=0.5$
[[ ]] $p$ не зависит от $n$
[[?]] Предельная теорема Пуассона.
**************************************************
Это аппроксимация для большого числа испытаний с малой вероятностью успеха.
**************************************************

**11. Вероятность того, что событие Пуассона не произойдет ни разу ($k=0$):**

[[ ]] 0
[[ ]] $\lambda$
[[X]] $e^{-\lambda}$
[[ ]] 1
[[?]] Подставьте $k=0$ в формулу.
**************************************************
$P(0) = \frac{\lambda^0 e^{-\lambda}}{0!} = e^{-\lambda}$.
**************************************************

**12. Может ли в геометрическом распределении $p_1 < p_2$?**

[[ ]] Да.
[[X]] Нет, последовательность вероятностей всегда убывает.
[[ ]] Да, если $p > 0.5$.
[[ ]] Нет, они равны.
[[?]] $p_k = q^{k-1}p$. Так как $q < 1$, каждый следующий член меньше предыдущего.
**************************************************
Наивероятнейшее число попыток в геометрическом распределении всегда равно 1.
**************************************************

**13. Среднее число опечаток на странице — 2. Какова вероятность, что опечаток не будет?**

[[ ]] 0
[[X]] $e^{-2} \approx 0.135$
[[ ]] $2e^{-2}$
[[ ]] $0.5$
[[?]] Распределение Пуассона с $\lambda=2, k=0$.
**************************************************
Применение формулы $P(0)$ для Пуассона.
**************************************************

**14. Чем отличается геометрическое распределение от биномиального?**

[[ ]] Вероятностями исходов.
[[ ]] Независимостью испытаний.
[[X]] В биномиальном фиксировано $n$ (число опытов), в геометрическом фиксировано $k=1$ (число успехов), а число опытов случайно.
[[ ]] Ничем.
[[?]] Разные остановки эксперимента.
**************************************************
В одном случае мы ограничиваем время, в другом — ждем результата.
**************************************************

**15. Если $\lambda$ в распределении Пуассона велико (например, 100), то оно:**

[[ ]] Становится равномерным.
[[ ]] Становится геометрическим.
[[X]] Приближается к нормальному (Гаусса).
[[ ]] Вырождается в точку.
[[?]] ЦПТ работает и здесь.
**************************************************
При больших $\lambda$ распределение Пуассона становится симметричным и похожим на колокол.
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 14, 15 и 16) в формате LiaScript.

# Тема 14: Закон распределения непрерывной случайной величины (НСВ)

**1. Главное отличие непрерывной случайной величины (НСВ) от дискретной:**

[[ ]] НСВ принимает только целые значения.
[[X]] Множество значений НСВ несчетно (заполняет интервал), а вероятность каждого отдельного значения равна 0.
[[ ]] НСВ не имеет математического ожидания.
[[ ]] НСВ всегда положительна.
[[?]] Дискретная — точки, непрерывная — сплошная линия.
**************************************************
НСВ может принять любое значение из некоторого промежутка, число этих значений бесконечно велико.
**************************************************

**2. Чему равна вероятность $P(X = c)$ для любой непрерывной случайной величины?**

[[ ]] $F(c)$
[[ ]] $f(c)$
[[X]] 0
[[ ]] 1
[[?]] Площадь линии равна нулю.
**************************************************
Попадание в конкретную точку для НСВ является событием с нулевой вероятностью (хотя и возможным).
**************************************************

**3. Каким способом НЕЛЬЗЯ задать непрерывную случайную величину?**

[[ ]] Функцией распределения $F(x)$.
[[ ]] Плотностью вероятности $f(x)$.
[[X]] Рядом распределения (таблицей).
[[ ]] Аналитической формулой.
[[?]] Можно ли перечислить все числа на отрезке?
**************************************************
Так как множество значений несчетно, их невозможно записать в таблицу.
**************************************************

**4. Плотность распределения $f(x)$ связана с функцией распределения $F(x)$ соотношением:**

[[ ]] $f(x) = \int F(x) dx$
[[X]] $f(x) = F'(x)$
[[ ]] $f(x) = F(x) - 1$
[[ ]] $f(x) = 1/F(x)$
[[?]] Плотность — это скорость изменения накопленной вероятности.
**************************************************
Плотность есть первая производная от функции распределения (в точках дифференцируемости).
**************************************************

**5. Как восстановить функцию распределения $F(x)$, зная плотность $f(t)$?**

[[ ]] $F(x) = f'(x)$
[[X]] $F(x) = \int_{-\infty}^x f(t) dt$
[[ ]] $F(x) = \int_0^x f(t) dt$
[[ ]] $F(x) = 1 - f(x)$
[[?]] Операция, обратная дифференцированию.
**************************************************
Функция распределения — это интеграл от плотности с переменным верхним пределом.
**************************************************

**6. Размерность плотности вероятности $f(x)$, если $X$ измеряется в метрах:**

[[ ]] Метры.
[[ ]] Безразмерная.
[[X]] $1/\text{метр}$.
[[ ]] Квадратные метры.
[[?]] $P \approx f(x) \Delta x$. Вероятность безразмерна.
**************************************************
Плотность — это вероятность на единицу длины, поэтому размерность обратная.
**************************************************

**7. Может ли плотность вероятности $f(x)$ быть больше 1?**

[[X]] Да.
[[ ]] Нет, никогда.
[[ ]] Только если $X$ отрицательна.
[[ ]] Только для нормального распределения.
[[?]] Плотность — это не вероятность.
**************************************************
Если интервал значений очень мал (например, $[0, 0.1]$), то плотность на нем будет высокой (например, 10), чтобы интеграл был равен 1.
**************************************************

**8. Является ли функция распределения $F(x)$ непрерывной для НСВ?**

[[ ]] Нет, она ступенчатая.
[[X]] Да, всегда непрерывна.
[[ ]] Только если $f(x)$ непрерывна.
[[ ]] Нет, она имеет разрывы 2-го рода.
[[?]] Определение НСВ.
**************************************************
Для НСВ вероятность скачка равна 0, поэтому функция распределения не имеет разрывов.
**************************************************

**9. Верно ли равенство $P(a < X < b) = P(a \le X \le b)$ для НСВ?**

[[X]] Да, всегда.
[[ ]] Нет, никогда.
[[ ]] Только если $a=0$.
[[ ]] Только для симметричных распределений.
[[?]] Включение границ не меняет вероятность (так как вероятность границы 0).
**************************************************
Для непрерывных величин строгие и нестрогие неравенства равносильны по вероятности.
**************************************************

**10. График плотности вероятности $f(x)$ всегда расположен:**

[[ ]] Выше прямой $y=1$.
[[X]] Выше (или на) оси абсцисс ($y \ge 0$).
[[ ]] Ниже оси абсцисс.
[[ ]] В первой четверти.
[[?]] Производная неубывающей функции неотрицательна.
**************************************************
Вероятность не может убывать с ростом $x$, значит $F'(x) \ge 0$.
**************************************************

**11. Универсальный закон распределения, существующий для ЛЮБОЙ случайной величины (дискретной или непрерывной):**

[[ ]] Плотность вероятности.
[[ ]] Ряд распределения.
[[X]] Функция распределения $F(x)$.
[[ ]] Закон Пуассона.
[[?]] Накопленная вероятность работает везде.
**************************************************
$F(x)$ определена для любых СВ, в отличие от ряда (только ДСВ) или плотности (только НСВ).
**************************************************

**12. «Смешанная» случайная величина — это:**

[[ ]] Сумма двух независимых величин.
[[X]] Величина, имеющая и дискретные скачки вероятности, и непрерывные участки.
[[ ]] Величина без мат. ожидания.
[[ ]] Векторная величина.
[[?]] Пример: время ожидания такси (0, если стоит у входа, и непрерывное, если едет).
**************************************************
Ее функция распределения непрерывна почти везде, но имеет скачки.
**************************************************

**13. Если $f(x) = 0$ при $x < 0$, то $X$ называется:**

[[X]] Неотрицательной случайной величиной.
[[ ]] Нормированной.
[[ ]] Симметричной.
[[ ]] Вырожденной.
[[?]] Величина не принимает отрицательных значений.
**************************************************
Плотность равна нулю там, где значения невозможны.
**************************************************

**14. Геометрически вероятность $P(a < X < b)$ для НСВ равна:**

[[ ]] Длине отрезка $[a, b]$.
[[ ]] $F(b) \cdot F(a)$.
[[X]] Площади под графиком плотности $f(x)$ на интервале $(a, b)$.
[[ ]] Тангенсу угла наклона $F(x)$.
[[?]] Интеграл — это площадь.
**************************************************
Вероятность — это площадь криволинейной трапеции под кривой плотности.
**************************************************

**15. Если $f(x)$ симметрична относительно $x=0$, то:**

[[ ]] $F(0) = 0$
[[ ]] $F(x)$ четная функция.
[[X]] $F(0) = 0.5$
[[ ]] $F(x) = 1$
[[?]] Площадь под всем графиком 1.
**************************************************
Симметрия означает, что слева от нуля лежит половина всей площади (вероятности).
**************************************************

# Тема 15: Функция распределения и ее свойства

**1. Область значений функции распределения $F(x)$:**

[[ ]] $(-\infty, +\infty)$
[[X]] $[0, 1]$
[[ ]] $[0, +\infty)$
[[ ]] $\{-1, 0, 1\}$
[[?]] Это вероятность, она нормирована.
**************************************************
$F(x)$ — это вероятность события $X < x$, поэтому она лежит в пределах от 0 до 1.
**************************************************

**2. Функция распределения $F(x)$ является:**

[[ ]] Всегда возрастающей.
[[X]] Неубывающей.
[[ ]] Невозрастающей.
[[ ]] Периодической.
[[?]] Чем шире интервал $(-\infty, x)$, тем больше вероятность попасть в него.
**************************************************
При увеличении $x$ множество $(-\infty, x)$ расширяется, вероятность не может уменьшаться.
**************************************************

**3. Чему равен предел $\lim_{x \to -\infty} F(x)$?**

[[X]] 0
[[ ]] 1
[[ ]] $f(x)$
[[ ]] -1
[[?]] Вероятность попасть левее минус бесконечности.
**************************************************
Это вероятность невозможного события.
**************************************************

**4. Чему равен предел $\lim_{x \to +\infty} F(x)$?**

[[ ]] 0
[[X]] 1
[[ ]] $\infty$
[[ ]] $M[X]$
[[?]] Вероятность попасть в любое число.
**************************************************
Это вероятность достоверного события ($X < +\infty$).
**************************************************

**5. Вероятность попадания величины в полуинтервал $[a, b)$ через $F(x)$:**

[[ ]] $F(a) - F(b)$
[[ ]] $F(b) \cdot F(a)$
[[X]] $F(b) - F(a)$
[[ ]] $F(b-a)$
[[?]] Вероятность «меньше $b$» минус вероятность «меньше $a$».
**************************************************
$P(a \le X < b) = P(X < b) - P(X < a) = F(b) - F(a)$.
**************************************************

**6. Если $F(x)$ непрерывна в точке $a$, то вероятность $P(X=a)$ равна:**

[[ ]] $F(a)$
[[ ]] 1
[[X]] 0
[[ ]] $F'(a)$
[[?]] Нет скачка — нет сосредоточенной массы вероятности.
**************************************************
Вероятность отдельного значения равна величине скачка функции распределения. Если скачка нет, вероятность 0.
**************************************************

**7. Вероятность того, что $X > x$ (хвост распределения), равна:**

[[ ]] $F(x)$
[[X]] $1 - F(x)$
[[ ]] $1 / F(x)$
[[ ]] $F(-x)$
[[?]] Противоположное событие к $X < x$.
**************************************************
$P(X \ge x) = 1 - P(X < x) = 1 - F(x)$.
**************************************************

**8. Если график $F(x)$ — горизонтальная линия на интервале $(a, b)$, то:**

[[ ]] Вероятность попадания в этот интервал равна 1.
[[X]] Вероятность попадания в этот интервал равна 0.
[[ ]] Плотность $f(x)$ постоянна и не равна 0.
[[ ]] $X$ принимает только значения из этого интервала.
[[?]] $F(b) - F(a) = 0$.
**************************************************
Приращение вероятности на этом участке нулевое, значения из этого интервала не принимаются.
**************************************************

**9. Для дискретной случайной величины график $F(x)$ имеет вид:**

[[ ]] Наклонной прямой.
[[ ]] Колокола.
[[X]] Лестницы (ступенек).
[[ ]] Точек.
[[?]] Постоянна между значениями, скачок в значении.
**************************************************
Ступенчатая функция с разрывами в точках $x_i$.
**************************************************

**10. Медиана распределения $Me$ — это точка, где $F(Me)$ равно:**

[[ ]] 1
[[ ]] 0
[[X]] 0.5
[[ ]] $M[X]$
[[?]] Точка, делящая вероятность пополам.
**************************************************
Слева и справа от медианы лежит по 50% вероятности.
**************************************************

**11. Квантиль порядка $p$ ($x_p$) определяется уравнением:**

[[X]] $F(x_p) = p$
[[ ]] $f(x_p) = p$
[[ ]] $x_p = F(p)$
[[ ]] $x_p = 1-p$
[[?]] Значение, левее которого лежит вероятность $p$.
**************************************************
Это обратная функция к функции распределения.
**************************************************

**12. Может ли $F(2) = 0.3$, а $F(3) = 0.2$?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Да, если плотность отрицательна.
[[ ]] Зависит от распределения.
[[?]] Свойство неубывания.
**************************************************
$F(x)$ не может убывать. $F(3) \ge F(2)$.
**************************************************

**13. Если $X$ равномерно распределена на $[0, 1]$, то при $x \in (0, 1)$ $F(x)$ равна:**

[[ ]] 1
[[ ]] $x^2$
[[X]] $x$
[[ ]] $e^x$
[[?]] Линейный рост от 0 до 1.
**************************************************
$F(x) = \frac{x-a}{b-a} = \frac{x-0}{1-0} = x$.
**************************************************

**14. В определении $F(x) = P(X < x)$ функция считается:**

[[X]] Непрерывной слева.
[[ ]] Непрерывной справа.
[[ ]] Непрерывной везде.
[[ ]] Разрывной везде.
[[?]] В точке скачка значение совпадает с нижним пределом.
**************************************************
Это соглашение отечественной школы (Колмогоров). Западная школа часто использует $P(X \le x)$ (непрерывность справа).
**************************************************

**15. Если $F(x) = 0.5 + \frac{1}{\pi} \arctan x$, то это распределение:**

[[ ]] Нормальное.
[[ ]] Равномерное.
[[X]] Коши.
[[ ]] Показательное.
[[?]] Арктангенс меняется от $-\pi/2$ до $\pi/2$.
**************************************************
Это классический пример распределения Коши (у которого нет мат. ожидания).
**************************************************

# Тема 16: Плотность вероятности

**1. Плотность вероятности $f(x)$ — это:**

[[ ]] Вероятность события $X=x$.
[[X]] Предел отношения вероятности попадания в малый интервал к длине интервала.
[[ ]] Интеграл от функции распределения.
[[ ]] Количество исходов.
[[?]] $f(x) \approx P(x < X < x+\Delta x) / \Delta x$.
**************************************************
Это характеристика концентрации вероятности в точке.
**************************************************

**2. Условие нормировки для плотности вероятности:**

[[ ]] $\int f(x) dx = 0$
[[X]] $\int_{-\infty}^{+\infty} f(x) dx = 1$
[[ ]] $\max f(x) = 1$
[[ ]] $f(x) \le 1$
[[?]] Полная площадь под кривой.
**************************************************
Суммарная вероятность всех возможных значений равна 1.
**************************************************

**3. Может ли плотность вероятности быть отрицательной?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Только на бесконечности.
[[ ]] Если $F(x)$ убывает.
[[?]] Производная неубывающей функции.
**************************************************
Вероятность не может быть отрицательной $\to F(x)$ не убывает $\to f(x) \ge 0$.
**************************************************

**4. Вероятность $P(\alpha < X < \beta)$ вычисляется через плотность как:**

[[ ]] $f(\beta) - f(\alpha)$
[[X]] $\int_\alpha^\beta f(x) dx$
[[ ]] $f(\beta) \cdot f(\alpha)$
[[ ]] $\frac{\beta - \alpha}{f(x)}$
[[?]] Площадь под графиком на участке.
**************************************************
Определенный интеграл от плотности по интервалу.
**************************************************

**5. Если $f(x) = C$ (const) на отрезке $[2, 6]$ и 0 вне его, то $C$ равно:**

[[ ]] 1
[[ ]] 4
[[X]] 0.25
[[ ]] 0.5
[[?]] Площадь прямоугольника: $C \times (6-2) = 1$.
**************************************************
$C = 1 / (6-2) = 1/4$.
**************************************************

**6. Для дискретной случайной величины плотность вероятности:**

[[ ]] Равна константе.
[[X]] Не существует в обычном смысле (или выражается через дельта-функции).
[[ ]] Равна 0 везде.
[[ ]] Совпадает с рядом распределения.
[[?]] У нее нет производной в точках скачков.
**************************************************
В классическом курсе говорят, что плотности у ДСВ нет. В обобщенных функциях это сумма $\delta(x-x_i)p_i$.
**************************************************

**7. Если $X$ — время до отказа прибора (показательное распределение), график $f(x)$:**

[[ ]] Колокол.
[[X]] Убывающая экспонента (при $x>0$).
[[ ]] Прямая линия.
[[ ]] Парабола.
[[?]] $f(x) = \lambda e^{-\lambda x}$.
**************************************************
Максимум при $x=0$, далее монотонное убывание.
**************************************************

**8. Максимум плотности вероятности называется:**

[[ ]] Медианой.
[[ ]] Мат. ожиданием.
[[X]] Модой.
[[ ]] Дисперсией.
[[?]] Самое «модное» (частое) значение.
**************************************************
Точка локального максимума $f(x)$.
**************************************************

**9. Что означает, если $f(x) = 0$ на некотором интервале?**

[[ ]] $X$ принимает значения из этого интервала с вероятностью 1.
[[X]] Попадание $X$ в этот интервал невозможно (вероятность 0).
[[ ]] Функция распределения имеет разрыв.
[[ ]] Это ошибка.
[[?]] Площадь под графиком на этом участке равна 0.
**************************************************
Случайная величина не принимает значения из областей, где плотность нулевая.
**************************************************

**10. Элемент вероятности $f(x)dx$ приближенно равен:**

[[ ]] $F(x)$
[[X]] Вероятности попадания в бесконечно малый интервал $(x, x+dx)$.
[[ ]] Единице.
[[ ]] Плотности в точке $x$.
[[?]] Дифференциал вероятности.
**************************************************
$P(x < X < x+dx) \approx f(x)dx$.
**************************************************

**11. Если $f(x)$ — четная функция, то $M[X]$:**

[[ ]] Не существует.
[[ ]] Равно 1.
[[X]] Равно 0 (если интеграл сходится).
[[ ]] Равно $\sigma$.
[[?]] Центр симметрии графика.
**************************************************
Распределение симметрично относительно нуля.
**************************************************

**12. Размерность плотности вероятности:**

[[ ]] Совпадает с размерностью $X$.
[[X]] Обратна размерности $X$.
[[ ]] Квадрат размерности $X$.
[[ ]] Безразмерна.
[[?]] $f(x) = dP / dx$. Вероятность делить на метры/секунды.
**************************************************
[1/единица измерения СВ].
**************************************************

**13. Чтобы найти коэффициент $A$ в формуле плотности $f(x) = A \sin x$ на $[0, \pi]$, нужно:**

[[ ]] Найти производную.
[[X]] Взять интеграл $\int_0^\pi A \sin x dx$ и приравнять к 1.
[[ ]] Подставить $x=0$.
[[ ]] Найти $F(x)$.
[[?]] Условие нормировки.
**************************************************
$\int_0^\pi \sin x dx = 2$. Значит $2A=1 \Rightarrow A=0.5$.
**************************************************

**14. Чем быстрее убывает плотность $f(x)$ при удалении от центра, тем:**

[[ ]] Больше дисперсия.
[[X]] Меньше дисперсия.
[[ ]] Больше мат. ожидание.
[[ ]] Меньше мода.
[[?]] Значения кучкуются в центре.
**************************************************
Узкий пик означает малый разброс значений.
**************************************************

**15. Плотность нормального распределения имеет форму:**

[[ ]] Прямоугольника.
[[ ]] Треугольника.
[[X]] Симметричного колокола.
[[ ]] Гиперболы.
[[?]] Кривая Гаусса.
**************************************************
Колоколообразная кривая с максимумом в математическом ожидании.
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 17, 18 и 19) в формате LiaScript.

# Тема 17: Числовые характеристики дискретной случайной величины

**1. Математическое ожидание $M[X]$ дискретной случайной величины — это:**

[[ ]] Наиболее вероятное значение.
[[X]] Сумма произведений всех возможных значений на их вероятности ($\sum x_i p_i$).
[[ ]] Среднее арифметическое значений $x_i$.
[[ ]] Значение, делящее ряд пополам.
[[?]] «Взвешенная» сумма значений.
**************************************************
Это теоретическое среднее значение, вокруг которого группируются исходы при многократном повторении опыта.
**************************************************

**2. Физический смысл математического ожидания:**

[[ ]] Момент инерции.
[[X]] Центр тяжести (масс) системы материальных точек.
[[ ]] Плотность вещества.
[[ ]] Скорость движения.
[[?]] Если на оси расположить грузики массой $p_i$ в точках $x_i$.
**************************************************
Точка на оси, относительно которой суммарный момент сил тяжести равен нулю.
**************************************************

**3. Чему равно математическое ожидание константы $M[C]$?**

[[ ]] 0
[[ ]] 1
[[X]] $C$
[[ ]] $\infty$
[[?]] Константа принимает значение $C$ с вероятностью 1.
**************************************************
$M[C] = C \cdot 1 = C$.
**************************************************

**4. Свойство линейности математического ожидания:**

[[ ]] $M[X+Y] = M[X] \cdot M[Y]$
[[X]] $M[aX + b] = a M[X] + b$
[[ ]] $M[X^2] = (M[X])^2$
[[ ]] $M[1/X] = 1/M[X]$
[[?]] Константа выносится, сдвиг сохраняется.
**************************************************
Математическое ожидание — линейный оператор.
**************************************************

**5. Дисперсия $D[X]$ характеризует:**

[[ ]] Среднее значение.
[[ ]] Асимметрию распределения.
[[X]] Меру рассеяния (разброса) значений вокруг математического ожидания.
[[ ]] Вероятность ошибки.
[[?]] Насколько сильно значения отклоняются от центра.
**************************************************
Это математическое ожидание квадрата отклонения от среднего.
**************************************************

**6. Формула для вычисления дисперсии (расчетная):**

[[ ]] $M[X^2] - M[X]$
[[X]] $M[X^2] - (M[X])^2$
[[ ]] $(M[X])^2 - M[X^2]$
[[ ]] $\sum x_i^2 p_i$
[[?]] Средний квадрат минус квадрат среднего.
**************************************************
$D[X] = \sum x_i^2 p_i - (\sum x_i p_i)^2$.
**************************************************

**7. Дисперсия постоянной величины $D[C]$ равна:**

[[ ]] $C$
[[ ]] $C^2$
[[X]] 0
[[ ]] 1
[[?]] Есть ли разброс у константы?
**************************************************
Константа не имеет разброса, она всегда принимает одно и то же значение.
**************************************************

**8. Как меняется дисперсия при умножении случайной величины на число $a$ ($D[aX]$)?**

[[ ]] $a D[X]$
[[ ]] $|a| D[X]$
[[X]] $a^2 D[X]$
[[ ]] $D[X] + a$
[[?]] Размерность дисперсии — квадрат величины.
**************************************************
Константа выносится из-под знака дисперсии в квадрате.
**************************************************

**9. Среднее квадратическое отклонение $\sigma[X]$ равно:**

[[ ]] $D[X]^2$
[[X]] $\sqrt{D[X]}$
[[ ]] $|M[X]|$
[[ ]] $D[X] / n$
[[?]] Чтобы вернуть размерность к исходной величине.
**************************************************
Корень квадратный из дисперсии.
**************************************************

**10. Если $X$ и $Y$ независимы, то $D[X+Y]$ равно:**

[[ ]] $D[X] + D[Y] + 2M[XY]$
[[X]] $D[X] + D[Y]$
[[ ]] $D[X] - D[Y]$
[[ ]] $\sqrt{D[X] + D[Y]}$
[[?]] Дисперсия суммы независимых величин аддитивна.
**************************************************
Для независимых величин ковариация равна 0, поэтому дисперсии просто складываются.
**************************************************

**11. Чему равно $D[X-Y]$ для независимых величин?**

[[ ]] $D[X] - D[Y]$
[[X]] $D[X] + D[Y]$
[[ ]] $D[X] - D[Y] + 2\text{cov}$
[[ ]] 0
[[?]] $D[-Y] = (-1)^2 D[Y] = D[Y]$.
**************************************************
Разброс при вычитании независимых величин также складывается (неопределенность растет).
**************************************************

**12. Математическое ожидание числа успехов в распределении Бернулли (1 опыт):**

[[ ]] $np$
[[X]] $p$
[[ ]] $pq$
[[ ]] $1$
[[?]] $1 \cdot p + 0 \cdot q$.
**************************************************
$M[X] = p$.
**************************************************

**13. Дисперсия числа успехов в распределении Бернулли:**

[[ ]] $p^2$
[[ ]] $p(1+p)$
[[X]] $pq$ (или $p(1-p)$)
[[ ]] $\sqrt{pq}$
[[?]] $M[X^2] - (M[X])^2 = p - p^2$.
**************************************************
$D[X] = p(1-p)$. Максимальна при $p=0.5$.
**************************************************

**14. Если сдвинуть все значения случайной величины на константу $C$ ($Y = X + C$), то дисперсия:**

[[ ]] Увеличится на $C$.
[[ ]] Увеличится на $C^2$.
[[X]] Не изменится.
[[ ]] Станет равна $C$.
[[?]] Сдвиг графика влево-вправо не меняет его ширину.
**************************************************
$D[X+C] = D[X]$. Разброс относительно центра остается тем же.
**************************************************

**15. Может ли математическое ожидание быть дробным числом, если сама величина принимает только целые значения?**

[[X]] Да, конечно.
[[ ]] Нет, только целым.
[[ ]] Только для непрерывных величин.
[[ ]] Нет, это невозможное событие.
[[?]] Пример: среднее число детей в семье (2.5 ребенка).
**************************************************
Мат. ожидание — это теоретический центр («центр масс»), оно не обязано совпадать с возможными значениями.
**************************************************

# Тема 18: Производящая функция для ДСВ

**1. Как определяется производящая функция $G_X(z)$ для ДСВ с неотрицательными целыми значениями?**

[[ ]] $\sum p_k e^{zk}$
[[X]] $\sum_{k=0}^\infty p_k z^k$
[[ ]] $\sum p_k z^{-k}$
[[ ]] $\int p(x) z^x dx$
[[?]] Степенной ряд, где коэффициенты — вероятности.
**************************************************
Это ряд, где вероятность $p_k$ стоит при $z$ в степени $k$.
**************************************************

**2. Чему равно значение производящей функции в точке $z=1$ ($G_X(1)$)?**

[[ ]] $M[X]$
[[ ]] 0
[[X]] 1
[[ ]] $D[X]$
[[?]] $\sum p_k \cdot 1^k$.
**************************************************
Сумма всех вероятностей равна 1 (условие нормировки).
**************************************************

**3. Первая производная производящей функции в точке $z=1$ ($G'(1)$) равна:**

[[ ]] Вероятности $p_1$.
[[X]] Математическому ожиданию $M[X]$.
[[ ]] Дисперсии $D[X]$.
[[ ]] $M[X^2]$.
[[?]] $(p_0 + p_1 z + p_2 z^2 + ...)' = p_1 + 2p_2 z + ...$ при $z=1$.
**************************************************
$G'(1) = \sum k p_k = M[X]$.
**************************************************

**4. Вторая производная $G''(1)$ равна:**

[[ ]] $D[X]$
[[ ]] $M[X^2]$
[[X]] $M[X(X-1)] = M[X^2] - M[X]$
[[ ]] $M[X]^2$
[[?]] Второй факториальный момент.
**************************************************
$G''(1) = \sum k(k-1)p_k$.
**************************************************

**5. Формула для вычисления дисперсии через производящую функцию:**

[[ ]] $G''(1) - (G'(1))^2$
[[X]] $G''(1) + G'(1) - (G'(1))^2$
[[ ]] $G'(1) + G''(1)$
[[ ]] $G(0)$
[[?]] $M[X^2] - (M[X])^2$ через факториальные моменты.
**************************************************
Нужно выразить $M[X^2]$ через $G''(1)$ и $G'(1)$.
**************************************************

**6. Производящая функция для распределения Бернулли ($q$ — неудача, $p$ — успех):**

[[ ]] $p + qz$
[[X]] $q + pz$
[[ ]] $z(p+q)$
[[ ]] $e^{pz}$
[[?]] $P(0)z^0 + P(1)z^1$.
**************************************************
$q \cdot 1 + p \cdot z = q + pz$.
**************************************************

**7. Производящая функция биномиального распределения $B(n, p)$:**

[[ ]] $(q + pz)^2$
[[X]] $(q + pz)^n$
[[ ]] $q^n + p^n z^n$
[[ ]] $n(q+pz)$
[[?]] Произведение $n$ функций Бернулли.
**************************************************
Сумме независимых величин соответствует произведение их производящих функций.
**************************************************

**8. Производящая функция распределения Пуассона:**

[[ ]] $e^{\lambda z}$
[[X]] $e^{\lambda (z-1)}$
[[ ]] $\lambda z$
[[ ]] $\frac{1}{1-\lambda z}$
[[?]] Ряд Тейлора для экспоненты.
**************************************************
$\sum \frac{\lambda^k e^{-\lambda}}{k!} z^k = e^{-\lambda} e^{\lambda z} = e^{\lambda(z-1)}$.
**************************************************

**9. Если $X$ и $Y$ независимы, то производящая функция их суммы $Z = X+Y$ равна:**

[[ ]] $G_X(z) + G_Y(z)$
[[X]] $G_X(z) \cdot G_Y(z)$
[[ ]] $G_X(G_Y(z))$
[[ ]] $G_X(z) / G_Y(z)$
[[?]] Свертке последовательностей соответствует умножение полиномов.
**************************************************
Это ключевое свойство, упрощающее работу с суммами величин.
**************************************************

**10. Коэффициент при $z^k$ в разложении $G_X(z)$ в ряд Маклорена равен:**

[[ ]] $F(k)$
[[ ]] $M[X^k]$
[[X]] $P(X=k)$
[[ ]] $k!$
[[?]] Определение производящей функции.
**************************************************
Производящая функция «хранит» вероятности $p_k$ как коэффициенты степенного ряда.
**************************************************

**11. Производящая функция геометрического распределения (число попыток до первого успеха):**

[[ ]] $\frac{p}{1-qz}$
[[X]] $\frac{pz}{1-qz}$
[[ ]] $pz$
[[ ]] $\frac{1}{1-z}$
[[?]] Сумма геометрической прогрессии, начинающейся с $pz$.
**************************************************
$G(z) = \sum_{k=1}^\infty q^{k-1}p z^k = \frac{pz}{1-qz}$.
**************************************************

**12. Для чего в основном используются производящие функции в теории вероятностей?**

[[ ]] Для визуализации данных.
[[X]] Для упрощения вычисления моментов и работы с суммами независимых величин.
[[ ]] Для вычисления медианы.
[[ ]] Для проверки гипотез.
[[?]] Дифференцировать функцию проще, чем суммировать сложные ряды.
**************************************************
Это мощный аналитический инструмент комбинаторики и тервера.
**************************************************

**13. Чему равно $G_X(0)$?**

[[X]] $P(X=0)$
[[ ]] 0
[[ ]] 1
[[ ]] $M[X]$
[[?]] Подставьте $z=0$ в ряд. Все члены с $z$ исчезнут.
**************************************************
Останется только свободный член $p_0$.
**************************************************

**14. Область сходимости производящей функции вероятностей всегда включает:**

[[ ]] Всю комплексную плоскость.
[[X]] Единичный круг $|z| \le 1$.
[[ ]] Только положительную полуось.
[[ ]] Только точку 1.
[[?]] Коэффициенты $p_k$ ограничены и суммируемы.
**************************************************
Так как $\sum p_k = 1$, степенной ряд сходится как минимум при $|z| \le 1$.
**************************************************

**15. Какое распределение имеет производящую функцию $G(z) = z^C$ (где $C$ — целое число)?**

[[ ]] Равномерное.
[[ ]] Бернулли.
[[X]] Вырожденное (константа $C$).
[[ ]] Биномиальное.
[[?]] Только один член ряда отличен от нуля.
**************************************************
Вероятность $P(X=C) = 1$.
**************************************************

# Тема 19: Числовые характеристики НСВ

**1. Математическое ожидание непрерывной случайной величины вычисляется как:**

[[ ]] $\sum x_i p_i$
[[X]] $\int_{-\infty}^{+\infty} x f(x) dx$
[[ ]] $\int f(x) dx$
[[ ]] Значение в середине отрезка.
[[?]] Аналог суммы для непрерывного случая.
**************************************************
Интеграл от произведения значения $x$ на плотность вероятности в этой точке.
**************************************************

**2. Дисперсия НСВ вычисляется по формуле:**

[[ ]] $\int x^2 f(x) dx$
[[X]] $\int_{-\infty}^{+\infty} (x - M[X])^2 f(x) dx$
[[ ]] $\int (x - M[X]) f(x) dx$
[[ ]] $\sqrt{D[X]}$
[[?]] Мат. ожидание квадрата отклонения.
**************************************************
Интеграл взвешенных квадратов отклонений от среднего.
**************************************************

**3. Расчетная формула для дисперсии НСВ:**

[[ ]] $M[X^2] - M[X]$
[[X]] $\int_{-\infty}^{+\infty} x^2 f(x) dx - (M[X])^2$
[[ ]] $(M[X])^2 - \int x^2 f(x) dx$
[[ ]] $\int x f(x^2) dx$
[[?]] Второй начальный момент минус квадрат первого.
**************************************************
$D[X] = \alpha_2 - \alpha_1^2$.
**************************************************

**4. Мода непрерывной случайной величины — это:**

[[ ]] Среднее значение.
[[X]] Точка локального максимума плотности вероятности $f(x)$.
[[ ]] Точка, где $F(x) = 0.5$.
[[ ]] Середина интервала.
[[?]] Самое «частое» (плотное) значение.
**************************************************
Значение $x$, при котором $f(x)$ принимает наибольшее значение.
**************************************************

**5. Медиана НСВ — это точка $Me$, для которой:**

[[ ]] $f(Me) = 0.5$
[[X]] $F(Me) = 0.5$
[[ ]] $f'(Me) = 0$
[[ ]] $Me = M[X]$
[[?]] Делит площадь под графиком пополам.
**************************************************
Вероятность попасть левее и правее медианы одинакова и равна 0.5.
**************************************************

**6. Для симметричного распределения (например, нормального):**

[[ ]] $M[X] > Me$
[[ ]] $M[X] < Me$
[[X]] $M[X] = Me = Mo$ (если мода единственная).
[[ ]] Связи нет.
[[?]] Центр симметрии.
**************************************************
В симметричных унимодальных распределениях среднее, медиана и мода совпадают.
**************************************************

**7. Если $f(x) = 0.5$ на отрезке $[0, 2]$, чему равно $M[X]$?**

[[ ]] 0.5
[[X]] 1
[[ ]] 2
[[ ]] 0
[[?]] Центр отрезка.
**************************************************
Для равномерного распределения среднее — это середина интервала.
**************************************************

**8. Начальный момент $k$-го порядка $\alpha_k$ определяется как:**

[[X]] $\int_{-\infty}^{+\infty} x^k f(x) dx$
[[ ]] $\int_{-\infty}^{+\infty} (x-M[X])^k f(x) dx$
[[ ]] $(M[X])^k$
[[ ]] Производная $k$-го порядка.
[[?]] Мат. ожидание $k$-й степени величины.
**************************************************
Это $M[X^k]$. При $k=1$ это мат. ожидание.
**************************************************

**9. Центральный момент $k$-го порядка $\mu_k$ (или $\beta_k$) определяется как:**

[[ ]] $M[X^k]$
[[X]] $\int_{-\infty}^{+\infty} (x - M[X])^k f(x) dx$
[[ ]] $D[X]^k$
[[ ]] $M[X^k] - (M[X])^k$
[[?]] Момент относительно центра тяжести (среднего).
**************************************************
При $k=2$ это дисперсия. При $k=1$ он всегда равен 0.
**************************************************

**10. Коэффициент асимметрии ($As$) характеризует:**

[[ ]] Островершинность распределения.
[[X]] Степень скошенности распределения относительно среднего.
[[ ]] Ширину распределения.
[[ ]] Среднее значение.
[[?]] Связан с 3-м центральным моментом.
**************************************************
Если у распределения длинный «хвост» справа, асимметрия положительна.
**************************************************

**11. Коэффициент эксцесса ($Ex$ или $Es$) характеризует:**

[[ ]] Асимметрию.
[[X]] Островершинность (крутость пика) по сравнению с нормальным распределением.
[[ ]] Масштаб.
[[ ]] Сдвиг.
[[?]] Связан с 4-м центральным моментом.
**************************************************
Для нормального распределения $Ex=0$. Более острый пик — $Ex>0$.
**************************************************

**12. Чему равен центральный момент первого порядка $\mu_1$?**

[[ ]] $M[X]$
[[ ]] 1
[[X]] 0
[[ ]] $\infty$
[[?]] $M[X - M[X]]$.
**************************************************
Среднее отклонение от среднего всегда равно нулю.
**************************************************

**13. Квантиль порядка $p$ ($x_p$) для НСВ находится из уравнения:**

[[ ]] $f(x_p) = p$
[[X]] $\int_{-\infty}^{x_p} f(t) dt = p$
[[ ]] $x_p = M[X] \cdot p$
[[ ]] $x_p = 1-p$
[[?]] Площадь слева от квантиля равна $p$.
**************************************************
Это значение, ниже которого величина попадает с вероятностью $p$.
**************************************************

**14. Может ли $M[X]$ для НСВ не существовать?**

[[X]] Да, если интеграл $\int |x|f(x)dx$ расходится.
[[ ]] Нет, всегда существует.
[[ ]] Только если $X$ отрицательна.
[[ ]] Только для ограниченных величин.
[[?]] Пример: распределение Коши.
**************************************************
Если «хвосты» распределения убывают слишком медленно, мат. ожидание может быть бесконечным или неопределенным.
**************************************************

**15. Если $f(x) = 2x$ на $[0, 1]$, то $M[X^2]$ равно:**

[[ ]] $1/3$
[[X]] $1/2$
[[ ]] $2/3$
[[ ]] $1/4$
[[?]] $\int_0^1 x^2 \cdot 2x dx$.
**************************************************
$\int_0^1 2x^3 dx = 2 \cdot [x^4/4]_0^1 = 2 \cdot 1/4 = 0.5$.
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 20, 21 и 22) в формате LiaScript.

# Тема 20: Равномерный закон распределения

**1. Плотность вероятности $f(x)$ равномерного распределения на отрезке $[a, b]$ имеет вид:**

[[ ]] Колокола.
[[X]] Прямоугольника ("ступеньки").
[[ ]] Треугольника.
[[ ]] Экспоненты.
[[?]] $f(x) = const$.
**************************************************
Плотность постоянна на отрезке и равна нулю вне его.
**************************************************

**2. Чему равна высота плотности $f(x)$ на интервале $[a, b]$?**

[[ ]] $b-a$
[[X]] $\frac{1}{b-a}$
[[ ]] 1
[[ ]] $\frac{a+b}{2}$
[[?]] Площадь прямоугольника должна быть равна 1.
**************************************************
$C \cdot (b-a) = 1 \Rightarrow C = \frac{1}{b-a}$.
**************************************************

**3. Математическое ожидание $M[X]$ равномерного распределения равно:**

[[ ]] $b-a$
[[ ]] $\sqrt{ab}$
[[X]] $\frac{a+b}{2}$
[[ ]] $\frac{(b-a)^2}{12}$
[[?]] Середина отрезка.
**************************************************
В силу симметрии распределения, среднее значение находится в центре интервала.
**************************************************

**4. Дисперсия равномерного распределения $D[X]$ вычисляется по формуле:**

[[ ]] $\frac{a+b}{2}$
[[X]] $\frac{(b-a)^2}{12}$
[[ ]] $\frac{(b-a)^2}{2}$
[[ ]] $b-a$
[[?]] Запомните число 12 в знаменателе.
**************************************************
Это результат вычисления интеграла $\int_a^b (x - \frac{a+b}{2})^2 \frac{1}{b-a} dx$.
**************************************************

**5. Функция распределения $F(x)$ на интервале $[a, b]$ для равномерного закона:**

[[X]] Линейно возрастает: $\frac{x-a}{b-a}$.
[[ ]] Постоянна.
[[ ]] Растет как парабола.
[[ ]] Равна 1.
[[?]] Интеграл от константы.
**************************************************
Накопленная вероятность растет пропорционально продвижению $x$ от $a$ к $b$.
**************************************************

**6. Вероятность попадания равномерной СВ в подотрезок $[\alpha, \beta] \subset [a, b]$ равна:**

[[ ]] $\beta - \alpha$
[[X]] $\frac{\beta - \alpha}{b - a}$
[[ ]] $\frac{b - a}{\beta - \alpha}$
[[ ]] $0.5$
[[?]] Отношение длин.
**************************************************
Это классическое геометрическое определение вероятности для одномерного случая (длина части к длине целого).
**************************************************

**7. Если $X \sim R[0, 10]$, то вероятность попадания в интервал $(2, 4)$ равна:**

[[X]] $0.2$
[[ ]] $0.4$
[[ ]] $0.1$
[[ ]] $2$
[[?]] Длина интервала 2, длина всего отрезка 10.
**************************************************
$P = \frac{4-2}{10-0} = \frac{2}{10} = 0.2$.
**************************************************

**8. Равномерное распределение часто используется для моделирования:**

[[ ]] Роста людей.
[[ ]] Времени до отказа прибора.
[[X]] Ошибок округления или времени ожидания при фиксированном интервале движения транспорта.
[[ ]] Количества звонков.
[[?]] Когда "все исходы равновозможны".
**************************************************
Это "распределение незнания" в ограниченном диапазоне.
**************************************************

**9. Медиана $Me$ равномерного распределения:**

[[ ]] Меньше $M[X]$.
[[X]] Равна $M[X]$.
[[ ]] Больше $M[X]$.
[[ ]] Равна $a$.
[[?]] Распределение симметрично.
**************************************************
Середина отрезка делит вероятность пополам.
**************************************************

**10. Чему равен коэффициент эксцесса ($Ex$) равномерного распределения?**

[[ ]] 0
[[ ]] 3
[[X]] $-1.2$ (отрицательный).
[[ ]] 1
[[?]] График "приплюснут" по сравнению с нормальным (у которого $Ex=0$).
**************************************************
Равномерное распределение более плосковершинное, чем нормальное, поэтому эксцесс отрицательный.
**************************************************

**11. Если $f(x) = 0.5$ на интервале $[c, d]$, то какова длина интервала?**

[[ ]] 0.5
[[X]] 2
[[ ]] 1
[[ ]] 4
[[?]] Высота $1/L = 0.5$.
**************************************************
$L = 1/0.5 = 2$.
**************************************************

**12. Что происходит с $F(x)$ при $x > b$?**

[[ ]] Она убывает.
[[ ]] Она растет.
[[X]] Она равна 1.
[[ ]] Она равна 0.
[[?]] Вся вероятность накоплена.
**************************************************
Правее интервала событие становится достоверным.
**************************************************

**13. Стандартное равномерное распределение обозначается $U[0, 1]$. Его дисперсия равна:**

[[ ]] 1
[[X]] $1/12$
[[ ]] $1/2$
[[ ]] 0
[[?]] Формула $\frac{(1-0)^2}{12}$.
**************************************************
$D[X] = 1/12 \approx 0.083$.
**************************************************

**14. Может ли плотность равномерного распределения быть больше 1?**

[[X]] Да, если длина отрезка меньше 1.
[[ ]] Нет, никогда.
[[ ]] Только если $a < 0$.
[[ ]] Только в точке $M[X]$.
[[?]] Если $b-a = 0.1$, то $f(x) = 10$.
**************************************************
Высота прямоугольника обратно пропорциональна ширине.
**************************************************

**15. Если автобусы ходят строго раз в 10 минут, а пассажир приходит случайно, то время ожидания распределено:**

[[ ]] Нормально.
[[ ]] Показательно.
[[X]] Равномерно на $[0, 10]$.
[[ ]] По Пуассону.
[[?]] Любое время ожидания равновозможно.
**************************************************
Классический пример равномерного закона.
**************************************************

# Тема 21: Показательный (экспоненциальный) закон

**1. Плотность вероятности показательного распределения (при $x \ge 0$):**

[[ ]] $e^{-\lambda x}$
[[X]] $\lambda e^{-\lambda x}$
[[ ]] $1 - e^{-\lambda x}$
[[ ]] $\lambda x$
[[?]] Не забудьте множитель $\lambda$ для нормировки.
**************************************************
$f(x) = \lambda e^{-\lambda x}$.
**************************************************

**2. Параметр $\lambda$ в показательном распределении имеет смысл:**

[[ ]] Среднего времени.
[[ ]] Дисперсии.
[[X]] Интенсивности (среднего числа событий в единицу времени).
[[ ]] Вероятности успеха.
[[?]] Обратная величина к среднему времени.
**************************************************
$\lambda$ — это скорость потока событий (например, отказов в час).
**************************************************

**3. Функция распределения $F(x)$ показательного закона ($x \ge 0$):**

[[ ]] $e^{-\lambda x}$
[[X]] $1 - e^{-\lambda x}$
[[ ]] $\lambda e^{-\lambda x}$
[[ ]] $1 - \lambda x$
[[?]] Интеграл от плотности. В нуле должно быть 0, в бесконечности 1.
**************************************************
$F(x) = P(X < x) = 1 - e^{-\lambda x}$.
**************************************************

**4. Математическое ожидание показательного распределения:**

[[ ]] $\lambda$
[[X]] $1/\lambda$
[[ ]] $1/\lambda^2$
[[ ]] $\ln \lambda$
[[?]] Если интенсивность 2 события в час, то среднее время между ними...
**************************************************
$M[X] = 1/\lambda$. Например, 0.5 часа.
**************************************************

**5. Дисперсия показательного распределения:**

[[ ]] $1/\lambda$
[[ ]] $\lambda^2$
[[X]] $1/\lambda^2$
[[ ]] $2/\lambda^2$
[[?]] Квадрат среднего.
**************************************************
Для показательного закона $D[X] = (M[X])^2 = 1/\lambda^2$.
**************************************************

**6. Уникальное свойство показательного распределения:**

[[ ]] Симметричность.
[[X]] Отсутствие последействия («отсутствие памяти»).
[[ ]] Ограниченность значений.
[[ ]] Равенство моды и медианы.
[[?]] $P(X > t+s | X > s) = P(X > t)$.
**************************************************
Вероятность проработать еще время $t$ не зависит от того, сколько прибор уже проработал. «Прибор не стареет».
**************************************************

**7. Среднее квадратическое отклонение $\sigma[X]$ равно:**

[[ ]] Дисперсии.
[[X]] Математическому ожиданию ($1/\lambda$).
[[ ]] $\lambda$.
[[ ]] 0.
[[?]] Корень из $1/\lambda^2$.
**************************************************
У показательного закона $\sigma = M[X]$.
**************************************************

**8. Функция надежности $R(t) = P(X > t)$ (вероятность безотказной работы за время $t$):**

[[ ]] $1 - e^{-\lambda t}$
[[X]] $e^{-\lambda t}$
[[ ]] $\lambda t$
[[ ]] $1/\lambda$
[[?]] $1 - F(t)$.
**************************************************
Это вероятность того, что время жизни $X$ превысит $t$.
**************************************************

**9. Где находится мода показательного распределения?**

[[ ]] В точке $1/\lambda$.
[[X]] В точке $0$ (при $x \ge 0$).
[[ ]] В бесконечности.
[[ ]] Распределение не имеет моды.
[[?]] Где функция $\lambda e^{-\lambda x}$ максимальна?
**************************************************
Плотность монотонно убывает при $x>0$, максимум в нуле. Самые короткие интервалы — самые частые.
**************************************************

**10. Показательное распределение описывает:**

[[ ]] Ошибки измерений.
[[ ]] Размеры деталей.
[[X]] Время безотказной работы сложных систем или время между событиями в простейшем потоке.
[[ ]] Число выпадений орла.
[[?]] Непрерывный аналог геометрического распределения.
**************************************************
Оно моделирует длительность интервалов между случайными событиями.
**************************************************

**11. Медиана показательного распределения:**

[[ ]] Равна $1/\lambda$.
[[X]] Равна $(\ln 2) / \lambda \approx 0.693 M[X]$.
[[ ]] Меньше 0.
[[ ]] Равна 0.5.
[[?]] Решение уравнения $1 - e^{-\lambda x} = 0.5$.
**************************************************
Медиана меньше среднего значения (из-за длинного правого «хвоста»).
**************************************************

**12. Может ли $\lambda$ быть отрицательным?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Только если время течет вспять.
[[ ]] Да, если вероятность падает.
[[?]] Интенсивность и вероятность должны быть положительны.
**************************************************
$\lambda > 0$, иначе интеграл от плотности разойдется (будет бесконечным).
**************************************************

**13. Если среднее время обслуживания клиента 5 минут (показательный закон), то $\lambda$ равно:**

[[ ]] 5
[[X]] $1/5 = 0.2$ (клиента в минуту).
[[ ]] 25
[[ ]] $e^{-5}$
[[?]] $\lambda = 1/M[X]$.
**************************************************
Интенсивность обратна среднему времени.
**************************************************

**14. Как ведут себя «хвосты» показательного распределения?**

[[ ]] Обрываются резко.
[[X]] Убывают экспоненциально (быстро, но уходят в бесконечность).
[[ ]] Убывают степенным образом (медленно).
[[ ]] Растут.
[[?]] Функция $e^{-x}$.
**************************************************
Вероятность очень больших значений мала, но не равна нулю.
**************************************************

**15. Вероятность попадания в интервал $(0, M[X])$:**

[[ ]] 0.5
[[ ]] 0.68
[[X]] $1 - e^{-1} \approx 0.632$
[[ ]] 0.99
[[?]] $F(1/\lambda) = 1 - e^{-\lambda \cdot (1/\lambda)}$.
**************************************************
Вероятность того, что значение будет меньше среднего, составляет около 63%.
**************************************************

# Тема 22: Простейший поток событий

**1. Простейшим (пуассоновским) потоком событий называется поток, обладающий свойствами:**

[[ ]] Конечности, дискретности, определенности.
[[X]] Стационарности, ординарности, отсутствия последействия.
[[ ]] Нормальности, равномерности, независимости.
[[ ]] Симметричности, эксцесса, асимметрии.
[[?]] Три ключевых аксиомы.
**************************************************
Эти три свойства определяют пуассоновский процесс.
**************************************************

**2. Свойство стационарности потока означает:**

[[ ]] События не происходят.
[[X]] Вероятность появления $k$ событий зависит только от длины интервала, но не от его начала (интенсивность $\lambda = const$).
[[ ]] Поток останавливается со временем.
[[ ]] События происходят через равные промежутки.
[[?]] Режим работы стабилен во времени.
**************************************************
Средняя плотность событий постоянна во времени.
**************************************************

**3. Свойство отсутствия последействия означает:**

[[ ]] События не запоминаются.
[[X]] Вероятность событий в одном интервале не зависит от того, что происходило в других (непересекающихся) интервалах.
[[ ]] События происходят парами.
[[ ]] После одного события сразу следует другое.
[[?]] Взаимная независимость событий в разных промежутках времени.
**************************************************
Прошлое не влияет на будущее (вероятностно).
**************************************************

**4. Свойство ординарности означает:**

[[ ]] События обычные, не редкие.
[[X]] Вероятность появления двух и более событий за малый промежуток времени пренебрежимо мала (события идут поодиночке).
[[ ]] События выстроены в ряд.
[[ ]] Ордината графика равна 1.
[[?]] Невозможность одновременного появления.
**************************************************
События не происходят "пачками", в одной точке времени не более одного события.
**************************************************

**5. Число событий $K$, попадающих в интервал длины $t$, в простейшем потоке распределено по закону:**

[[ ]] Бернулли.
[[ ]] Показательному.
[[X]] Пуассона с параметром $a = \lambda t$.
[[ ]] Гаусса.
[[?]] Дискретное число событий за время.
**************************************************
$P_t(k) = \frac{(\lambda t)^k e^{-\lambda t}}{k!}$.
**************************************************

**6. Интервалы времени $T$ между соседними событиями в простейшем потоке распределены по закону:**

[[ ]] Равномерному.
[[X]] Показательному (экспоненциальному).
[[ ]] Нормальному.
[[ ]] Пуассона.
[[?]] Непрерывное время ожидания.
**************************************************
Это связь между дискретным счетом (Пуассон) и непрерывным временем (Экспонента).
**************************************************

**7. Если интенсивность потока $\lambda = 3$ события в час, то среднее число событий за 2 часа:**

[[ ]] 1.5
[[X]] 6
[[ ]] 9
[[ ]] $e^{-6}$
[[?]] $\lambda \cdot t$.
**************************************************
$M[X] = 3 \times 2 = 6$.
**************************************************

**8. Вероятность того, что за время $t$ не произойдет ни одного события ($k=0$):**

[[ ]] 0
[[ ]] 1
[[X]] $e^{-\lambda t}$
[[ ]] $\lambda t$
[[?]] Формула Пуассона при $k=0$.
**************************************************
Это также значение функции надежности для времени ожидания первого события.
**************************************************

**9. Является ли поток звонков в скорую помощь в течение суток простейшим?**

[[X]] Нет, так как нарушено свойство стационарности (ночью звонят реже).
[[ ]] Да, это классический пример.
[[ ]] Да, если город большой.
[[ ]] Нет, так как звонки зависимы.
[[?]] Меняется ли $\lambda$ со временем?
**************************************************
Интенсивность $\lambda(t)$ меняется в зависимости от времени суток, поток нестационарный (хотя локально может быть пуассоновским).
**************************************************

**10. Связь между параметром $\lambda$ потока и средним временем между событиями $\bar{T}$:**

[[ ]] $\lambda = \bar{T}$
[[X]] $\lambda = 1 / \bar{T}$
[[ ]] $\lambda = \bar{T}^2$
[[ ]] Нет связи.
[[?]] Частота и период.
**************************************************
Чем больше событий в час, тем меньше времени между ними.
**************************************************

**11. Если события происходят строго по расписанию (детерминировано), является ли этот поток простейшим?**

[[ ]] Да.
[[X]] Нет, нарушено отсутствие последействия.
[[ ]] Да, это предельный случай.
[[ ]] Зависит от расписания.
[[?]] Зная время прошлого события, мы точно знаем время будущего.
**************************************************
В детерминированном потоке события жестко зависимы ($P=1$ в нужный момент), что противоречит независимости.
**************************************************

**12. Интенсивность потока отказов $\lambda(t)$ для «стареющих» механизмов:**

[[ ]] Постоянна.
[[ ]] Убывает.
[[X]] Возрастает со временем.
[[ ]] Равна 0.
[[?]] Чем старее, тем чаще ломается.
**************************************************
Это уже не простейший поток, а поток с последействием (износ).
**************************************************

**13. Параметр $a$ в формуле Пуассона для потока безразмерен?**

[[X]] Да (количество штук).
[[ ]] Нет, измеряется в секундах.
[[ ]] Измеряется в $1/c$.
[[ ]] Зависит от $\lambda$.
[[?]] $\lambda$ ($1/c$) умножить на $t$ ($c$).
**************************************************
Это среднее *число* событий, безразмерная величина.
**************************************************

**14. Для простейшего потока вероятность появления события в малом интервале $\Delta t$ примерно равна:**

[[ ]] 1
[[ ]] 0
[[X]] $\lambda \Delta t$
[[ ]] $(\lambda \Delta t)^2$
[[?]] Линейная зависимость для малых $t$.
**************************************************
С точностью до бесконечно малых высшего порядка.
**************************************************

**15. Можно ли складывать независимые простейшие потоки?**

[[X]] Да, сумма простейших потоков — простейший поток с $\lambda = \lambda_1 + \lambda_2$.
[[ ]] Нет, структура разрушается.
[[ ]] Только если $\lambda_1 = \lambda_2$.
[[ ]] Получится нормальное распределение.
[[?]] Поток вызовов на пульт от двух районов.
**************************************************
Это свойство устойчивости пуассоновских потоков.
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 23, 24 и 25) в формате LiaScript.

# Тема 23: Нормальный закон распределения (Кривая Гаусса)

**1. График плотности вероятности нормального распределения называется:**

[[ ]] Параболой.
[[ ]] Гиперболой.
[[X]] Кривой Гаусса (гауссианой).
[[ ]] Синусоидой.
[[?]] Форма колокола.
**************************************************
Симметричная колоколообразная кривая.
**************************************************

**2. Нормальное распределение полностью определяется двумя параметрами:**

[[ ]] $n$ и $p$.
[[ ]] $\lambda$ и $t$.
[[X]] $a$ (мат. ожидание) и $\sigma$ (среднее квадратическое отклонение).
[[ ]] $a$ и $b$ (границы отрезка).
[[?]] Центр и ширина.
**************************************************
Обозначается $N(a, \sigma)$.
**************************************************

**3. График нормальной плотности симметричен относительно прямой:**

[[ ]] $x = 0$
[[X]] $x = a$
[[ ]] $x = \sigma$
[[ ]] $y = 1$
[[?]] Где находится вершина колокола?
**************************************************
Вершина графика и ось симметрии соответствуют математическому ожиданию $a$.
**************************************************

**4. Как влияет параметр $\sigma$ на форму кривой Гаусса?**

[[ ]] Сдвигает график вдоль оси X.
[[X]] Меняет высоту и ширину графика (чем больше $\sigma$, тем ниже и шире кривая).
[[ ]] Делает график асимметричным.
[[ ]] Не влияет на форму.
[[?]] $\sigma$ — мера разброса.
**************************************************
При увеличении $\sigma$ разброс растет, кривая «расплывается» и становится ниже (чтобы площадь осталась равной 1).
**************************************************

**5. Область определения нормальной случайной величины:**

[[ ]] $[0, +\infty)$
[[ ]] $[a-\sigma, a+\sigma]$
[[X]] $(-\infty, +\infty)$
[[ ]] $[0, 1]$
[[?]] «Хвосты» уходят в бесконечность.
**************************************************
Теоретически нормальная величина может принять любое действительное значение.
**************************************************

**6. Максимальное значение плотности вероятности нормального закона равно:**

[[ ]] 1
[[ ]] $a$
[[X]] $\frac{1}{\sigma \sqrt{2\pi}}$
[[ ]] 0.5
[[?]] Значение формулы при $x=a$ (экспонента равна 1).
**************************************************
Высота пика обратно пропорциональна $\sigma$.
**************************************************

**7. Асимптотой для графика нормальной плотности является:**

[[ ]] Прямая $x=a$.
[[X]] Ось абсцисс ($y=0$).
[[ ]] Прямая $y=1$.
[[ ]] Асимптот нет.
[[?]] К чему стремится график при $x \to \infty$?
**************************************************
Ветви кривой бесконечно приближаются к оси X, но не пересекают её.
**************************************************

**8. Точки перегиба кривой Гаусса находятся на расстоянии:**

[[ ]] $3\sigma$ от центра.
[[X]] $\sigma$ от центра (в точках $a-\sigma$ и $a+\sigma$).
[[ ]] $a$ от нуля.
[[ ]] Перегибов нет.
[[?]] Где выпуклость меняется на вогнутость?
**************************************************
Вторая производная плотности обращается в ноль при $x = a \pm \sigma$.
**************************************************

**9. Вероятность попадания нормальной СВ в интервал $(\alpha, \beta)$ вычисляется через:**

[[ ]] Прямое интегрирование экспоненты.
[[X]] Разность функций Лапласа $\Phi(\frac{\beta-a}{\sigma}) - \Phi(\frac{\alpha-a}{\sigma})$.
[[ ]] Произведение $\sigma \times (b-a)$.
[[ ]] Формулу Бернулли.
[[?]] Табличный интеграл.
**************************************************
Интеграл от плотности не берется в элементарных функциях, используются таблицы $\Phi(x)$.
**************************************************

**10. В формуле плотности $f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-...}$ в показателе степени стоит:**

[[ ]] $-x^2$
[[ ]] $-(x-a)$
[[X]] $-\frac{(x-a)^2}{2\sigma^2}$
[[ ]] $\frac{x-a}{\sigma}$
[[?]] Квадрат стандартизованной переменной пополам.
**************************************************
Это ядро нормального распределения.
**************************************************

**11. Если $a=0$, то график функции:**

[[ ]] Сдвинут вправо.
[[ ]] Лежит только справа от оси Y.
[[X]] Симметричен относительно оси ординат ($Oy$).
[[ ]] Вырождается в линию.
[[?]] Четная функция.
**************************************************
Центр распределения совпадает с началом координат.
**************************************************

**12. Почему нормальный закон так распространен в природе?**

[[ ]] Потому что он самый простой.
[[X]] Согласно Центральной предельной теореме, сумма многих независимых факторов распределена нормально.
[[ ]] Это совпадение.
[[ ]] Потому что его открыл Гаусс.
[[?]] Результат сложения множества мелких влияний.
**************************************************
ЦПТ объясняет универсальность нормального закона (ошибки, рост, вес).
**************************************************

**13. Интеграл $\int_{-\infty}^{+\infty} e^{-x^2/2} dx$ равен:**

[[ ]] 1
[[X]] $\sqrt{2\pi}$ (интеграл Пуассона).
[[ ]] 0
[[ ]] $\pi$
[[?]] Вспоминаем нормировочный множитель.
**************************************************
Чтобы площадь была 1, мы делим на этот интеграл ($\sqrt{2\pi}$).
**************************************************

**14. Может ли функция распределения $F(x)$ нормального закона быть равна 0?**

[[ ]] Да, при $x=a$.
[[ ]] Да, при $x=0$.
[[X]] Нет, она стремится к 0 только при $x \to -\infty$.
[[ ]] Да, если $\sigma \to 0$.
[[?]] Плотность везде положительна.
**************************************************
Строго говоря, $F(x) > 0$ для любого конечного $x$, так как «хвост» уходит в бесконечность.
**************************************************

**15. Если мы изменим $a$, не меняя $\sigma$, график:**

[[ ]] Изменит форму (станет шире).
[[ ]] Изменит высоту.
[[X]] Сдвинется вдоль оси X без изменения формы.
[[ ]] Перевернется.
[[?]] $a$ — параметр сдвига (location).
**************************************************
Колокол будет скользить по оси влево или вправо.
**************************************************

# Тема 24: Центрированная и нормированная СВ

**1. Центрированная случайная величина $\mathring{X}$ получается путем:**

[[X]] Вычитания математического ожидания ($X - M[X]$).
[[ ]] Деления на дисперсию.
[[ ]] Деления на мат. ожидание.
[[ ]] Взятия модуля.
[[?]] Сдвиг центра в ноль.
**************************************************
Центрирование переносит начало отсчета в центр распределения.
**************************************************

**2. Математическое ожидание центрированной величины всегда равно:**

[[ ]] $M[X]$
[[ ]] 1
[[X]] 0
[[ ]] $\sigma$
[[?]] $M[X - M[X]] = M[X] - M[X]$.
**************************************************
Среднее отклонение от среднего равно нулю.
**************************************************

**3. Стандартизованная (нормированная) случайная величина $Z$ получается формулой:**

[[ ]] $X - a$
[[ ]] $X / \sigma$
[[X]] $\frac{X - a}{\sigma}$
[[ ]] $\frac{X - a}{\sigma^2}$
[[?]] Сначала центрируем, потом нормируем (масштабируем).
**************************************************
Это $z$-score: отклонение от среднего, выраженное в единицах стандартного отклонения.
**************************************************

**4. Дисперсия стандартизованной случайной величины $Z$ равна:**

[[ ]] $\sigma^2$
[[ ]] 0
[[X]] 1
[[ ]] $\sigma$
[[?]] $D[X/\sigma] = \frac{1}{\sigma^2} D[X]$.
**************************************************
$\frac{1}{\sigma^2} \cdot \sigma^2 = 1$. Масштаб приведен к единичному.
**************************************************

**5. Стандартное нормальное распределение обозначается:**

[[ ]] $N(a, \sigma)$
[[ ]] $N(1, 0)$
[[X]] $N(0, 1)$
[[ ]] $U[0, 1]$
[[?]] $a=0, \sigma=1$.
**************************************************
Это частный случай нормального закона с нулевым средним и единичной дисперсией.
**************************************************

**6. Зачем нужна стандартизация нормальной величины?**

[[ ]] Чтобы увеличить точность.
[[ ]] Чтобы изменить вид распределения.
[[X]] Чтобы свести произвольный нормальный закон к стандартному и пользоваться одной таблицей.
[[ ]] Чтобы убрать отрицательные значения.
[[?]] Таблицы составлены только для $N(0,1)$.
**************************************************
Интеграл не берется, а составлять бесконечно много таблиц для всех $a$ и $\sigma$ невозможно.
**************************************************

**7. Плотность стандартного нормального распределения обозначается $\varphi(x)$ и равна:**

[[ ]] $e^{-x^2}$
[[X]] $\frac{1}{\sqrt{2\pi}} e^{-x^2/2}$
[[ ]] $\frac{1}{\sigma} e^{-(x-a)^2}$
[[ ]] $\sin x$
[[?]] Формула Гаусса при $a=0, \sigma=1$.
**************************************************
Это канонический вид плотности вероятности.
**************************************************

**8. Функция $\varphi(x)$ (стандартная плотность):**

[[ ]] Нечетная.
[[X]] Четная ($\varphi(-x) = \varphi(x)$).
[[ ]] Периодическая.
[[ ]] Монотонная.
[[?]] График симметричен относительно $Oy$.
**************************************************
$e^{-(-x)^2/2} = e^{-x^2/2}$.
**************************************************

**9. Значение $\varphi(0)$ (максимум стандартной плотности) примерно равно:**

[[ ]] 1
[[ ]] 0.5
[[X]] $0.3989$ ($1/\sqrt{2\pi}$)
[[ ]] 0
[[?]] $1 / 2.5$.
**************************************************
$\frac{1}{\sqrt{6.28}} \approx 0.4$.
**************************************************

**10. Единица измерения стандартизованной величины $Z$:**

[[ ]] Метры (как у $X$).
[[ ]] Квадратные метры.
[[X]] Безразмерная величина.
[[ ]] Проценты.
[[?]] Метры делим на метры (отклонение на сигму).
**************************************************
Это относительная величина ("сколько сигм").
**************************************************

**11. Если $X \sim N(10, 2)$, а мы получили значение $x=14$, то $z$-значение равно:**

[[ ]] 4
[[X]] 2
[[ ]] 0.5
[[ ]] 14
[[?]] $(14-10)/2$.
**************************************************
Значение отклонилось на 2 сигмы вправо.
**************************************************

**12. Дисперсия центрированной (но не нормированной) величины $\mathring{X}$:**

[[ ]] 1
[[ ]] 0
[[X]] Равна дисперсии исходной величины $D[X]$.
[[ ]] Изменяется на величину $a$.
[[?]] Сдвиг графика не меняет его ширину.
**************************************************
$D[X-a] = D[X]$.
**************************************************

**13. Функция Лапласа $\Phi(x)$, используемая в таблицах, обычно является интегралом от $\varphi(t)$ в пределах:**

[[ ]] $(-\infty, +\infty)$
[[X]] $(0, x)$ (для отечественной литературы).
[[ ]] $(-\infty, x)$ (для западной литературы).
[[ ]] $(x, +\infty)$
[[?]] Зависит от соглашения, но часто нормируют так, чтобы $\Phi(0)=0$.
**************************************************
В данном курсе используется определение $\int_0^x$.
**************************************************

**14. Если случайная величина равномерно распределена, можно ли её стандартизировать?**

[[X]] Да, процедура $Z = (X-M)/\sigma$ применима к любому распределению.
[[ ]] Нет, только нормальную.
[[ ]] Да, но дисперсия не станет равна 1.
[[ ]] Нет, у равномерной нет $M[X]$.
[[?]] Мат. ожидание и дисперсия существуют.
**************************************************
Стандартизация — универсальная операция, просто для других законов $Z$ не станет нормальной, но будет иметь $M=0, D=1$.
**************************************************

**15. Какова связь между $\Phi(-x)$ и $\Phi(x)$ (для интеграла от 0 до $x$)?**

[[ ]] Они равны.
[[X]] $\Phi(-x) = -\Phi(x)$.
[[ ]] $\Phi(-x) = 1 - \Phi(x)$.
[[ ]] $\Phi(-x) = 0$.
[[?]] Интеграл от четной функции по несимметричному интервалу.
**************************************************
Функция Лапласа нечетна (площадь от 0 до $-x$ берется со знаком минус, так как идем "назад").
**************************************************

# Тема 25: Числовые характеристики и правило 3 сигм

**1. Математическое ожидание нормального распределения $N(a, \sigma)$:**

[[ ]] $\sigma$
[[ ]] 0
[[X]] $a$
[[ ]] $a^2$
[[?]] Центр симметрии.
**************************************************
Параметр $a$ в формуле плотности является мат. ожиданием.
**************************************************

**2. Дисперсия нормального распределения:**

[[ ]] $\sigma$
[[X]] $\sigma^2$
[[ ]] $\sqrt{\sigma}$
[[ ]] $a$
[[?]] Параметр ширины в квадрате.
**************************************************
Параметр $\sigma$ — это СКО, дисперсия — его квадрат.
**************************************************

**3. Мода и медиана нормального распределения:**

[[ ]] Различны.
[[ ]] $Mo=0, Me=1$.
[[X]] Совпадают с математическим ожиданием $a$.
[[ ]] Не существуют.
[[?]] Из-за симметрии и единственного пика.
**************************************************
Для симметричного унимодального распределения $Mean = Median = Mode$.
**************************************************

**4. Коэффициент асимметрии ($As$) нормального распределения:**

[[ ]] 3
[[ ]] 1
[[X]] 0
[[ ]] Зависит от $\sigma$.
[[?]] График не скошен.
**************************************************
Идеальная симметрия.
**************************************************

**5. Коэффициент эксцесса ($Ex$) нормального распределения (по определению, принятому в статистике):**

[[ ]] 3
[[X]] 0 (так как вычитается 3).
[[ ]] 1
[[ ]] -1
[[?]] Нормальное распределение — эталон крутости вершины.
**************************************************
Эксцесс показывает отклонение островершинности от нормальной кривой. Для самой нормальной кривой он принят за 0.
**************************************************

**6. Вероятность попадания нормальной СВ в интервал $(a-\sigma, a+\sigma)$ примерно равна:**

[[ ]] 30%
[[ ]] 50%
[[X]] 68%
[[ ]] 95%
[[?]] "Правило одной сигмы".
**************************************************
Точное значение $2\Phi(1) \approx 0.6826$.
**************************************************

**7. Вероятность попадания в интервал $(a-2\sigma, a+2\sigma)$:**

[[ ]] 90%
[[X]] 95% (точнее 95.44%)
[[ ]] 99%
[[ ]] 68%
[[?]] "Правило двух сигм".
**************************************************
Почти вся масса вероятности, кроме 5%.
**************************************************

**8. Правило трех сигм гласит, что:**

[[ ]] Величина никогда не выйдет за $3\sigma$.
[[X]] Вероятность отклонения от среднего более чем на $3\sigma$ пренебрежимо мала (около 0.3%).
[[ ]] Вероятность попадания в $3\sigma$ равна 100%.
[[ ]] Дисперсия равна $3\sigma$.
[[?]] Практическая достоверность.
**************************************************
Событие $|X-a| < 3\sigma$ считается практически достоверным ($P \approx 0.9973$).
**************************************************

**9. Вероятность того, что значение выйдет за пределы $3\sigma$ (попадет в "хвосты"):**

[[ ]] 0
[[ ]] 0.05
[[X]] 0.0027
[[ ]] 0.01
[[?]] $1 - 0.9973$.
**************************************************
Это крайне редкое событие для нормального закона.
**************************************************

**10. Формула вероятности отклонения $|X-a| < \delta$:**

[[ ]] $\Phi(\delta)$
[[X]] $2\Phi(\delta/\sigma)$
[[ ]] $\Phi(\delta/\sigma)$
[[ ]] $1 - \Phi(\delta)$
[[?]] Интервал симметричен, вероятности складываются.
**************************************************
В силу симметрии вероятность отклонения в обе стороны равна удвоенной функции Лапласа от нормированного отклонения.
**************************************************

**11. Если мы хотим построить доверительный интервал с надежностью 99.73%, его ширина должна быть:**

[[ ]] $\sigma$
[[ ]] $2\sigma$
[[X]] $6\sigma$ (от $-3\sigma$ до $+3\sigma$).
[[ ]] $3\sigma$
[[?]] $\pm 3\sigma$.
**************************************************
Полная ширина интервала $3\sigma - (-3\sigma) = 6\sigma$.
**************************************************

**12. Чему равен третий центральный момент $\mu_3$ нормального распределения?**

[[ ]] $\sigma^3$
[[X]] 0
[[ ]] $3\sigma$
[[ ]] $a^3$
[[?]] Момент нечетного порядка для симметричного распределения.
**************************************************
Так как распределение симметрично, все центральные моменты нечетных порядков равны нулю.
**************************************************

**13. Если производятся измерения детали, и ошибки распределены нормально, какой процент деталей будет бракованным, если допуск равен $\pm 3\sigma$?**

[[ ]] 3%
[[X]] 0.27%
[[ ]] 5%
[[ ]] 0%
[[?]] Обратная сторона правила трех сигм.
**************************************************
Брак — это выход за пределы допуска.
**************************************************

**14. Чем меньше $\sigma$, тем точность измерений (кучность):**

[[X]] Выше.
[[ ]] Ниже.
[[ ]] Не меняется.
[[ ]] Зависит от $a$.
[[?]] Узкий колокол.
**************************************************
Малая сигма означает, что значения группируются тесно вокруг среднего.
**************************************************

**15. Если $X \sim N(0, 1)$, то вероятность $P(X > 3)$:**

[[ ]] 0.5
[[ ]] 0.05
[[X]] 0.00135 (половина от 0.0027)
[[ ]] 0
[[?]] Один "хвост" за тремя сигмами.
**************************************************
Так как $P(|X|>3) \approx 0.0027$, а распределение симметрично, то один хвост — это половина от этой вероятности.
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 26, 27 и 28) в формате LiaScript.

# Тема 26: Распределение линейной функции случайного аргумента

**1. Линейной функцией случайной величины $X$ называется величина $Y$, равная:**

[[ ]] $X^2$
[[ ]] $e^X$
[[X]] $aX + b$
[[ ]] $\ln X$
[[?]] Уравнение прямой.
**************************************************
Линейное преобразование включает умножение на константу и сдвиг.
**************************************************

**2. Как изменяется математическое ожидание при линейном преобразовании $Y = aX + b$?**

[[ ]] $M[Y] = M[X]$
[[ ]] $M[Y] = a M[X]$
[[X]] $M[Y] = a M[X] + b$
[[ ]] $M[Y] = a^2 M[X] + b$
[[?]] Свойство линейности мат. ожидания.
**************************************************
Центр распределения смещается на $b$ и масштабируется в $a$ раз.
**************************************************

**3. Как изменяется дисперсия при преобразовании $Y = aX + b$?**

[[ ]] $D[Y] = a D[X] + b$
[[X]] $D[Y] = a^2 D[X]$
[[ ]] $D[Y] = a D[X]$
[[ ]] $D[Y] = D[X] + b$
[[?]] Дисперсия — это квадрат отклонения. Сдвиг $b$ не влияет на разброс.
**************************************************
Константа $b$ не влияет на разброс, а множитель $a$ выносится в квадрате.
**************************************************

**4. Чему равно среднее квадратическое отклонение $\sigma[Y]$, если $Y = -3X$?**

[[ ]] $-3\sigma[X]$
[[X]] $3\sigma[X]$
[[ ]] $9\sigma[X]$
[[ ]] $\sqrt{3}\sigma[X]$
[[?]] $\sigma$ всегда положительна. $\sqrt{(-3)^2} = 3$.
**************************************************
$\sigma[Y] = |a| \sigma[X]$. Знак минус переворачивает распределение, но не меняет его ширину.
**************************************************

**5. Если $X$ распределена нормально $N(m, \sigma)$, то $Y = aX + b$ будет иметь распределение:**

[[ ]] Равномерное.
[[X]] Нормальное.
[[ ]] Экспоненциальное.
[[ ]] Произвольное.
[[?]] Линейное преобразование сохраняет тип нормального закона.
**************************************************
Нормальное распределение устойчиво к линейным преобразованиям.
**************************************************

**6. Формула пересчета плотности вероятности $g(y)$ через $f(x)$ при $y = ax+b$:**

[[ ]] $f(\frac{y-b}{a})$
[[X]] $\frac{1}{|a|} f\left(\frac{y-b}{a}\right)$
[[ ]] $|a| f(ay+b)$
[[ ]] $f(y)$
[[?]] Нужно учесть изменение масштаба (якобиан преобразования).
**************************************************
Множитель $1/|a|$ нужен для сохранения нормировки (площади под графиком). Если растягиваем ось X, график должен стать ниже.
**************************************************

**7. Если $X \sim R[0, 1]$ (равномерное), то $Y = 2X + 3$ будет иметь распределение:**

[[ ]] $R[0, 1]$
[[X]] $R[3, 5]$
[[ ]] $N(0, 1)$
[[ ]] $R[0, 5]$
[[?]] Преобразуйте границы интервала: $2(0)+3$ и $2(1)+3$.
**************************************************
Линейная функция переводит отрезок в отрезок. Равномерность сохраняется.
**************************************************

**8. Операция стандартизации $Z = \frac{X - M[X]}{\sigma[X]}$ является:**

[[X]] Линейным преобразованием.
[[ ]] Нелинейным преобразованием.
[[ ]] Логарифмированием.
[[ ]] Интегрированием.
[[?]] Это вид $aX+b$, где $a=1/\sigma, b=-M/\sigma$.
**************************************************
Это частный случай линейной функции, приводящий к $M=0, D=1$.
**************************************************

**9. Если $D[X] = 4$, чему равно $D[2X + 5]$?**

[[ ]] 13
[[ ]] 9
[[X]] 16
[[ ]] 8
[[?]] $2^2 \cdot 4$.
**************************************************
$a^2 D[X] = 4 \cdot 4 = 16$. Слагаемое 5 не влияет.
**************************************************

**10. Влияет ли сдвиг $X + b$ на форму графика плотности вероятности?**

[[ ]] Да, график растягивается.
[[X]] Нет, график просто смещается вдоль оси абсцисс.
[[ ]] Да, график становится выше.
[[ ]] Да, график становится асимметричным.
[[?]] Форма (дисперсия, эксцесс) сохраняется.
**************************************************
Это параллельный перенос.
**************************************************

**11. Если $Y = -X$, что происходит с графиком плотности $f(x)$ (если он был симметричен относительно 0)?**

[[X]] Ничего (совпадает с $f(x)$).
[[ ]] Он сдвигается влево.
[[ ]] Он переворачивается вверх ногами.
[[ ]] Он исчезает.
[[?]] $f(-x) = f(x)$ для четной функции.
**************************************************
Для симметричного распределения (как нормальное $N(0,1)$) смена знака не меняет плотность.
**************************************************

**12. Якобиан преобразования $J$ при переходе $y = ax+b$ равен:**

[[ ]] $a$
[[X]] $1/a$ (в формуле плотности стоит $|J| = 1/|a|$).
[[ ]] $x$
[[ ]] 1
[[?]] Производная обратной функции $x = (y-b)/a$.
**************************************************
Мы выражаем $x$ через $y$, производная по $y$ равна $1/a$.
**************************************************

**13. Если $X$ — температура в Цельсиях, а $Y$ — в Фаренгейтах ($Y = 1.8X + 32$), как связаны их коэффициенты вариации?**

[[ ]] Они равны.
[[X]] Они разные (из-за сдвига $b \neq 0$).
[[ ]] $V_y = 1.8 V_x$.
[[ ]] $V_y = V_x + 32$.
[[?]] $V = \sigma/M$. Сдвиг $M$ меняет знаменатель, но не числитель.
**************************************************
Коэффициент вариации не инвариантен к сдвигу.
**************************************************

**14. Плотность распределения $Y = aX$ ($a>0$) получается из плотности $X$ путем:**

[[ ]] Сжатия графика к оси Y в $a$ раз по горизонтали и растяжения в $a$ раз по вертикали.
[[X]] Растяжения графика вдоль оси X в $a$ раз и сжатия в $a$ раз по вертикали.
[[ ]] Только сдвига.
[[ ]] Возведения в квадрат.
[[?]] Если $X \in [0,1]$, то $Y \in [0, a]$. Интервал стал шире.
**************************************************
Интервал значений расширяется, значит плотность (высота) должна уменьшиться для сохранения площади.
**************************************************

**15. Если $f(x)$ — плотность $X$, то плотность величины $Y = X$ (тождественное преобразование):**

[[ ]] $2f(x)$
[[X]] $f(y)$
[[ ]] 1
[[ ]] $F(x)$
[[?]] $a=1, b=0$.
**************************************************
Распределение не меняется.
**************************************************

# Тема 27: Системы дискретных случайных величин

**1. Система двух случайных величин $(X, Y)$ геометрически интерпретируется как:**

[[ ]] Две точки на оси.
[[X]] Случайная точка на плоскости.
[[ ]] Отрезок.
[[ ]] Вектор в трехмерном пространстве.
[[?]] Координаты $(x, y)$.
**************************************************
Это случайный вектор на плоскости $xOy$.
**************************************************

**2. Закон распределения системы двух ДСВ задается:**

[[ ]] Рядом распределения (одной строкой).
[[X]] Матрицей (таблицей) вероятностей $p_{ij}$.
[[ ]] Плотностью $f(x)$.
[[ ]] Двумя числами.
[[?]] Таблица с входами $x_i$ и $y_j$.
**************************************************
В таблице на пересечении стоят вероятности $P(X=x_i, Y=y_j)$.
**************************************************

**3. Условие нормировки для матрицы распределения:**

[[ ]] Сумма по строкам равна 1.
[[ ]] Сумма по столбцам равна 1.
[[X]] Сумма всех элементов матрицы равна 1 ($\sum_i \sum_j p_{ij} = 1$).
[[ ]] Произведение элементов равно 1.
[[?]] Вероятность достоверного события (попасть хоть в какую-то клетку).
**************************************************
Сумма вероятностей всех возможных пар значений должна быть единицей.
**************************************************

**4. Чтобы найти закон распределения компоненты $X$ (частное распределение), нужно:**

[[ ]] Выбрать максимальный элемент в каждой строке.
[[X]] Просуммировать вероятности в каждой строке таблицы.
[[ ]] Просуммировать вероятности в каждом столбце.
[[ ]] Умножить элементы на $y_j$.
[[?]] Формула согласованности: $P(X=x_i) = \sum_j p_{ij}$.
**************************************************
Суммируя по всем значениям $Y$, мы избавляемся от зависимости от $Y$ и получаем вероятность конкретного $x_i$.
**************************************************

**5. Частное распределение $Y$ получается путем:**

[[ ]] Суммирования по строкам.
[[X]] Суммирования по столбцам.
[[ ]] Диагонального суммирования.
[[ ]] Умножения на $X$.
[[?]] $P(Y=y_j) = \sum_i p_{ij}$.
**************************************************
Сумма вероятностей в столбце дает полную вероятность значения $y_j$.
**************************************************

**6. Условие независимости дискретных величин $X$ и $Y$:**

[[ ]] $p_{ij} = 0$
[[X]] $p_{ij} = P(X=x_i) \cdot P(Y=y_j)$ для всех $i, j$.
[[ ]] $p_{ij} = p_i + p_j$
[[ ]] $p_{ij} = 1$
[[?]] Вероятность совместного события равна произведению вероятностей.
**************************************************
Каждый элемент матрицы должен быть равен произведению соответствующих маргинальных вероятностей.
**************************************************

**7. Условное распределение $X$ при условии $Y=y_k$ находится по формуле:**

[[ ]] $p_{ik} \cdot p_{y_k}$
[[X]] $p_{ik} / P(Y=y_k)$
[[ ]] $p_{ik}$
[[ ]] $P(X=x_i)$
[[?]] Элемент матрицы делить на сумму столбца.
**************************************************
Это классическая формула условной вероятности $P(AB)/P(B)$.
**************************************************

**8. Сумма вероятностей в условном законе распределения (например, $X$ при $Y=y_1$) равна:**

[[ ]] $P(Y=y_1)$
[[X]] 1
[[ ]] 0
[[ ]] $p_{11}$
[[?]] Условное распределение — тоже распределение.
**************************************************
Это вероятности полной группы событий в новых условиях, их сумма равна 1.
**************************************************

**9. Функция распределения системы двух величин $F(x, y)$ определяется как:**

[[ ]] $P(X=x, Y=y)$
[[X]] $P(X < x, Y < y)$ (совместное выполнение неравенств).
[[ ]] $P(X < x) + P(Y < y)$
[[ ]] $P(X < x) \cdot P(Y < y)$
[[?]] Вероятность попасть в левый нижний квадрант относительно точки $(x,y)$.
**************************************************
Накопленная вероятность по обеим координатам одновременно.
**************************************************

**10. Чему равно $F(+\infty, +\infty)$?**

[[ ]] 0
[[X]] 1
[[ ]] $\infty$
[[ ]] $0.5$
[[?]] Вероятность того, что $X < \infty$ И $Y < \infty$.
**************************************************
Это достоверное событие.
**************************************************

**11. Корреляционный момент (ковариация) $K_{xy}$ вычисляется для ДСВ как:**

[[ ]] $\sum x_i y_j$
[[X]] $\sum_i \sum_j (x_i - m_x)(y_j - m_y) p_{ij}$
[[ ]] $\sum (x_i - m_x) p_i$
[[ ]] $M[X] M[Y]$
[[?]] Мат. ожидание произведения центрированных величин.
**************************************************
Двойная сумма взвешенных отклонений от средних.
**************************************************

**12. Может ли вероятность $p_{ij}$ в ячейке таблицы быть больше, чем сумма вероятностей в соответствующей строке?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Только если $Y$ отрицательно.
[[ ]] Зависит от дисперсии.
[[?]] Часть не может быть больше целого.
**************************************************
$p_{ij}$ — это лишь одно слагаемое в сумме $P(X=x_i)$.
**************************************************

**13. Если в таблице распределения много нулей, это говорит о:**

[[ ]] Независимости величин.
[[X]] Сильной (возможно, функциональной) зависимости.
[[ ]] Ошибке в данных.
[[ ]] Равномерном распределении.
[[?]] Значения жестко связаны: если $X=1$, то $Y$ только 2 (остальные 0).
**************************************************
Нули означают невозможность определенных сочетаний, что сужает свободу $Y$ при данном $X$.
**************************************************

**14. Количество слагаемых в двойной сумме для вычисления $M[XY]$ при таблице $3 \times 4$:**

[[ ]] 7
[[X]] 12
[[ ]] 3
[[ ]] 4
[[?]] Количество ячеек.
**************************************************
Нужно перебрать все $3 \times 4 = 12$ пар значений.
**************************************************

**15. Если $X$ и $Y$ независимы, то матрица вероятностей имеет ранг:**

[[X]] 1 (строки пропорциональны).
[[ ]] 2
[[ ]] $n$
[[ ]] 0
[[?]] $p_{ij} = p_i \cdot q_j$.
**************************************************
Все строки получаются из вектора $P(Y)$ умножением на константу $P(X=x_i)$.
**************************************************

# Тема 28: Системы непрерывных случайных величин

**1. Совместная плотность распределения $f(x, y)$ — это:**

[[ ]] Вероятность.
[[X]] Вторая смешанная производная функции распределения: $\frac{\partial^2 F}{\partial x \partial y}$.
[[ ]] Произведение плотностей.
[[ ]] Интеграл от $F(x,y)$.
[[?]] Аналог $f(x) = F'(x)$ для 2D.
**************************************************
Это поверхность, объем под которой равен вероятности.
**************************************************

**2. Геометрический смысл вероятности попадания в область $D$ для системы НСВ:**

[[ ]] Площадь области $D$.
[[X]] Объем цилиндрического тела, ограниченного сверху поверхностью $f(x, y)$, а снизу областью $D$.
[[ ]] Длина границы области.
[[ ]] Значение функции в центре области.
[[?]] Двойной интеграл от плотности.
**************************************************
$P((X,Y) \in D) = \iint_D f(x,y) dx dy$.
**************************************************

**3. Условие нормировки для двумерной плотности:**

[[ ]] $\iint f(x, y) dx dy = 0$
[[X]] $\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) dx dy = 1$
[[ ]] $\max f(x, y) = 1$
[[ ]] Объем равен $\infty$.
[[?]] Полный объем под поверхностью распределения.
**************************************************
Вероятность попадания в плоскость $xOy$ равна 1.
**************************************************

**4. Формула согласованности для нахождения $f_1(x)$ (плотности $X$) из $f(x, y)$:**

[[ ]] $\int f(x, y) dx$
[[X]] $\int_{-\infty}^{+\infty} f(x, y) dy$
[[ ]] $f(x, 0)$
[[ ]] $\frac{d}{dx} f(x, y)$
[[?]] Нужно "просуммировать" (интегрировать) по всем значениям "лишней" переменной $Y$.
**************************************************
Мы усредняем влияние $Y$, интегрируя по $dy$.
**************************************************

**5. Если $X$ и $Y$ независимы, то совместная плотность $f(x, y)$ равна:**

[[ ]] $f_1(x) + f_2(y)$
[[X]] $f_1(x) \cdot f_2(y)$
[[ ]] $f_1(x) / f_2(y)$
[[ ]] 0
[[?]] Теорема умножения для плотностей.
**************************************************
Поверхность распадается на произведение профилей.
**************************************************

**6. Может ли $f(x, y)$ быть отрицательной?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Только в 3-м квадранте.
[[ ]] Если корреляция отрицательная.
[[?]] Вероятность (объем) не может быть отрицательной.
**************************************************
Как и одномерная плотность, двумерная всегда неотрицательна.
**************************************************

**7. Плотность равномерного распределения в прямоугольнике $D$ площади $S$ равна:**

[[ ]] $S$
[[X]] $1/S$ внутри прямоугольника и 0 вне.
[[ ]] 1
[[ ]] $x \cdot y$
[[?]] Объем параллелепипеда $S \cdot h = 1$.
**************************************************
Высота постоянна и равна обратному значению площади основания.
**************************************************

**8. Функция распределения $F(x, y)$ является:**

[[ ]] Убывающей по $x$ и возрастающей по $y$.
[[X]] Неубывающей по каждому аргументу.
[[ ]] Периодической.
[[ ]] Всегда равной 1.
[[?]] Свойство накопленной вероятности.
**************************************************
При увеличении любого аргумента область расширяется, вероятность не может падать.
**************************************************

**9. Условная плотность $f(x|y)$ вычисляется как:**

[[ ]] $f(x, y) \cdot f_2(y)$
[[X]] $\frac{f(x, y)}{f_2(y)}$
[[ ]] $f(x, y)$
[[ ]] $f_1(x)$
[[?]] Совместная плотность, нормированная на плотность условия.
**************************************************
Это сечение поверхности $f(x,y)$, масштабированное так, чтобы площадь сечения стала равна 1.
**************************************************

**10. Чему равен предел $\lim_{x \to -\infty} F(x, y)$?**

[[X]] 0
[[ ]] 1
[[ ]] $F_2(y)$
[[ ]] $f(x, y)$
[[?]] Вероятность того, что $X < -\infty$.
**************************************************
Если одно из событий невозможно, то и их пересечение невозможно.
**************************************************

**11. Вероятность попадания точки $(X, Y)$ в прямоугольник $[a, b] \times [c, d]$ через функцию распределения:**

[[ ]] $F(b, d) - F(a, c)$
[[X]] $F(b, d) - F(a, d) - F(b, c) + F(a, c)$
[[ ]] $F(b, d) \cdot F(a, c)$
[[ ]] $F(b-a, d-c)$
[[?]] Аналог формулы включений-исключений для прямоугольника.
**************************************************
Берем общий угол, вычитаем две полосы, прибавляем дважды вычтенный угол.
**************************************************

**12. Если $f(x, y) = e^{-(x+y)}$ при $x, y > 0$, являются ли величины независимыми?**

[[X]] Да, так как $e^{-(x+y)} = e^{-x} \cdot e^{-y}$.
[[ ]] Нет.
[[ ]] Зависит от корреляции.
[[ ]] Нельзя определить.
[[?]] Функция распадается на множители, зависящие только от $x$ и только от $y$.
**************************************************
Это произведение двух показательных плотностей.
**************************************************

**13. Размерность двумерной плотности $f(x, y)$ (если $X, Y$ в метрах):**

[[ ]] $1/м$
[[X]] $1/м^2$
[[ ]] Безразмерная.
[[ ]] $м^2$
[[?]] Вероятность делить на площадь ($dx dy$).
**************************************************
Плотность на единицу площади.
**************************************************

**14. Что такое линия регрессии $Y$ на $X$?**

[[ ]] График плотности.
[[X]] График условного математического ожидания $M[Y|x]$ в зависимости от $x$.
[[ ]] Прямая $y=x$.
[[ ]] Линия уровня $f(x, y) = C$.
[[?]] Как меняется среднее значение $Y$ при изменении $X$.
**************************************************
Это кривая, показывающая зависимость среднего значения одной величины от значения другой.
**************************************************

**15. Если совместная плотность $f(x, y)$ симметрична относительно начала координат, то $M[X]$ и $M[Y]$:**

[[ ]] Равны 1.
[[X]] Равны 0 (если существуют).
[[ ]] Равны друг другу, но не 0.
[[ ]] Не определены.
[[?]] Центр тяжести симметричной фигуры.
**************************************************
Центр распределения находится в $(0, 0)$.
**************************************************

Вот 45 вопросов (по 15 на каждую из тем 29, 30 и 31) в формате LiaScript.

# Тема 29: Нормальный двумерный закон распределения

**1. Сколько параметров полностью определяют двумерное нормальное распределение?**

[[ ]] 2 ($a, \sigma$)
[[ ]] 3
[[X]] 5 ($a_x, a_y, \sigma_x, \sigma_y, \rho$)
[[ ]] 4
[[?]] Мат. ожидания, дисперсии и коэффициент связи.
**************************************************
Два параметра центра, два параметра масштаба и один параметр связи (корреляция).
**************************************************

**2. График совместной плотности вероятности $f(x, y)$ нормального закона представляет собой:**

[[ ]] Плоскость.
[[ ]] Пирамиду.
[[X]] Гладкий «холм» (колоколообразную поверхность).
[[ ]] Цилиндр.
[[?]] Обобщение кривой Гаусса на 3D.
**************************************************
Поверхность имеет один пик и убывает во всех направлениях.
**************************************************

**3. Линии уровня (сечения плоскостями, параллельными $xOy$) нормального распределения имеют форму:**

[[ ]] Квадратов.
[[X]] Эллипсов (эллипсов рассеяния).
[[ ]] Прямых линий.
[[ ]] Гипербол.
[[?]] Уравнение в показателе экспоненты — квадратичная форма.
**************************************************
Это концентрические эллипсы с центром в $(a_x, a_y)$.
**************************************************

**4. Если коэффициент корреляции $\rho = 0$, то оси эллипсов рассеяния:**

[[ ]] Наклонены под углом $45^\circ$.
[[X]] Параллельны осям координат $Ox$ и $Oy$.
[[ ]] Совпадают с биссектрисами.
[[ ]] Эллипсы превращаются в круги (только при $\sigma_x = \sigma_y$).
[[?]] Отсутствие смешанного члена $xy$ в уравнении.
**************************************************
При $\rho=0$ величины независимы, и «перекоса» в распределении нет.
**************************************************

**5. Если $\rho > 0$, то эллипсы рассеяния:**

[[X]] Вытянуты вдоль направления $y=x$ (1 и 3 четверти относительно центра).
[[ ]] Вытянуты вдоль направления $y=-x$.
[[ ]] Вырождаются в круг.
[[ ]] Ориентированы горизонтально.
[[?]] Положительная связь: больше $x$ — больше $y$.
**************************************************
Облако точек вытягивается вдоль возрастающей прямой.
**************************************************

**6. Если две случайные величины имеют совместное нормальное распределение, то их частные (маргинальные) распределения:**

[[ ]] Равномерные.
[[ ]] Экспоненциальные.
[[X]] Тоже нормальные.
[[ ]] Могут быть любыми.
[[?]] Проекция "холма" на стену.
**************************************************
Свойство устойчивости нормального закона: проекции и сечения гауссианы — гауссианы.
**************************************************

**7. Условные распределения (например, $X$ при условии $Y=y$) для двумерного нормального закона являются:**

[[ ]] Вырожденными.
[[X]] Нормальными.
[[ ]] Равномерными.
[[ ]] Стьюдента.
[[?]] Сечение холма вертикальной плоскостью.
**************************************************
Если разрезать холм ножом, профиль разреза будет колоколом (нормальной кривой).
**************************************************

**8. Функция регрессии $Y$ на $X$ ($M[Y|x]$) для нормального закона является:**

[[ ]] Параболой.
[[X]] Прямой линией (линейная регрессия).
[[ ]] Экспонентой.
[[ ]] Логарифмом.
[[?]] Линия, соединяющая центры условных распределений.
**************************************************
Нормальная корреляция всегда линейна.
**************************************************

**9. В формуле плотности нормального закона перед экспонентой стоит множитель:**

[[ ]] $\frac{1}{2\pi}$
[[X]] $\frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}}$
[[ ]] $\frac{1}{\sqrt{2\pi} \sigma}$
[[ ]] 1
[[?]] Нормировка объема.
**************************************************
Множитель учитывает оба СКО и степень сжатия из-за корреляции.
**************************************************

**10. Если $\rho \to 1$ или $\rho \to -1$, то двумерное нормальное распределение:**

[[ ]] Становится равномерным.
[[X]] Вырождается в одномерное (вся масса вероятности на прямой линии).
[[ ]] Становится круговым.
[[ ]] Исчезает.
[[?]] Жесткая функциональная связь.
**************************************************
Эллипсы сплющиваются в отрезок, дисперсия поперек линии стремится к 0.
**************************************************

**11. Что означает термин «круговое нормальное распределение»?**

[[ ]] Распределение на окружности.
[[X]] Частный случай, когда $\sigma_x = \sigma_y$ и $\rho = 0$.
[[ ]] Когда $\rho = 1$.
[[ ]] Когда $a_x = a_y = 0$.
[[?]] Линии уровня — окружности.
**************************************************
Равноточный разброс по всем направлениям (например, стрельба в яблочко).
**************************************************

**12. Коэффициент корреляции $\rho$ в нормальном законе совпадает с:**

[[ ]] $\sigma_x / \sigma_y$
[[ ]] Угловым коэффициентом регрессии.
[[X]] Параметром $\rho$ в формуле плотности.
[[ ]] Нулем.
[[?]] Параметр аналитического выражения имеет прямой статистический смысл.
**************************************************
Это тот самый линейный коэффициент корреляции Пирсона.
**************************************************

**13. Если компоненты $X$ и $Y$ некоррелированы ($\rho=0$), то для нормального закона они:**

[[ ]] Зависимы.
[[X]] Независимы.
[[ ]] Несовместны.
[[ ]] Противоположны.
[[?]] Уникальное свойство нормального закона.
**************************************************
Только для гауссовских величин некоррелированность равносильна независимости.
**************************************************

**14. Центр рассеивания двумерного нормального распределения — это точка:**

[[ ]] $(0,0)$
[[ ]] $(\sigma_x, \sigma_y)$
[[X]] $(a_x, a_y)$
[[ ]] $(1, 1)$
[[?]] Координаты вершины.
**************************************************
Точка, соответствующая математическим ожиданиям компонентов.
**************************************************

**15. Какую поверхность описывает функция $z = f(x, y)$?**

[[ ]] Сферу.
[[ ]] Эллипсоид.
[[X]] «Колокол», вытянутый или сжатый.
[[ ]] Конус.
[[?]] Гауссова шапка.
**************************************************
Поверхность, асимптотически приближающаяся к плоскости $z=0$.
**************************************************

# Тема 30: Независимость и некоррелированность

**1. Случайные величины $X$ и $Y$ называются независимыми, если:**

[[ ]] Их сумма равна константе.
[[X]] Их совместный закон распределения равен произведению частных законов ($F(x,y) = F(x)F(y)$).
[[ ]] Их корреляция равна 0.
[[ ]] Они не пересекаются.
[[?]] Определение через произведение вероятностей.
**************************************************
Это самое сильное условие отсутствия связи: информация об $X$ не меняет распределение $Y$.
**************************************************

**2. Случайные величины называются некоррелированными, если:**

[[ ]] Они независимы.
[[X]] Их ковариация (или коэффициент корреляции) равна 0.
[[ ]] Их дисперсии равны.
[[ ]] Их мат. ожидания равны 0.
[[?]] Отсутствие линейной связи.
**************************************************
$K_{xy} = M[XY] - M[X]M[Y] = 0$.
**************************************************

**3. Верно ли утверждение: «Если величины независимы, то они некоррелированы»?**

[[X]] Да, всегда.
[[ ]] Нет, никогда.
[[ ]] Только для нормального закона.
[[ ]] Только для дискретных величин.
[[?]] Из сильного условия следует слабое.
**************************************************
Если связи нет вообще, то нет и линейной связи.
**************************************************

**4. Верно ли утверждение: «Если величины некоррелированы, то они независимы»?**

[[ ]] Да, всегда.
[[X]] Нет, не обязательно (общий случай).
[[ ]] Только если дисперсии равны.
[[ ]] Нет, это вообще несовместимые понятия.
[[?]] Пример $Y=X^2$ на $[-1, 1]$.
**************************************************
Может существовать сильная нелинейная зависимость при нулевой корреляции.
**************************************************

**5. Для какого распределения некоррелированность ЭКВИВАЛЕНТНА независимости?**

[[ ]] Равномерного.
[[ ]] Биномиального.
[[X]] Нормального (Гаусса).
[[ ]] Стьюдента.
[[?]] См. тему 29.
**************************************************
В нормальном законе вся зависимость описывается параметром $\rho$. Если он 0, зависимости нет.
**************************************************

**6. Если $Y = X^2$, а $X$ симметрична около 0 (например, $N(0,1)$), то коэффициент корреляции $r_{xy}$ равен:**

[[ ]] 1
[[ ]] -1
[[X]] 0
[[ ]] 0.5
[[?]] $M[X^3] = 0$.
**************************************************
Величины функционально зависимы, но корреляция равна нулю (линейный тренд отсутствует).
**************************************************

**7. Чему равно $M[XY]$, если $X$ и $Y$ независимы?**

[[ ]] 0
[[ ]] $M[X+Y]$
[[X]] $M[X] \cdot M[Y]$
[[ ]] $1$
[[?]] Мат. ожидание произведения.
**************************************************
Для независимых величин среднее произведения равно произведению средних.
**************************************************

**8. Если $r_{xy} = 0.9$, то зависимость между величинами:**

[[ ]] Отсутствует.
[[X]] Сильная линейная (положительная).
[[ ]] Функциональная.
[[ ]] Нелинейная.
[[?]] Близко к 1.
**************************************************
Точки группируются тесно вдоль прямой линии.
**************************************************

**9. Условие независимости для плотностей вероятности $f(x, y)$:**

[[ ]] $f(x, y) = f_1(x) + f_2(y)$
[[X]] $f(x, y) = f_1(x) \cdot f_2(y)$
[[ ]] $f(x, y) = 0$
[[ ]] $f(x, y) = const$
[[?]] Поверхность распадается на произведение профилей.
**************************************************
Аналог $P(AB) = P(A)P(B)$.
**************************************************

**10. Чему равна дисперсия суммы независимых величин $D[X+Y]$?**

[[ ]] $D[X] \cdot D[Y]$
[[X]] $D[X] + D[Y]$
[[ ]] $D[X] - D[Y]$
[[ ]] $\sqrt{D[X] + D[Y]}$
[[?]] Аддитивность дисперсии.
**************************************************
Ковариация равна нулю, поэтому дисперсии просто складываются.
**************************************************

**11. Если $f(x, y) = 1$ в квадрате $[0,1] \times [0,1]$ и 0 вне, величины:**

[[X]] Независимы.
[[ ]] Зависимы.
[[ ]] Коррелированы.
[[ ]] Не определены.
[[?]] $1 = 1 \cdot 1$. Плотность факторизуется.
**************************************************
Это произведение двух равномерных плотностей.
**************************************************

**12. Если $f(x, y) = 2$ в треугольнике $0 < x < y < 1$, величины:**

[[ ]] Независимы.
[[X]] Зависимы.
[[ ]] Некоррелированы.
[[ ]] Равномерны.
[[?]] Область определения (треугольник) не является декартовым произведением (прямоугольником).
**************************************************
Диапазон возможных значений $X$ зависит от того, чему равно $Y$ (и наоборот). Зависимость есть.
**************************************************

**13. Независимость означает, что условный закон распределения:**

[[ ]] Не существует.
[[X]] Совпадает с безусловным.
[[ ]] Вырожденный.
[[ ]] Нормальный.
[[?]] $f(x|y) = f(x)$.
**************************************************
Знание $Y$ не дает никакой новой информации об $X$.
**************************************************

**14. Может ли коэффициент корреляции быть равен 0, если величины связаны как $Y = \cos X$?**

[[X]] Да, например, если $X$ равномерна на $[-\pi, \pi]$.
[[ ]] Нет, связь функциональная.
[[ ]] Только если $X=0$.
[[ ]] Нет, косинус четная функция.
[[?]] Линейный тренд отсутствует (облако точек симметрично).
**************************************************
Функциональная зависимость есть, но корреляция нулевая из-за симметрии.
**************************************************

**15. Если $X$ и $Y$ независимы, чему равен $D[X-Y]$?**

[[ ]] $D[X] - D[Y]$
[[X]] $D[X] + D[Y]$
[[ ]] 0
[[ ]] $D[X] D[Y]$
[[?]] $D[-Y] = D[Y]$.
**************************************************
Дисперсии всегда складываются для независимых величин (ошибка растет).
**************************************************

# Тема 31: Свойства M и D в системах СВ

**1. Математическое ожидание суммы $M[X+Y]$ равно:**

[[ ]] $M[X] \cdot M[Y]$
[[X]] $M[X] + M[Y]$
[[ ]] $M[X] + M[Y] + 2K_{xy}$
[[ ]] $M[X] - M[Y]$
[[?]] Линейность оператора $M$.
**************************************************
Это свойство работает всегда, независимо от зависимости величин.
**************************************************

**2. Математическое ожидание произведения $M[XY]$ в общем случае (зависимые величины) равно:**

[[ ]] $M[X] M[Y]$
[[X]] $M[X] M[Y] + K_{xy}$ (ковариация).
[[ ]] $M[X] + M[Y]$
[[ ]] $K_{xy}$
[[?]] Из определения ковариации.
**************************************************
$K_{xy} = M[XY] - M[X]M[Y]$.
**************************************************

**3. Дисперсия суммы $D[X+Y]$ для ЗАВИСИМЫХ величин равна:**

[[ ]] $D[X] + D[Y]$
[[X]] $D[X] + D[Y] + 2K_{xy}$
[[ ]] $D[X] + D[Y] - 2K_{xy}$
[[ ]] $D[X] D[Y]$
[[?]] Квадрат суммы $(a+b)^2 = a^2 + b^2 + 2ab$.
**************************************************
Добавляется удвоенная ковариация.
**************************************************

**4. Дисперсия разности $D[X-Y]$ для НЕЗАВИСИМЫХ величин:**

[[ ]] $D[X] - D[Y]$
[[X]] $D[X] + D[Y]$
[[ ]] $D[X] - D[Y] + 2K_{xy}$
[[ ]] 0
[[?]] Разброс разности двух случайных чисел не может быть меньше разброса одного.
**************************************************
Неопределенности складываются.
**************************************************

**5. Если $Y = -X$ (жесткая отрицательная связь), то $D[X+Y]$ равна:**

[[ ]] $2D[X]$
[[X]] 0
[[ ]] $4D[X]$
[[ ]] $-2D[X]$
[[?]] $X + (-X) = 0$. Дисперсия константы 0.
**************************************************
По формуле: $D[X] + D[-X] + 2 Cov(X, -X) = D[X] + D[X] - 2D[X] = 0$.
**************************************************

**6. Если $X$ и $Y$ коррелированы положительно ($K_{xy} > 0$), то дисперсия их суммы:**

[[X]] Больше суммы их дисперсий.
[[ ]] Меньше суммы их дисперсий.
[[ ]] Равна сумме дисперсий.
[[ ]] Равна 0.
[[?]] Ошибки усиливают друг друга.
**************************************************
$D[X+Y] = D[X] + D[Y] + \text{положительное число}$.
**************************************************

**7. Дисперсия среднего арифметического $\bar{X} = \frac{X_1 + ... + X_n}{n}$ для $n$ независимых одинаковых величин (с дисперсией $\sigma^2$):**

[[ ]] $\sigma^2$
[[ ]] $n \sigma^2$
[[X]] $\sigma^2 / n$
[[ ]] $\sigma / \sqrt{n}$
[[?]] Константа $1/n$ выносится в квадрате.
**************************************************
$D[\frac{1}{n}\sum X_i] = \frac{1}{n^2} \sum D[X_i] = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$.
**************************************************

**8. Свойство линейности дисперсии ($D[aX+bY] = aD[X] + bD[Y]$):**

[[ ]] Выполняется всегда.
[[X]] Не выполняется (дисперсия квадратична).
[[ ]] Выполняется только для независимых.
[[ ]] Выполняется, если $a,b > 0$.
[[?]] Константы выносятся в квадрате.
**************************************************
Правильно: $a^2 D[X] + b^2 D[Y] + 2ab K_{xy}$.
**************************************************

**9. Если $M[X] = 2, M[Y] = 5$, то $M[3X - 2Y + 1]$ равно:**

[[ ]] -3
[[X]] $3(2) - 2(5) + 1 = -3$
[[ ]] 17
[[ ]] 0
[[?]] Подставьте числа.
**************************************************
$6 - 10 + 1 = -3$.
**************************************************

**10. Чему равна ковариация $K_{xx}$ (ковариация величины с самой собой)?**

[[ ]] 0
[[ ]] $M[X]$
[[X]] $D[X]$
[[ ]] 1
[[?]] $M[(X-m)(X-m)]$.
**************************************************
Это определение дисперсии.
**************************************************

**11. Если $X$ и $Y$ независимы, то $D[XY]$ (дисперсия произведения):**

[[ ]] $D[X] D[Y]$
[[X]] $D[X]D[Y] + (M[X])^2 D[Y] + (M[Y])^2 D[X]$
[[ ]] $M[X^2 Y^2]$
[[ ]] Не вычисляется.
[[?]] Формула сложнее простой суммы.
**************************************************
$M[X^2 Y^2] - (M[XY])^2 = M[X^2]M[Y^2] - (M[X]M[Y])^2$. Раскрываем $M[X^2] = D+M^2$.
**************************************************

**12. Ковариация $K(X, C)$, где $C$ — константа:**

[[ ]] $C$
[[X]] 0
[[ ]] $D[X]$
[[ ]] $M[X]$
[[?]] Константа не меняется, отклонение всегда 0.
**************************************************
Нет совместной вариации, так как $C$ не варьируется.
**************************************************

**13. Если $X_1, ..., X_n$ попарно некоррелированы, дисперсия их суммы равна:**

[[ ]] 0
[[X]] Сумме их дисперсий.
[[ ]] Произведению дисперсий.
[[ ]] Сумме квадратов мат. ожиданий.
[[?]] Все перекрестные слагаемые $K_{ij}$ равны 0.
**************************************************
Для дисперсии суммы достаточно некоррелированности (не обязательно полной независимости).
**************************************************

**14. $M[X - Y]$ всегда равно:**

[[X]] $M[X] - M[Y]$
[[ ]] $M[X] + M[Y]$
[[ ]] $M[X] / M[Y]$
[[ ]] $|M[X] - M[Y]|$
[[?]] Линейность.
**************************************************
Разность средних.
**************************************************

**15. Если вы измеряете длину двумя способами с ошибками $X$ и $Y$ ($D[X]=1, D[Y]=1$), и берете среднее $(X+Y)/2$, то дисперсия результата (при независимости):**

[[ ]] 1
[[ ]] 2
[[X]] 0.5
[[ ]] 0.25
[[?]] $D[Sum]/4 = 2/4$.
**************************************************
Усреднение уменьшает случайную ошибку в $n$ раз (для дисперсии в $n$, для СКО в $\sqrt{n}$).
**************************************************

Вот 60 вопросов (по 15 на каждую из тем 32, 33, 34 и 35) в формате LiaScript.

# Тема 32: Свойства корреляционного момента и коэффициента корреляции

**1. Ковариация $K_{xy}$ (корреляционный момент) характеризует:**

[[ ]] Только наличие зависимости.
[[X]] Наличие и направление линейной связи между случайными величинами.
[[ ]] Дисперсию суммы.
[[ ]] Отношение мат. ожиданий.
[[?]] Мера совместной вариации.
**************************************************
Ковариация показывает, как две величины меняются вместе (линейно).
**************************************************

**2. Главный недостаток ковариации как меры связи:**

[[ ]] Она всегда положительна.
[[X]] Она зависит от единиц измерения величин (не нормирована).
[[ ]] Она сложна в вычислении.
[[ ]] Она равна нулю для независимых величин.
[[?]] Если перевести метры в сантиметры, ковариация вырастет в 100 раз.
**************************************************
По значению $K_{xy}$ трудно судить о силе связи, если не знать масштаб величин.
**************************************************

**3. Коэффициент корреляции $r_{xy}$ определяется формулой:**

[[ ]] $K_{xy} / D[X]$
[[ ]] $K_{xy} / (M[X]M[Y])$
[[X]] $K_{xy} / (\sigma_x \sigma_y)$
[[ ]] $\sigma_x \sigma_y / K_{xy}$
[[?]] Нормировка на произведение СКО.
**************************************************
Это безразмерная величина, показывающая тесноту линейной связи.
**************************************************

**4. Область значений коэффициента корреляции:**

[[ ]] $[0, 1]$
[[X]] $[-1, 1]$
[[ ]] $(-\infty, +\infty)$
[[ ]] $[0, \infty)$
[[?]] Нормированная величина.
**************************************************
От жесткой отрицательной до жесткой положительной связи.
**************************************************

**5. Если $r_{xy} = 1$, то величины связаны:**

[[X]] Жесткой линейной зависимостью $Y = aX + b$ ($a > 0$).
[[ ]] Нелинейной зависимостью.
[[ ]] Независимы.
[[ ]] $Y = X^2$.
[[?]] Точки лежат на одной прямой.
**************************************************
Это функциональная линейная зависимость.
**************************************************

**6. Если $r_{xy} = -1$, то:**

[[ ]] Связи нет.
[[ ]] Связь квадратичная.
[[X]] При увеличении $X$ величина $Y$ линейно убывает (строго).
[[ ]] $Y = -X^2$.
[[?]] Обратная пропорциональность (линейная).
**************************************************
Максимальная отрицательная корреляция.
**************************************************

**7. Если $X$ и $Y$ независимы, то $r_{xy}$ равен:**

[[ ]] 1
[[ ]] -1
[[X]] 0
[[ ]] 0.5
[[?]] Линейной связи нет, как и никакой другой.
**************************************************
Независимость влечет некоррелированность.
**************************************************

**8. Меняется ли коэффициент корреляции при смене начала отсчета ($X' = X+c$)?**

[[ ]] Да.
[[X]] Нет.
[[ ]] Меняет знак.
[[ ]] Становится нулевым.
[[?]] Инвариантность к сдвигу.
**************************************************
Корреляция зависит только от отклонений от среднего, сдвиг сокращается.
**************************************************

**9. Меняется ли коэффициент корреляции при умножении $X$ на положительное число $a$?**

[[ ]] Увеличивается в $a$ раз.
[[X]] Не меняется.
[[ ]] Уменьшается в $a$ раз.
[[ ]] Становится равным 1.
[[?]] Инвариантность к масштабу.
**************************************************
Множитель $a$ появляется и в числителе (ковариация), и в знаменателе ($\sigma$), и сокращается.
**************************************************

**10. Чему равен знак ковариации?**

[[ ]] Всегда плюс.
[[X]] Совпадает со знаком коэффициента корреляции.
[[ ]] Всегда минус.
[[ ]] Не имеет смысла.
[[?]] Знаменатель $\sigma_x \sigma_y$ всегда положителен.
**************************************************
Знак показывает направление связи.
**************************************************

**11. Если $r_{xy} = 0$, то величины называются:**

[[ ]] Независимыми.
[[X]] Некоррелированными.
[[ ]] Несовместными.
[[ ]] Ортогональными (в геометрическом смысле векторов центрированных СВ — да, но термин "некоррелированные" стандартнее).
[[?]] Термин для отсутствия линейной связи.
**************************************************
Это необходимое, но не достаточное условие независимости.
**************************************************

**12. Ковариация $X$ с самой собой ($K_{xx}$) равна:**

[[ ]] 0
[[ ]] 1
[[X]] $D[X]$
[[ ]] $M[X]$
[[?]] $M[(X-m)(X-m)]$.
**************************************************
Дисперсия — это частный случай ковариации.
**************************************************

**13. Если $Y = X^2$ и $X$ симметрична относительно 0, то $r_{xy}$:**

[[ ]] 1
[[ ]] -1
[[X]] 0
[[ ]] 0.5
[[?]] Линейная связь отсутствует (облако точек — парабола).
**************************************************
Некоррелированность при наличии функциональной зависимости.
**************************************************

**14. Матрица ковариаций симметрична?**

[[X]] Да, всегда ($K_{xy} = K_{yx}$).
[[ ]] Нет.
[[ ]] Только для независимых величин.
[[ ]] Только для нормальных величин.
[[?]] От перестановки множителей...
**************************************************
Свойство симметрии.
**************************************************

**15. Если $Y = -2X + 5$, чему равен $r_{xy}$?**

[[ ]] 2
[[ ]] -2
[[X]] -1
[[ ]] 0
[[?]] Строгая линейная зависимость с отрицательным коэффициентом.
**************************************************
Коэффициент при $X$ отрицательный, значит корреляция полная отрицательная.
**************************************************

# Тема 33: Неравенства Чебышева и ЗБЧ

**1. Неравенство Чебышева дает оценку для:**

[[ ]] Вероятности того, что СВ примет положительное значение.
[[X]] Вероятности отклонения СВ от ее мат. ожидания.
[[ ]] Дисперсии.
[[ ]] Ошибки первого рода.
[[?]] $P(|X-M| \ge \varepsilon)$.
**************************************************
Оно ограничивает вероятность больших отклонений.
**************************************************

**2. Формула неравенства Чебышева:**

[[ ]] $P(|X-M| \ge \varepsilon) \le \varepsilon^2 / D[X]$
[[X]] $P(|X-M| \ge \varepsilon) \le D[X] / \varepsilon^2$
[[ ]] $P(|X-M| \ge \varepsilon) \ge 1 - D[X]$
[[ ]] $P(|X-M| \le \varepsilon) \le D[X]$
[[?]] Чем меньше дисперсия, тем меньше вероятность отклонения.
**************************************************
Вероятность уйти далеко от центра обратно пропорциональна квадрату расстояния.
**************************************************

**3. Сходимость по вероятности $X_n \xrightarrow{P} a$ означает:**

[[ ]] $\lim P(X_n = a) = 1$
[[X]] $\lim_{n \to \infty} P(|X_n - a| < \varepsilon) = 1$ для любого $\varepsilon > 0$.
[[ ]] $X_n$ становится константой, начиная с некоторого $n$.
[[ ]] Дисперсия стремится к $\infty$.
[[?]] Вероятность больших отклонений стремится к нулю.
**************************************************
Это стохастическая сходимость: значения группируются все теснее около $a$.
**************************************************

**4. Закон Больших Чисел (теорема Чебышева) утверждает устойчивость:**

[[ ]] Суммы случайных величин.
[[X]] Среднего арифметического независимых случайных величин.
[[ ]] Дисперсии.
[[ ]] Медианы.
[[?]] $\frac{1}{n} \sum X_i \to M[X]$.
**************************************************
Среднее значение перестает быть случайным при $n \to \infty$.
**************************************************

**5. Какое требование налагается на дисперсии в теореме Чебышева (ЗБЧ)?**

[[ ]] Они должны быть равны 0.
[[ ]] Они должны стремиться к 0.
[[X]] Они должны быть равномерно ограничены ($D[X_i] \le C$).
[[ ]] Они должны быть равны 1.
[[?]] Дисперсии не должны расти слишком быстро.
**************************************************
Если дисперсии не ограничены, среднее может не стабилизироваться.
**************************************************

**6. Теорема Бернулли является частным случаем ЗБЧ для:**

[[ ]] Нормальных величин.
[[X]] Индикаторов событий (схемы Бернулли).
[[ ]] Непрерывных величин.
[[ ]] Зависимых величин.
[[?]] Сходимость частоты к вероятности.
**************************************************
Частота $m/n$ сходится по вероятности к $p$.
**************************************************

**7. Физический смысл ЗБЧ:**

[[ ]] Случайности не существует.
[[X]] Совокупное действие большого числа случайных факторов приводит к детерминированному (предсказуемому) результату.
[[ ]] Невозможно предсказать среднее.
[[ ]] Ошибки всегда накапливаются.
[[?]] "Порядок из хаоса".
**************************************************
Это основа всех статистических измерений: многократное измерение гасит ошибку.
**************************************************

**8. Неравенство Маркова (для неотрицательных СВ) оценивает:**

[[X]] $P(X \ge \varepsilon) \le M[X] / \varepsilon$
[[ ]] $P(X \ge \varepsilon) \le D[X]$
[[ ]] $P(X < M[X])$
[[ ]] $P(X > 0)$
[[?]] Оценка "хвоста" через мат. ожидание.
**************************************************
Это база для вывода неравенства Чебышева.
**************************************************

**9. Если дисперсия $D[X] = 0.01$, то вероятность отклонения от среднего больше чем на 0.5 не превышает:**

[[ ]] $0.5$
[[ ]] $0.1$
[[X]] $0.04$
[[ ]] $0.01$
[[?]] $0.01 / 0.5^2 = 0.01 / 0.25$.
**************************************************
По неравенству Чебышева.
**************************************************

**10. Сходимость "почти наверное" (почти всюду) является:**

[[ ]] Более слабой, чем по вероятности.
[[X]] Более сильной, чем по вероятности.
[[ ]] Эквивалентной.
[[ ]] Не применимой к ЗБЧ.
[[?]] Усиленный закон больших чисел.
**************************************************
Это более строгое условие (вероятность того, что последовательность *не* сойдется, равна 0).
**************************************************

**11. Почему казино всегда выигрывает в долгосрочной перспективе?**

[[ ]] Из-за удачи.
[[X]] Из-за Закона Больших Чисел.
[[ ]] Из-за ЦПТ.
[[ ]] Из-за мошенничества.
[[?]] Стабильность среднего выигрыша при большом числе игр.
**************************************************
Мат. ожидание игры смещено в пользу казино, и при большом $n$ среднее отклонение стремится к нулю.
**************************************************

**12. Для применения теоремы Чебышева величины должны быть:**

[[ ]] Одинаково распределены.
[[ ]] Нормальными.
[[X]] Попарно независимыми.
[[ ]] Зависимыми.
[[?]] Условие на корреляцию.
**************************************************
Достаточно попарной независимости (или некоррелированности), чтобы дисперсия суммы была суммой дисперсий.
**************************************************

**13. Теорема Хинчина (ЗБЧ) работает для:**

[[ ]] Величин с бесконечной дисперсией.
[[X]] Независимых одинаково распределенных величин с конечным мат. ожиданием.
[[ ]] Любых величин.
[[ ]] Только дискретных величин.
[[?]] Требование существования дисперсии снимается, если распределения одинаковы.
**************************************************
Это обобщение ЗБЧ для одинаково распределенных величин.
**************************************************

**14. Что происходит с дисперсией среднего арифметического $D[\bar{X}]$ при $n \to \infty$?**

[[ ]] Стремится к $\infty$.
[[ ]] Стремится к $D[X]$.
[[X]] Стремится к 0.
[[ ]] Стремится к 1.
[[?]] $D[X]/n$.
**************************************************
Разброс среднего значения уменьшается с ростом выборки.
**************************************************

**15. Можно ли утверждать, опираясь на ЗБЧ, что после 10 "орлов" обязательно выпадет "решка"?**

[[ ]] Да, для равновесия.
[[X]] Нет, ЗБЧ работает только на бесконечности ("ошибка игрока").
[[ ]] Да, вероятность повышается.
[[ ]] Нет, монета меняется.
[[?]] Локальная компенсация не гарантируется.
**************************************************
ЗБЧ говорит о доле в длинной серии, а не о коррекции в короткой.
**************************************************

# Тема 34: Центральная Предельная Теорема (ЦПТ)

**1. Центральная Предельная Теорема утверждает, что сумма большого числа независимых случайных величин имеет распределение, близкое к:**

[[ ]] Равномерному.
[[ ]] Пуассоновскому.
[[X]] Нормальному.
[[ ]] Показательному.
[[?]] Колоколообразная кривая.
**************************************************
Это фундаментальный закон природы: сумма множества факторов дает Гаусса.
**************************************************

**2. Какое главное условие (условие Ляпунова) необходимо для выполнения ЦПТ?**

[[ ]] Величины должны быть нормальными.
[[X]] Влияние каждого отдельного слагаемого на общую сумму должно быть ничтожно малым.
[[ ]] Величины должны быть одинаковыми.
[[ ]] Дисперсия должна быть равна 0.
[[?]] Отсутствие доминирующего слагаемого.
**************************************************
Ни одна величина не должна вносить определяющий вклад в дисперсию суммы.
**************************************************

**3. Параметры предельного нормального закона для суммы $S_n$ ($M[X_i]=a, D[X_i]=\sigma^2$):**

[[ ]] $a, \sigma$
[[X]] $n a, \sigma \sqrt{n}$
[[ ]] $a/n, \sigma/n$
[[ ]] $0, 1$
[[?]] Мат. ожидание и дисперсия суммы складываются.
**************************************************
$M[Sum] = n \cdot M$, $D[Sum] = n \cdot D$. СКО — корень из дисперсии.
**************************************************

**4. Распределение среднего арифметического $\bar{X}$ при большом $n$ стремится к:**

[[ ]] $N(a, \sigma)$
[[X]] $N(a, \sigma / \sqrt{n})$
[[ ]] $N(0, 1)$
[[ ]] Равномерному.
[[?]] Центр тот же, разброс меньше.
**************************************************
Среднее значение имеет мат. ожидание $a$ и дисперсию $\sigma^2/n$.
**************************************************

**5. ЦПТ объясняет, почему ошибки измерений часто распределены:**

[[ ]] Хаотично.
[[X]] Нормально.
[[ ]] Равномерно.
[[ ]] По Бернулли.
[[?]] Ошибка — сумма многих малых влияний.
**************************************************
Температура, вибрация, взгляд наблюдателя — много малых факторов.
**************************************************

**6. Нормированная сумма $Z_n = \frac{S_n - M[S_n]}{\sqrt{D[S_n]}}$ сходится по распределению к:**

[[ ]] $N(a, \sigma)$
[[ ]] Равномерному на $[0,1]$.
[[X]] Стандартному нормальному $N(0, 1)$.
[[ ]] $t$-распределению.
[[?]] Центрирование и нормирование.
**************************************************
Предел функции распределения — $\Phi(x)$.
**************************************************

**7. Если слагаемые сами по себе распределены нормально, то их сумма:**

[[ ]] Распределена нормально только при $n \to \infty$.
[[X]] Распределена нормально при любом $n$ (свойство устойчивости).
[[ ]] Распределена по Стьюденту.
[[ ]] Не определена.
[[?]] Композиция нормальных законов.
**************************************************
Сумма гауссовских величин — всегда гауссиана. ЦПТ нужна для *не* нормальных слагаемых.
**************************************************

**8. Скорость сходимости к нормальному закону зависит от:**

[[ ]] Мат. ожидания.
[[X]] Асимметрии исходных распределений (чем симметричнее, тем быстрее).
[[ ]] Знака величин.
[[ ]] Времени суток.
[[?]] Неравенство Берри-Эссеена.
**************************************************
Для симметричных распределений сходимость очень быстрая, для сильно скошенных — медленнее.
**************************************************

**9. Какое распределение имеет сумма 12 независимых равномерно распределенных величин $R[0, 1]$?**

[[ ]] Равномерное.
[[X]] Очень близкое к Нормальному (часто используется для генерации $N(0,1)$ в компьютерах).
[[ ]] Треугольное.
[[ ]] Симпсона.
[[?]] Уже при $n=3$ видно колокол.
**************************************************
При $n=12$ приближение практически идеальное для инженерных задач.
**************************************************

**10. В чем разница между ЗБЧ и ЦПТ?**

[[ ]] Это одно и то же.
[[ ]] ЗБЧ для дискретных, ЦПТ для непрерывных.
[[X]] ЗБЧ говорит о сходимости к числу (константе), ЦПТ — о форме распределения отклонений вокруг этого числа.
[[ ]] ЦПТ слабее ЗБЧ.
[[?]] Точка vs Облако.
**************************************************
ЗБЧ утверждает факт стабилизации, ЦПТ описывает структуру флуктуаций.
**************************************************

**11. Если складывать величины с распределением Коши, будет ли работать ЦПТ?**

[[ ]] Да.
[[X]] Нет, так как у распределения Коши нет дисперсии.
[[ ]] Да, но предел будет другим.
[[ ]] Только при $n > 1000$.
[[?]] Нарушено условие конечной дисперсии.
**************************************************
Сумма величин Коши снова имеет распределение Коши (оно устойчиво), нормализации не происходит.
**************************************************

**12. Работает ли ЦПТ для дискретных слагаемых?**

[[X]] Да, сумма дискретных величин сглаживается и становится почти непрерывной (нормальной).
[[ ]] Нет, только для непрерывных.
[[ ]] Только для биномиального.
[[ ]] Нет, сумма дискретных всегда дискретна.
[[?]] Теорема Муавра-Лапласа тому пример.
**************************************************
Гистограмма суммы становится похожей на гладкую кривую.
**************************************************

**13. «Асимптотическая нормальность» означает, что:**

[[ ]] Величина всегда нормальна.
[[X]] При увеличении параметра (обычно объема выборки) распределение стремится к нормальному.
[[ ]] Плотность равна 0.
[[ ]] Асимптоты графика вертикальные.
[[?]] Свойство в пределе.
**************************************************
Распределение оценок, статистик и сумм становится гауссовским.
**************************************************

**14. Смысл условия Линдеберга (или Ляпунова) простыми словами:**

[[ ]] Слагаемых должно быть много.
[[X]] Слагаемые должны быть равномерно малыми, без "лидеров".
[[ ]] Дисперсии должны быть равны.
[[ ]] Мат. ожидания должны быть 0.
[[?]] Демократия среди слагаемых.
**************************************************
Вклад каждого в общую дисперсию стремится к 0.
**************************************************

**15. Если $X_i$ — ошибки округления (равномерное распределение), то ошибка суммы миллиона чисел:**

[[ ]] Равномерна.
[[ ]] Равна 0.
[[X]] Имеет нормальное распределение.
[[ ]] Бесконечна.
[[?]] Применение ЦПТ.
**************************************************
Суммарная ошибка округления — классический пример нормальности.
**************************************************

# Тема 35: Интегральная теорема Муавра-Лапласа

**1. Интегральная теорема Муавра-Лапласа является частным случаем:**

[[ ]] ЗБЧ.
[[X]] Центральной Предельной Теоремы.
[[ ]] Теоремы Пуассона.
[[ ]] Формулы Байеса.
[[?]] Сумма индикаторов Бернулли.
**************************************************
Число успехов — это сумма независимых одинаковых слагаемых, поэтому работает ЦПТ.
**************************************************

**2. Случайная величина в теореме Муавра-Лапласа — это:**

[[ ]] Время ожидания.
[[X]] Число успехов в $n$ испытаниях Бернулли (биномиальная СВ).
[[ ]] Номер испытания.
[[ ]] Ошибка измерения.
[[?]] $\mu = \sum I_i$.
**************************************************
Мы аппроксимируем биномиальное распределение.
**************************************************

**3. Теорема работает хорошо, когда:**

[[ ]] $n$ мало.
[[ ]] $p$ очень близко к 0.
[[X]] $n$ велико, а $p$ не слишком близко к краям ($npq$ велико).
[[ ]] Всегда.
[[?]] Условие $npq > 10$.
**************************************************
При малом $npq$ распределение асимметрично, и нормальное приближение работает плохо.
**************************************************

**4. Аргумент функции Лапласа в формуле:**

[[ ]] $k$
[[ ]] $k - np$
[[X]] $\frac{k - np}{\sqrt{npq}}$
[[ ]] $npq$
[[?]] Стандартизация.
**************************************************
Мы приводим биномиальную величину к $N(0, 1)$, вычитая среднее и деля на СКО.
**************************************************

**5. Почему теорема называется «интегральной»?**

[[ ]] Использует интегралы для доказательства.
[[X]] Позволяет найти вероятность попадания в интервал (сумму вероятностей).
[[ ]] Использует целочисленные значения.
[[ ]] Это ошибка перевода.
[[?]] В отличие от локальной теоремы (для точки).
**************************************************
Она оценивает $P(k_1 \le X \le k_2) \approx \int \varphi(x) dx$.
**************************************************

**6. Поправка на непрерывность (continuity correction) заключается в:**

[[ ]] Увеличении $n$.
[[ ]] Изменении $p$.
[[X]] Расширении интервала на 0.5 в обе стороны ($k_1 - 0.5, k_2 + 0.5$).
[[ ]] Умножении на $\pi$.
[[?]] Аппроксимация дискретных столбиков непрерывной площадью.
**************************************************
Это повышает точность, так как биномиальное распределение дискретно.
**************************************************

**7. Предельное распределение для частоты $m/n$ (доли успехов):**

[[ ]] $N(p, npq)$
[[X]] $N(p, \sqrt{pq/n})$
[[ ]] $N(0, 1)$
[[ ]] $R[0, 1]$
[[?]] Делим сумму на $n$, дисперсия делится на $n^2$.
**************************************************
Дисперсия частоты равна $pq/n$.
**************************************************

**8. Вероятность отклонения частоты от вероятности $|m/n - p| < \varepsilon$ приближенно равна:**

[[ ]] $2\Phi(\varepsilon \sqrt{n/pq})$
[[X]] $2\Phi(\varepsilon \sqrt{n/pq})$
[[ ]] $\Phi(\varepsilon)$
[[ ]] $1$
[[?]] Формула для доверительного интервала.
**************************************************
Используется для оценки необходимого числа наблюдений в социологии.
**************************************************

**9. Если $p=0.5$, то биномиальное распределение:**

[[ ]] Скошено вправо.
[[X]] Симметрично (идеально подходит под Гаусса).
[[ ]] Скошено влево.
[[ ]] Двумодально.
[[?]] $p=q$.
**************************************************
Сходимость к нормальному закону самая быстрая при $p=0.5$.
**************************************************

**10. Можно ли использовать Муавра-Лапласа для $n=100, p=0.01$?**

[[ ]] Да.
[[X]] Нет, лучше Пуассона ($np=1$).
[[ ]] Да, точность будет высокой.
[[ ]] Нельзя, $n$ слишком мало.
[[?]] Редкие события.
**************************************************
Здесь распределение сильно асимметрично, нормальное приближение даст большую ошибку.
**************************************************

**11. Функция $\Phi(x)$ в формуле это:**

[[ ]] Плотность вероятности.
[[X]] Функция Лапласа (интеграл вероятности).
[[ ]] Производная.
[[ ]] Факториал.
[[?]] Табличная функция площади.
**************************************************
Она дает площадь под нормальной кривой.
**************************************************

**12. Максимум биномиального распределения при больших $n$ находится:**

[[ ]] В точке 0.
[[ ]] В точке $n$.
[[X]] В точке $np$.
[[ ]] В точке $\sqrt{n}$.
[[?]] Совпадает с мат. ожиданием нормального закона.
**************************************************
Наивероятнейшее число успехов.
**************************************************

**13. Если интервал $(k_1, k_2)$ симметричен относительно $np$, то вероятность равна:**

[[ ]] 0
[[X]] $2\Phi(x_2)$
[[ ]] $\Phi(x_2)$
[[ ]] 1
[[?]] Нечетность функции Лапласа.
**************************************************
$\Phi(x) - \Phi(-x) = 2\Phi(x)$.
**************************************************

**14. Чем уже график нормального приближения (меньше $\sqrt{npq}$), тем:**

[[ ]] Точнее прогноз числа успехов.
[[X]] Точнее прогноз, но график выше (пик острее).
[[ ]] Хуже работает теорема.
[[ ]] Меньше $n$.
[[?]] С ростом $n$ дисперсия $npq$ растет, график становится шире в абсолютных единицах, но уже в относительных ($m/n$).
**************************************************
В масштабе абсолютного числа успехов график расплывается ($\sigma \sim \sqrt{n}$), в масштабе частоты — сужается ($\sigma \sim 1/\sqrt{n}$).
**************************************************

**15. Какая теорема исторически появилась первой?**

[[X]] Муавра-Лапласа (для монетки).
[[ ]] Общая ЦПТ (Ляпунова).
[[ ]] Теорема Пуассона.
[[ ]] Закон Чебышева.
[[?]] 18 век.
**************************************************
Это был первый шаг к пониманию нормального закона как предела сумм.
**************************************************
